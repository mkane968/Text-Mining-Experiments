{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 5-2: Sentiment Analysis with nltk.sentiment.SentimentAnalyzer and VADER tools.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyT1GM05v5T4E+gzR13BLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Tutorial%208%3A%20Sentiment%20Analysis%20with%20nltk.sentiment.SentimentAnalyzer%20and%20VADER%20tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial 8: Sentiment Analysis with `nltk.sentiment.SentimentAnalyzer` and VADER tools"
      ],
      "metadata": {
        "id": "COYEfQzej8-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Step 1: Exploring the `subjectivity` corpus***\n",
        "\n",
        "The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences. Learn more about the subjectivity corpus [here](https://www.nltk.org/howto/corpus.html).\n",
        "\n",
        "Import subjectivity corpus and get the file ids."
      ],
      "metadata": {
        "id": "HHAKVRRnlGxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ6vs51Tj8F_",
        "outputId": "01c93ac4-ecfc-4560-e115-cf8d0cf8a6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plot.tok.gt9.5000', 'quote.tok.gt9.5000']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('subjectivity')\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "subjectivity.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in plot.tok file"
      ],
      "metadata": {
        "id": "bbPO1cNqmD_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents('plot.tok.gt9.5000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OvcGQBpmE5T",
        "outputId": "dc348af1-9a46-49cd-c84d-a543818e4ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in quote.tok file"
      ],
      "metadata": {
        "id": "BspFeKyCmomm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents('quote.tok.gt9.5000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZ_rKt8mvzk",
        "outputId": "fc29a2ca-68f7-4355-c2af-b5a827e9dbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the categories in subjectivity corpus (objective and subjective sentences)."
      ],
      "metadata": {
        "id": "MUq_FHPgmzcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.categories() # The mapping between documents and categories does not depend on the file structure."
      ],
      "metadata": {
        "id": "iFSv2oNqmy5c",
        "outputId": "260e22f2-bb3b-46ef-9248-4651ecf2dccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['obj', 'subj']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in subjectivity that are categorized as \"objective\""
      ],
      "metadata": {
        "id": "CcnY55Pjnhsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents(categories='obj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTtbpLjnh2p",
        "outputId": "41ecd593-d959-4331-debb-facce1e8328a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in subjectivity that are categorized as \"subjective\""
      ],
      "metadata": {
        "id": "S-aYzaRQoOw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents(categories='subj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J36LTi4PoO4b",
        "outputId": "81341808-62ea-46ee-d8d8-f31a5dc8d1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Step 2: Building and testing a classifier with `SentimentAnalyzer`***"
      ],
      "metadata": {
        "id": "-kh_y5Sloiln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary classifiers and modules. "
      ],
      "metadata": {
        "id": "Df9CQPoVoqgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\n",
        "from nltk.sentiment.util import (mark_negation, extract_unigram_feats) # mark_negation(): Append _NEG suffix to words that appear in the scope between a negation and a punctuation mark. extract_unigram_feats(): Populate a dictionary of unigram features, reflecting the presence/absence in the document of each of the tokens in unigrams.\n"
      ],
      "metadata": {
        "id": "uW1gngyPoqpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set number of instances at 100; then create two new lists for objective and subjective docs and put sentences up to number of n_instancse (100) in each list. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings.\n",
        "\n",
        "Print length of each list to check they both contain 100 sentences."
      ],
      "metadata": {
        "id": "_Ob_727eXrYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_instances = 100\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "len(obj_docs), len(subj_docs)"
      ],
      "metadata": {
        "id": "6ifXBgMoXrja",
        "outputId": "2f39cea0-56df-428b-ab9d-463555fbba5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sentence in obj_docs list to check:"
      ],
      "metadata": {
        "id": "r23Kr1xxYcjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_docs[0]"
      ],
      "metadata": {
        "id": "U1idOEHVYcq3",
        "outputId": "ddfd78df-e00c-4942-d243-9f95ef07f87c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['the',\n",
              "  'movie',\n",
              "  'begins',\n",
              "  'in',\n",
              "  'the',\n",
              "  'past',\n",
              "  'where',\n",
              "  'a',\n",
              "  'young',\n",
              "  'boy',\n",
              "  'named',\n",
              "  'sam',\n",
              "  'attempts',\n",
              "  'to',\n",
              "  'save',\n",
              "  'celebi',\n",
              "  'from',\n",
              "  'a',\n",
              "  'hunter',\n",
              "  '.'],\n",
              " 'obj')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divde sentences into training and testing groups; first 80 sentences of each are for training, last 20 for testing. Split evenly for objective and subjective docs, then combine into two larger groups (all training and all testing)."
      ],
      "metadata": {
        "id": "2LfDV64CYj7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_obj_docs = obj_docs[:80]\n",
        "test_obj_docs = obj_docs[80:100]\n",
        "train_subj_docs = subj_docs[:80]\n",
        "test_subj_docs = subj_docs[80:100]\n",
        "\n",
        "training_docs = train_obj_docs + train_subj_docs\n",
        "testing_docs = test_obj_docs + test_subj_docs"
      ],
      "metadata": {
        "id": "jg1aXbTsYkCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define sentiment analyzer as `SentimentAnalyzer()` and use it to append _NEG suffix to words that appear between a sensed negation and a punctuation mark."
      ],
      "metadata": {
        "id": "cvg-fUBHY3sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
        "#all_words_neg"
      ],
      "metadata": {
        "id": "Nt3vhXFgY3Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the list of most common 1-word features in all_words_neg, with a minimum frequency of 4 appearances."
      ],
      "metadata": {
        "id": "ICLzdAz_Zk9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "len(unigram_feats)"
      ],
      "metadata": {
        "id": "W0sx3HcoZldZ",
        "outputId": "d55523ba-a6f4-4b5e-ba46-043dfbf64de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add unigram_features to list of features that the sentiment analyzer will extract from the data."
      ],
      "metadata": {
        "id": "I_x6dJbicnFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
      ],
      "metadata": {
        "id": "0l2UG9qNcg6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redefine training and test set to include whether or not sents include the `unigram_feats`"
      ],
      "metadata": {
        "id": "wcIWyc4YcTl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = sentim_analyzer.apply_features(training_docs)\n",
        "test_set = sentim_analyzer.apply_features(testing_docs)\n",
        "#training_set[0]"
      ],
      "metadata": {
        "id": "Rc_HWRZPbiJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train our classifier on the training set, and subsequently output the evaluation results. "
      ],
      "metadata": {
        "id": "OmOgpJ21bh_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n"
      ],
      "metadata": {
        "id": "I1ZDFgfccTtW",
        "outputId": "97ab854b-b70d-4e57-8074-809a7fd1bbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of results from [Python NLTK Cookbook:](https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/)\n",
        "\n",
        "*  **Accuracy** measures the number of elements correctly identified in a data set.\n",
        "*  **F-measure** is the weighted harmonic mean of precision and recall. \n",
        "*  **Precision** measures the exactness of a classifier. A higher precision means less false positives, while a lower precision means more false positives.\n",
        "*   **Recall** measures the completeness, or sensitivity, of a classifier. Higher recall means less false negatives, while lower recall means more false negatives. Often improves inverse of precision.\n"
      ],
      "metadata": {
        "id": "HsO80ZOveRbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "    print('{0}: {1}'.format(key, value))"
      ],
      "metadata": {
        "id": "iZgR6_eSd-TJ",
        "outputId": "9fa43753-8902-433d-b1c3-b2b6533d4229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating NaiveBayesClassifier results...\n",
            "Accuracy: 0.8\n",
            "F-measure [obj]: 0.8\n",
            "F-measure [subj]: 0.8\n",
            "Precision [obj]: 0.8\n",
            "Precision [subj]: 0.8\n",
            "Recall [obj]: 0.8\n",
            "Recall [subj]: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Step 3: Building and testing a classifier with `nltk.sentiment.vader.SentimentIntensityAnalyzer`***"
      ],
      "metadata": {
        "id": "houZ2eJQfyrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import `SentimentIntensityAnalyzer `from [Vader](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). This will assign an \"intensity score\" to each sentence based on its identified sentiment."
      ],
      "metadata": {
        "id": "y74QTxjAgDIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "ktyOwbwJfy0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add list of sentences for analysis."
      ],
      "metadata": {
        "id": "HaN71m4Xgr0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"You are a jerk, and I will step on you.\",\n",
        "    \"THIS SUX!!!\",\n",
        "    \"This kinda sux...\",\n",
        "    \"You're good, man\",\n",
        "    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n",
        "            ]"
      ],
      "metadata": {
        "id": "2BsReT4tgsCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SentimentIntesnityAnalyzer (defined as sid) to get \"intensity\" of each sentence in list"
      ],
      "metadata": {
        "id": "-hXXSPfng3PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "for sentence in sentences:\n",
        "    print('\\n' + sentence)\n",
        "    ss = sid.polarity_scores(sentence)\n",
        "    for k in sorted(ss):\n",
        "        print('{0}: {1}, '.format(k, ss[k]), end='')"
      ],
      "metadata": {
        "id": "8UCeoLBXg3bI",
        "outputId": "c06a619f-c35f-456f-bc1d-52ca7ec859bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "\n",
            "You are a jerk, and I will step on you.\n",
            "compound: -0.34, neg: 0.255, neu: 0.745, pos: 0.0, \n",
            "THIS SUX!!!\n",
            "compound: -0.5229, neg: 0.771, neu: 0.229, pos: 0.0, \n",
            "This kinda sux...\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "You're good, man\n",
            "compound: 0.4404, neg: 0.0, neu: 0.408, pos: 0.592, \n",
            "HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\n",
            "compound: 0.8386, neg: 0.0, neu: 0.386, pos: 0.614, "
          ]
        }
      ]
    }
  ]
}