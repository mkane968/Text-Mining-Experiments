{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 2: Modules for Language Identification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPryXCz12iaU1oenznmBsTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Tutorial%202%3A%20Modules%20for%20Language%20Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Modules for Language Identification \n",
        "# (N-Gram, Stopword and Word Bigram Analysis)"
      ],
      "metadata": {
        "id": "9pJU0rMMcGtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.1: Deriving N-Grams from Text**\n",
        "\n",
        "Based on N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla\n",
        "\n",
        "What are n-grams? See [here](https://cloudmark.github.io/Language-Detection/)."
      ],
      "metadata": {
        "id": "GTJI2-ALU05f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization:*** Divides strings of text into substrings of letters and apostrophes ONLY to prepare for n-gram analysis"
      ],
      "metadata": {
        "id": "RjaYkNAEVDOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cLZvk_4ZUu1H"
      },
      "outputs": [],
      "source": [
        "#Lowercase text in string\n",
        "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
        "s = s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import regular expressions tokenizer and tokenize string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
        "s_tokenized = tokenizer.tokenize(s)\n",
        "s_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVBiQkhpVdzt",
        "outputId": "1e4955b5-43dd-4b3d-8b21-9a48eb779021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['le',\n",
              " 'temps',\n",
              " 'est',\n",
              " 'un',\n",
              " 'grand',\n",
              " 'maître',\n",
              " 'dit',\n",
              " 'on',\n",
              " 'le',\n",
              " 'malheur',\n",
              " 'est',\n",
              " \"qu'il\",\n",
              " 'tue',\n",
              " 'ses',\n",
              " 'élèves']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generating N-Grams:*** Finds n-length slices of a longer string, typically overlapping/in sequence; can be used for language detection"
      ],
      "metadata": {
        "id": "wkuxxJDxVwuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import ngrams module and create list for generated ngrams\n",
        "from nltk.util import ngrams\n",
        "generated_4grams = []\n",
        "\n",
        "#Generate ngram for each word in tokenized string\n",
        "for word in s_tokenized:\n",
        "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
        "generated_4grams"
      ],
      "metadata": {
        "id": "8MozGI4aV3LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that generated_4grams needs flattening since it's supposed to be a list of 4-grams:"
      ],
      "metadata": {
        "id": "YfeR9ZBqV2l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates list in which ????\n",
        "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
        "generated_4grams[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqLwYALY84E",
        "outputId": "64a05f5e-5291-4e43-bc52-8b8cd269b790"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_', '_', '_', 'l'),\n",
              " ('_', '_', 'l', 'e'),\n",
              " ('_', 'l', 'e', '_'),\n",
              " ('l', 'e', '_', '_'),\n",
              " ('e', '_', '_', '_'),\n",
              " ('_', '_', '_', 't'),\n",
              " ('_', '_', 't', 'e'),\n",
              " ('_', 't', 'e', 'm'),\n",
              " ('t', 'e', 'm', 'p'),\n",
              " ('e', 'm', 'p', 's')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining n-grams (n = 4)"
      ],
      "metadata": {
        "id": "-8lnKPwzaOGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Join 4grams into list of strings\n",
        "ng_list_4grams = generated_4grams\n",
        "for idx, val in enumerate(generated_4grams):\n",
        "    ng_list_4grams[idx] = ''.join(val)\n",
        "ng_list_4grams"
      ],
      "metadata": {
        "id": "N3540KisaNMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort n-grams by how frequently they appear within the text"
      ],
      "metadata": {
        "id": "--vvU1CZavQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create list for n-grams sorted by frequency\n",
        "freq_4grams = {}\n",
        "\n",
        "#Iterate through ngrams and add to freq_4grams list as many times as appearing in list of strings\n",
        "for ngram in ng_list_4grams:\n",
        "    if ngram not in freq_4grams:\n",
        "        freq_4grams.update({ngram: 1})\n",
        "    else:\n",
        "        ngram_occurrences = freq_4grams[ngram]\n",
        "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
        "        \n",
        "# The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
        "from operator import itemgetter \n",
        "\n",
        "# We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
        "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] \n",
        "freq_4grams_sorted"
      ],
      "metadata": {
        "id": "FIvRnUepavYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain n-grams for multiple values of n (n = 1, 2, 3, 4)"
      ],
      "metadata": {
        "id": "dPt3vftZbO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import everygrams\n",
        "\n",
        "# For the code below we need the raw sentence as opposed to the tokens.\n",
        "s_clean = ' '.join(s_tokenized) \n",
        "s_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcvlXoFtbWk2",
        "outputId": "acd053ba-ba85-48d3-aacd-0407c2ce0293"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define ngram extractor generating uni-grams, bigrams, trigrams and 4-grams of string (range set to 1-4)\n",
        "def ngram_extractor(sent):\n",
        "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
        "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
        "\n",
        "ngram_extractor(s_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imOf0UoEbrEC",
        "outputId": "2d5a64fb-ab0d-4a59-db59-7cbf2f331142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l',\n",
              " 'e',\n",
              " 't',\n",
              " 'e',\n",
              " 'm',\n",
              " 'p',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'n',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'm',\n",
              " 'a',\n",
              " 'î',\n",
              " 't',\n",
              " 'r',\n",
              " 'e',\n",
              " 'd',\n",
              " 'i',\n",
              " 't',\n",
              " 'o',\n",
              " 'n',\n",
              " 'l',\n",
              " 'e',\n",
              " 'm',\n",
              " 'a',\n",
              " 'l',\n",
              " 'h',\n",
              " 'e',\n",
              " 'u',\n",
              " 'r',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'q',\n",
              " 'u',\n",
              " \"'\",\n",
              " 'i',\n",
              " 'l',\n",
              " 't',\n",
              " 'u',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 'é',\n",
              " 'l',\n",
              " 'è',\n",
              " 'v',\n",
              " 'e',\n",
              " 's',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_t',\n",
              " 'te',\n",
              " 'em',\n",
              " 'mp',\n",
              " 'ps',\n",
              " 's_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_u',\n",
              " 'un',\n",
              " 'n_',\n",
              " '_g',\n",
              " 'gr',\n",
              " 'ra',\n",
              " 'an',\n",
              " 'nd',\n",
              " 'd_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'aî',\n",
              " 'ît',\n",
              " 'tr',\n",
              " 're',\n",
              " 'e_',\n",
              " '_d',\n",
              " 'di',\n",
              " 'it',\n",
              " 't_',\n",
              " '_o',\n",
              " 'on',\n",
              " 'n_',\n",
              " '_l',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'al',\n",
              " 'lh',\n",
              " 'he',\n",
              " 'eu',\n",
              " 'ur',\n",
              " 'r_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_q',\n",
              " 'qu',\n",
              " \"u'\",\n",
              " \"'i\",\n",
              " 'il',\n",
              " 'l_',\n",
              " '_t',\n",
              " 'tu',\n",
              " 'ue',\n",
              " 'e_',\n",
              " '_s',\n",
              " 'se',\n",
              " 'es',\n",
              " 's_',\n",
              " '_é',\n",
              " 'él',\n",
              " 'lè',\n",
              " 'èv',\n",
              " 've',\n",
              " 'es',\n",
              " 'le_',\n",
              " '_te',\n",
              " 'tem',\n",
              " 'emp',\n",
              " 'mps',\n",
              " 'ps_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_un',\n",
              " 'un_',\n",
              " '_gr',\n",
              " 'gra',\n",
              " 'ran',\n",
              " 'and',\n",
              " 'nd_',\n",
              " '_ma',\n",
              " 'maî',\n",
              " 'aît',\n",
              " 'îtr',\n",
              " 'tre',\n",
              " 're_',\n",
              " '_di',\n",
              " 'dit',\n",
              " 'it_',\n",
              " '_on',\n",
              " 'on_',\n",
              " '_le',\n",
              " 'le_',\n",
              " '_ma',\n",
              " 'mal',\n",
              " 'alh',\n",
              " 'lhe',\n",
              " 'heu',\n",
              " 'eur',\n",
              " 'ur_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_qu',\n",
              " \"qu'\",\n",
              " \"u'i\",\n",
              " \"'il\",\n",
              " 'il_',\n",
              " '_tu',\n",
              " 'tue',\n",
              " 'ue_',\n",
              " '_se',\n",
              " 'ses',\n",
              " 'es_',\n",
              " '_él',\n",
              " 'élè',\n",
              " 'lèv',\n",
              " 'ève',\n",
              " 'ves',\n",
              " '_tem',\n",
              " 'temp',\n",
              " 'emps',\n",
              " 'mps_',\n",
              " '_est',\n",
              " 'est_',\n",
              " '_un_',\n",
              " '_gra',\n",
              " 'gran',\n",
              " 'rand',\n",
              " 'and_',\n",
              " '_maî',\n",
              " 'maît',\n",
              " 'aîtr',\n",
              " 'ître',\n",
              " 'tre_',\n",
              " '_dit',\n",
              " 'dit_',\n",
              " '_on_',\n",
              " '_le_',\n",
              " '_mal',\n",
              " 'malh',\n",
              " 'alhe',\n",
              " 'lheu',\n",
              " 'heur',\n",
              " 'eur_',\n",
              " '_est',\n",
              " 'est_',\n",
              " \"_qu'\",\n",
              " \"qu'i\",\n",
              " \"u'il\",\n",
              " \"'il_\",\n",
              " '_tue',\n",
              " 'tue_',\n",
              " '_ses',\n",
              " 'ses_',\n",
              " '_élè',\n",
              " 'élèv',\n",
              " 'lève',\n",
              " 'èves']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.2:** Detecting Text Language by Counting Stopwords\n",
        "\n",
        "Based on Detecting Text Language With Python and NLTK by Alejandro Nolla\n",
        "\n",
        "Stop words are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature e.g. search engines remove words like 'want'."
      ],
      "metadata": {
        "id": "gvbJKxQRdPrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2-ImW1h0dPNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}