{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 2: Modules for Language Identification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUZwikNI2EURCbCXfHUP9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Tutorial%202%3A%20Modules%20for%20Language%20Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Modules for Language Identification \n",
        "# (N-Gram, Stopword and Word Bigram Analysis)"
      ],
      "metadata": {
        "id": "9pJU0rMMcGtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.1: Deriving N-Grams from Text**\n",
        "\n",
        "Based on [N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)\n",
        "\n",
        "What are n-grams? See [here](https://cloudmark.github.io/Language-Detection/)."
      ],
      "metadata": {
        "id": "GTJI2-ALU05f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization:*** Divides strings of text into substrings of letters and apostrophes ONLY to prepare for n-gram analysis"
      ],
      "metadata": {
        "id": "RjaYkNAEVDOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cLZvk_4ZUu1H"
      },
      "outputs": [],
      "source": [
        "#Lowercase text in string\n",
        "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
        "s = s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import regular expressions tokenizer and tokenize string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
        "s_tokenized = tokenizer.tokenize(s)\n",
        "s_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVBiQkhpVdzt",
        "outputId": "1e4955b5-43dd-4b3d-8b21-9a48eb779021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['le',\n",
              " 'temps',\n",
              " 'est',\n",
              " 'un',\n",
              " 'grand',\n",
              " 'maître',\n",
              " 'dit',\n",
              " 'on',\n",
              " 'le',\n",
              " 'malheur',\n",
              " 'est',\n",
              " \"qu'il\",\n",
              " 'tue',\n",
              " 'ses',\n",
              " 'élèves']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generating N-Grams:*** Finds n-length slices of a longer string, typically overlapping/in sequence; can be used for language detection"
      ],
      "metadata": {
        "id": "wkuxxJDxVwuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import ngrams module and create list for generated ngrams\n",
        "from nltk.util import ngrams\n",
        "generated_4grams = []\n",
        "\n",
        "#Generate ngram for each word in tokenized string\n",
        "for word in s_tokenized:\n",
        "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
        "generated_4grams"
      ],
      "metadata": {
        "id": "8MozGI4aV3LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that generated_4grams needs flattening since it's supposed to be a list of 4-grams:"
      ],
      "metadata": {
        "id": "YfeR9ZBqV2l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates list in which ????\n",
        "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
        "generated_4grams[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqLwYALY84E",
        "outputId": "64a05f5e-5291-4e43-bc52-8b8cd269b790"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_', '_', '_', 'l'),\n",
              " ('_', '_', 'l', 'e'),\n",
              " ('_', 'l', 'e', '_'),\n",
              " ('l', 'e', '_', '_'),\n",
              " ('e', '_', '_', '_'),\n",
              " ('_', '_', '_', 't'),\n",
              " ('_', '_', 't', 'e'),\n",
              " ('_', 't', 'e', 'm'),\n",
              " ('t', 'e', 'm', 'p'),\n",
              " ('e', 'm', 'p', 's')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining n-grams (n = 4)"
      ],
      "metadata": {
        "id": "-8lnKPwzaOGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Join 4grams into list of strings\n",
        "ng_list_4grams = generated_4grams\n",
        "for idx, val in enumerate(generated_4grams):\n",
        "    ng_list_4grams[idx] = ''.join(val)\n",
        "ng_list_4grams"
      ],
      "metadata": {
        "id": "N3540KisaNMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort n-grams by how frequently they appear within the text"
      ],
      "metadata": {
        "id": "--vvU1CZavQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create list for n-grams sorted by frequency\n",
        "freq_4grams = {}\n",
        "\n",
        "#Iterate through ngrams and add to freq_4grams list as many times as appearing in list of strings\n",
        "for ngram in ng_list_4grams:\n",
        "    if ngram not in freq_4grams:\n",
        "        freq_4grams.update({ngram: 1})\n",
        "    else:\n",
        "        ngram_occurrences = freq_4grams[ngram]\n",
        "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
        "        \n",
        "# The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
        "from operator import itemgetter \n",
        "\n",
        "# We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
        "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] \n",
        "freq_4grams_sorted"
      ],
      "metadata": {
        "id": "FIvRnUepavYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain n-grams for multiple values of n (n = 1, 2, 3, 4)"
      ],
      "metadata": {
        "id": "dPt3vftZbO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import everygrams\n",
        "\n",
        "# For the code below we need the raw sentence as opposed to the tokens.\n",
        "s_clean = ' '.join(s_tokenized) \n",
        "s_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcvlXoFtbWk2",
        "outputId": "acd053ba-ba85-48d3-aacd-0407c2ce0293"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define ngram extractor generating uni-grams, bigrams, trigrams and 4-grams of string (range set to 1-4)\n",
        "def ngram_extractor(sent):\n",
        "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
        "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
        "\n",
        "ngram_extractor(s_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imOf0UoEbrEC",
        "outputId": "2d5a64fb-ab0d-4a59-db59-7cbf2f331142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l',\n",
              " 'e',\n",
              " 't',\n",
              " 'e',\n",
              " 'm',\n",
              " 'p',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'n',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'm',\n",
              " 'a',\n",
              " 'î',\n",
              " 't',\n",
              " 'r',\n",
              " 'e',\n",
              " 'd',\n",
              " 'i',\n",
              " 't',\n",
              " 'o',\n",
              " 'n',\n",
              " 'l',\n",
              " 'e',\n",
              " 'm',\n",
              " 'a',\n",
              " 'l',\n",
              " 'h',\n",
              " 'e',\n",
              " 'u',\n",
              " 'r',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'q',\n",
              " 'u',\n",
              " \"'\",\n",
              " 'i',\n",
              " 'l',\n",
              " 't',\n",
              " 'u',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 'é',\n",
              " 'l',\n",
              " 'è',\n",
              " 'v',\n",
              " 'e',\n",
              " 's',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_t',\n",
              " 'te',\n",
              " 'em',\n",
              " 'mp',\n",
              " 'ps',\n",
              " 's_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_u',\n",
              " 'un',\n",
              " 'n_',\n",
              " '_g',\n",
              " 'gr',\n",
              " 'ra',\n",
              " 'an',\n",
              " 'nd',\n",
              " 'd_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'aî',\n",
              " 'ît',\n",
              " 'tr',\n",
              " 're',\n",
              " 'e_',\n",
              " '_d',\n",
              " 'di',\n",
              " 'it',\n",
              " 't_',\n",
              " '_o',\n",
              " 'on',\n",
              " 'n_',\n",
              " '_l',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'al',\n",
              " 'lh',\n",
              " 'he',\n",
              " 'eu',\n",
              " 'ur',\n",
              " 'r_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_q',\n",
              " 'qu',\n",
              " \"u'\",\n",
              " \"'i\",\n",
              " 'il',\n",
              " 'l_',\n",
              " '_t',\n",
              " 'tu',\n",
              " 'ue',\n",
              " 'e_',\n",
              " '_s',\n",
              " 'se',\n",
              " 'es',\n",
              " 's_',\n",
              " '_é',\n",
              " 'él',\n",
              " 'lè',\n",
              " 'èv',\n",
              " 've',\n",
              " 'es',\n",
              " 'le_',\n",
              " '_te',\n",
              " 'tem',\n",
              " 'emp',\n",
              " 'mps',\n",
              " 'ps_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_un',\n",
              " 'un_',\n",
              " '_gr',\n",
              " 'gra',\n",
              " 'ran',\n",
              " 'and',\n",
              " 'nd_',\n",
              " '_ma',\n",
              " 'maî',\n",
              " 'aît',\n",
              " 'îtr',\n",
              " 'tre',\n",
              " 're_',\n",
              " '_di',\n",
              " 'dit',\n",
              " 'it_',\n",
              " '_on',\n",
              " 'on_',\n",
              " '_le',\n",
              " 'le_',\n",
              " '_ma',\n",
              " 'mal',\n",
              " 'alh',\n",
              " 'lhe',\n",
              " 'heu',\n",
              " 'eur',\n",
              " 'ur_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_qu',\n",
              " \"qu'\",\n",
              " \"u'i\",\n",
              " \"'il\",\n",
              " 'il_',\n",
              " '_tu',\n",
              " 'tue',\n",
              " 'ue_',\n",
              " '_se',\n",
              " 'ses',\n",
              " 'es_',\n",
              " '_él',\n",
              " 'élè',\n",
              " 'lèv',\n",
              " 'ève',\n",
              " 'ves',\n",
              " '_tem',\n",
              " 'temp',\n",
              " 'emps',\n",
              " 'mps_',\n",
              " '_est',\n",
              " 'est_',\n",
              " '_un_',\n",
              " '_gra',\n",
              " 'gran',\n",
              " 'rand',\n",
              " 'and_',\n",
              " '_maî',\n",
              " 'maît',\n",
              " 'aîtr',\n",
              " 'ître',\n",
              " 'tre_',\n",
              " '_dit',\n",
              " 'dit_',\n",
              " '_on_',\n",
              " '_le_',\n",
              " '_mal',\n",
              " 'malh',\n",
              " 'alhe',\n",
              " 'lheu',\n",
              " 'heur',\n",
              " 'eur_',\n",
              " '_est',\n",
              " 'est_',\n",
              " \"_qu'\",\n",
              " \"qu'i\",\n",
              " \"u'il\",\n",
              " \"'il_\",\n",
              " '_tue',\n",
              " 'tue_',\n",
              " '_ses',\n",
              " 'ses_',\n",
              " '_élè',\n",
              " 'élèv',\n",
              " 'lève',\n",
              " 'èves']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.2: Detecting Text Language by Counting Stopwords** \n",
        "\n",
        "Based on [Detecting Text Language With Python and NLTK by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/)\n",
        "\n",
        "Stop words are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature (e.g. search engines remove words like 'want')"
      ],
      "metadata": {
        "id": "gvbJKxQRdPrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize text, regex-based tokenizer splitting text on whitespace and punctuation (except for underscore)\n",
        "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\"\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize import wordpunct_tokenize \n",
        "except ImportError:\n",
        "    print('[!] You need to install nltk (http://nltk.org/index.html)')\n",
        "test_tokens = wordpunct_tokenize(text)\n",
        "test_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-ImW1h0dPNJ",
        "outputId": "800a8e03-2f3f-4944-a54d-1aec3b80e0c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yo',\n",
              " 'man',\n",
              " ',',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'time',\n",
              " 'for',\n",
              " 'you',\n",
              " 'to',\n",
              " 'shut',\n",
              " 'yo',\n",
              " \"'\",\n",
              " 'mouth',\n",
              " '!',\n",
              " 'I',\n",
              " 'ain',\n",
              " \"'\",\n",
              " 't',\n",
              " 'even',\n",
              " 'messin',\n",
              " \"'\",\n",
              " 'dawg',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other tokenizers e.g. RegexpTokenizer where you can enter your own regexp, WhitespaceTokenizer (similar to Python's string.split()) and BlanklineTokenizer."
      ],
      "metadata": {
        "id": "_D-Ytvpy1v8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring NLTK Stopword Corpus** \n",
        "\n",
        "NLTK comes with a corpus of stop words in various languages."
      ],
      "metadata": {
        "id": "kl9cLYTb11Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Since this is raw text, we need to replace \\n's with spaces for it to be readable.\n",
        "stopwords.readme().replace('\\n', ' ') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "hvpqeg0R2DJr",
        "outputId": "199e14bd-9946-40f7-975c-61bb7fd8efcd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files is accessed via fileids(). Here you can see the list of languages the corpus contains\n",
        "stopwords.fileids() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RzNbpi72VQD",
        "outputId": "e2ddc915-e793-4095-9d61-1b76d2903607"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'bengali',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore raw corpus of Greek stopwords\n",
        "stopwords.raw('greek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Fy_-YGTs2wSi",
        "outputId": "9e40fb0c-b7cb-4bee-bf37-d0c2cced542e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"αλλα\\nαν\\nαντι\\nαπο\\nαυτα\\nαυτεσ\\nαυτη\\nαυτο\\nαυτοι\\nαυτοσ\\nαυτουσ\\nαυτων\\nαἱ\\nαἳ\\nαἵ\\nαὐτόσ\\nαὐτὸς\\nαὖ\\nγάρ\\nγα\\nγα^\\nγε\\nγια\\nγοῦν\\nγὰρ\\nδ'\\nδέ\\nδή\\nδαί\\nδαίσ\\nδαὶ\\nδαὶς\\nδε\\nδεν\\nδι'\\nδιά\\nδιὰ\\nδὲ\\nδὴ\\nδ’\\nεαν\\nειμαι\\nειμαστε\\nειναι\\nεισαι\\nειστε\\nεκεινα\\nεκεινεσ\\nεκεινη\\nεκεινο\\nεκεινοι\\nεκεινοσ\\nεκεινουσ\\nεκεινων\\nενω\\nεπ\\nεπι\\nεἰ\\nεἰμί\\nεἰμὶ\\nεἰς\\nεἰσ\\nεἴ\\nεἴμι\\nεἴτε\\nη\\nθα\\nισωσ\\nκ\\nκαί\\nκαίτοι\\nκαθ\\nκαι\\nκατ\\nκατά\\nκατα\\nκατὰ\\nκαὶ\\nκι\\nκἀν\\nκἂν\\nμέν\\nμή\\nμήτε\\nμα\\nμε\\nμεθ\\nμετ\\nμετά\\nμετα\\nμετὰ\\nμη\\nμην\\nμἐν\\nμὲν\\nμὴ\\nμὴν\\nνα\\nο\\nοι\\nομωσ\\nοπωσ\\nοσο\\nοτι\\nοἱ\\nοἳ\\nοἷς\\nοὐ\\nοὐδ\\nοὐδέ\\nοὐδείσ\\nοὐδεὶς\\nοὐδὲ\\nοὐδὲν\\nοὐκ\\nοὐχ\\nοὐχὶ\\nοὓς\\nοὔτε\\nοὕτω\\nοὕτως\\nοὕτωσ\\nοὖν\\nοὗ\\nοὗτος\\nοὗτοσ\\nπαρ\\nπαρά\\nπαρα\\nπαρὰ\\nπερί\\nπερὶ\\nποια\\nποιεσ\\nποιο\\nποιοι\\nποιοσ\\nποιουσ\\nποιων\\nποτε\\nπου\\nποῦ\\nπρο\\nπροσ\\nπρόσ\\nπρὸ\\nπρὸς\\nπως\\nπωσ\\nσε\\nστη\\nστην\\nστο\\nστον\\nσόσ\\nσύ\\nσύν\\nσὸς\\nσὺ\\nσὺν\\nτά\\nτήν\\nτί\\nτίς\\nτίσ\\nτα\\nταῖς\\nτε\\nτην\\nτησ\\nτι\\nτινα\\nτις\\nτισ\\nτο\\nτοί\\nτοι\\nτοιοῦτος\\nτοιοῦτοσ\\nτον\\nτοτε\\nτου\\nτούσ\\nτοὺς\\nτοῖς\\nτοῦ\\nτων\\nτό\\nτόν\\nτότε\\nτὰ\\nτὰς\\nτὴν\\nτὸ\\nτὸν\\nτῆς\\nτῆσ\\nτῇ\\nτῶν\\nτῷ\\nωσ\\nἀλλ'\\nἀλλά\\nἀλλὰ\\nἀλλ’\\nἀπ\\nἀπό\\nἀπὸ\\nἀφ\\nἂν\\nἃ\\nἄλλος\\nἄλλοσ\\nἄν\\nἄρα\\nἅμα\\nἐάν\\nἐγώ\\nἐγὼ\\nἐκ\\nἐμόσ\\nἐμὸς\\nἐν\\nἐξ\\nἐπί\\nἐπεὶ\\nἐπὶ\\nἐστι\\nἐφ\\nἐὰν\\nἑαυτοῦ\\nἔτι\\nἡ\\nἢ\\nἣ\\nἤ\\nἥ\\nἧς\\nἵνα\\nὁ\\nὃ\\nὃν\\nὃς\\nὅ\\nὅδε\\nὅθεν\\nὅπερ\\nὅς\\nὅσ\\nὅστις\\nὅστισ\\nὅτε\\nὅτι\\nὑμόσ\\nὑπ\\nὑπέρ\\nὑπό\\nὑπὲρ\\nὑπὸ\\nὡς\\nὡσ\\nὥς\\nὥστε\\nὦ\\nᾧ\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Greek stopwards by \\n with space\n",
        "stopwords.raw('greek').replace('\\n', ' ') # Better"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "vb2e4r3f21zI",
        "outputId": "39b45e80-50fc-4968-c07f-11622132f898"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"αλλα αν αντι απο αυτα αυτεσ αυτη αυτο αυτοι αυτοσ αυτουσ αυτων αἱ αἳ αἵ αὐτόσ αὐτὸς αὖ γάρ γα γα^ γε για γοῦν γὰρ δ' δέ δή δαί δαίσ δαὶ δαὶς δε δεν δι' διά διὰ δὲ δὴ δ’ εαν ειμαι ειμαστε ειναι εισαι ειστε εκεινα εκεινεσ εκεινη εκεινο εκεινοι εκεινοσ εκεινουσ εκεινων ενω επ επι εἰ εἰμί εἰμὶ εἰς εἰσ εἴ εἴμι εἴτε η θα ισωσ κ καί καίτοι καθ και κατ κατά κατα κατὰ καὶ κι κἀν κἂν μέν μή μήτε μα με μεθ μετ μετά μετα μετὰ μη μην μἐν μὲν μὴ μὴν να ο οι ομωσ οπωσ οσο οτι οἱ οἳ οἷς οὐ οὐδ οὐδέ οὐδείσ οὐδεὶς οὐδὲ οὐδὲν οὐκ οὐχ οὐχὶ οὓς οὔτε οὕτω οὕτως οὕτωσ οὖν οὗ οὗτος οὗτοσ παρ παρά παρα παρὰ περί περὶ ποια ποιεσ ποιο ποιοι ποιοσ ποιουσ ποιων ποτε που ποῦ προ προσ πρόσ πρὸ πρὸς πως πωσ σε στη στην στο στον σόσ σύ σύν σὸς σὺ σὺν τά τήν τί τίς τίσ τα ταῖς τε την τησ τι τινα τις τισ το τοί τοι τοιοῦτος τοιοῦτοσ τον τοτε του τούσ τοὺς τοῖς τοῦ των τό τόν τότε τὰ τὰς τὴν τὸ τὸν τῆς τῆσ τῇ τῶν τῷ ωσ ἀλλ' ἀλλά ἀλλὰ ἀλλ’ ἀπ ἀπό ἀπὸ ἀφ ἂν ἃ ἄλλος ἄλλοσ ἄν ἄρα ἅμα ἐάν ἐγώ ἐγὼ ἐκ ἐμόσ ἐμὸς ἐν ἐξ ἐπί ἐπεὶ ἐπὶ ἐστι ἐφ ἐὰν ἑαυτοῦ ἔτι ἡ ἢ ἣ ἤ ἥ ἧς ἵνα ὁ ὃ ὃν ὃς ὅ ὅδε ὅθεν ὅπερ ὅς ὅσ ὅστις ὅστισ ὅτε ὅτι ὑμόσ ὑπ ὑπέρ ὑπό ὑπὲρ ὑπὸ ὡς ὡσ ὥς ὥστε ὦ ᾧ \""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#List first 10 English stopwords\n",
        "stopwords.words('english')[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URw5kJmJ29sW",
        "outputId": "0919fc4e-839a-49fb-c7f1-c03f7f72cdd1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use .sents() which returns sentences. However, in our particular case, this will cause an error:"
      ],
      "metadata": {
        "id": "SwhMC1Lt3Gr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.sents('greek')"
      ],
      "metadata": {
        "id": "62ANltCg3D7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error is because the stopwords corpus reader is of type WordListCorpusReader so there are no sentences. It's the same for .paras()."
      ],
      "metadata": {
        "id": "XYocPA8t3mdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of stopwords in English and Greek. There is a total of 444 Greek and English stop words\n",
        "len(stopwords.words(['english', 'greek'])) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnkkt54N3rG3",
        "outputId": "6f56a5de-4930-44fb-a4fb-eeaaa8c8820e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "444"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Classifying texts based on stopwords:*** Loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words"
      ],
      "metadata": {
        "id": "1cf_06_2303X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create empty list to store ratio of stopwords in each language\n",
        "language_ratios = {}\n",
        "\n",
        "# lowercase all tokens in test_tokens (\"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\")\n",
        "test_words = [word.lower() for word in test_tokens] \n",
        "test_words_set = set(test_words)\n",
        "\n",
        "#Iterate through test_tokens and output number of stopwords in each language the string contains\n",
        "for language in stopwords.fileids():\n",
        "    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
        "    common_elements = test_words_set.intersection(stopwords_set)\n",
        "    language_ratios[language] = len(common_elements) # language \"score\"\n",
        "    \n",
        "language_ratios"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7GSjR5-32Kg",
        "outputId": "d5398f47-d033-497d-e435-e2801ad90b7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arabic': 0,\n",
              " 'azerbaijani': 0,\n",
              " 'bengali': 0,\n",
              " 'danish': 3,\n",
              " 'dutch': 0,\n",
              " 'english': 8,\n",
              " 'finnish': 0,\n",
              " 'french': 2,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 0,\n",
              " 'italian': 1,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 3,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 2,\n",
              " 'russian': 0,\n",
              " 'slovene': 2,\n",
              " 'spanish': 1,\n",
              " 'swedish': 2,\n",
              " 'tajik': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are determining the language for which the most stopwords were found in the string\n",
        "# The key parameter to the max() function is a function that computes a key. \n",
        "# In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\n",
        "most_rated_language = max(language_ratios, key=language_ratios.get) \n",
        "most_rated_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LmpdSKIo5QUa",
        "outputId": "13ae8ef6-e157-4ed6-f212-eb6d93d20900"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'english'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print stopwords in most rated language appearing in string (here it's English)\n",
        "test_words_set.intersection(set(stopwords.words(most_rated_language)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNeSJ2wj5Ca7",
        "outputId": "cba7b104-56bd-4b12-8b60-5891e31a51a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ain', 'for', 'i', 'it', 's', 't', 'to', 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.3: Language Identifier Using Word Bigrams** \n",
        "\n",
        "Based on [this language identifier program on Github.](https://github.com/asif31iqbal/language-identifier)"
      ],
      "metadata": {
        "id": "3m6tGv3l6DrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries \n",
        "import pickle\n",
        "import string\n",
        "import os\n",
        "from nltk import ngrams, FreqDist, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from numpy import arange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#Create tokenizer method\n",
        "def ultimate_tokenize(sentence):\n",
        "    # Remove punctuation and digits\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    return word_tokenize(sentence.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmzBMGUK6jSP",
        "outputId": "95560226-75cf-4d16-937d-605573e8757b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generating Word Tokens:*** Divide text into one-word slices; typically followed by frequency calcuations to determine language of origin"
      ],
      "metadata": {
        "id": "TAmj_wzw72OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate word tokens from sample string\n",
        "simple_example_text = 'Oh, then, I see Queen Mab hath been with you.'\n",
        "\n",
        "simple_example_tokens_words = ultimate_tokenize(simple_example_text)\n",
        "simple_example_tokens_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb5qEsaK61As",
        "outputId": "500e242f-c3ef-47d8-eee0-763584e2417f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh', 'then', 'i', 'see', 'queen', 'mab', 'hath', 'been', 'with', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Break word token into characters and print characters in token in position 0\n",
        "simple_example_tokens_chars = list(simple_example_tokens_words[0])\n",
        "simple_example_tokens_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C-wd5Cm7G4s",
        "outputId": "2bd9f38b-6145-4fc6-9cdf-d20109d579a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o', 'h']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate and print list of 1-word word tokens (unigrams)\n",
        "simple_example_tokens_words_unigrams = list(ngrams(simple_example_tokens_words, 1))\n",
        "simple_example_tokens_words_unigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVsn48ad7PNP",
        "outputId": "e7335262-fba4-4aa0-e760-f34f7022ef2a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('oh',),\n",
              " ('then',),\n",
              " ('i',),\n",
              " ('see',),\n",
              " ('queen',),\n",
              " ('mab',),\n",
              " ('hath',),\n",
              " ('been',),\n",
              " ('with',),\n",
              " ('you',)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate and print list of 2-word word tokends (bigrams)\n",
        "simple_example_tokens_words_bigrams = list(ngrams(simple_example_tokens_words, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))\n",
        "simple_example_tokens_words_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJy9tj8t7fOP",
        "outputId": "b5f375df-7443-4763-b4fb-1e2fc8d19050"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_', 'oh'),\n",
              " ('oh', 'then'),\n",
              " ('then', 'i'),\n",
              " ('i', 'see'),\n",
              " ('see', 'queen'),\n",
              " ('queen', 'mab'),\n",
              " ('mab', 'hath'),\n",
              " ('hath', 'been'),\n",
              " ('been', 'with'),\n",
              " ('with', 'you'),\n",
              " ('you', '_')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create frequency distribution of word token unigrams\n",
        "fdist = FreqDist(simple_example_tokens_words_unigrams)\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zNCVNOk8RvU",
        "outputId": "352a3f35-7323-40fb-c003-ee1fc2b71caf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('been',): 1,\n",
              "          ('hath',): 1,\n",
              "          ('i',): 1,\n",
              "          ('mab',): 1,\n",
              "          ('oh',): 1,\n",
              "          ('queen',): 1,\n",
              "          ('see',): 1,\n",
              "          ('then',): 1,\n",
              "          ('with',): 1,\n",
              "          ('you',): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary for the unigrams; keys = word tokens, values = frequency counts\n",
        "unigram_dict = dict()\n",
        "for k, v in fdist.items():\n",
        "        unigram_dict[' '.join(k)] = v\n",
        "unigram_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-KpM5A48f9c",
        "outputId": "6a7cf027-9b40-41e1-90e2-efb965e47abe"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'been': 1,\n",
              " 'hath': 1,\n",
              " 'i': 1,\n",
              " 'mab': 1,\n",
              " 'oh': 1,\n",
              " 'queen': 1,\n",
              " 'see': 1,\n",
              " 'then': 1,\n",
              " 'with': 1,\n",
              " 'you': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file = 'ngram_langid_files/LangId.train.English.txt'\n",
        "with open(file, encoding='utf8') as f:\n",
        "        content = f.read().lower()\n",
        "content.replace('\\n', '')[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "TfQH3LkV94A0",
        "outputId": "ffadcb90-8c7a-4baa-ed14-b9ccbd72bed8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-73e7b4785907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ngram_langid_files/LangId.train.English.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ngram_langid_files/LangId.train.English.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ngram_langid_files/English.unigram.pickle', 'rb') as handle:\n",
        "    unigram_english_dict = pickle.load(handle)\n",
        "unigram_english_dict"
      ],
      "metadata": {
        "id": "zEL8tw6D9_La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ngram_langid_files/English.bigram.pickle', 'rb') as handle:\n",
        "    bigram_english_dict = pickle.load(handle)\n",
        "bigram_english_dict"
      ],
      "metadata": {
        "id": "CmF90L4j-Ecq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bigram_english_dict.get('of the')"
      ],
      "metadata": {
        "id": "LS6_3qDA-KXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import operator\n",
        "english_unigram_freqs = sorted(unigram_english_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "english_unigram_freqs[:10]"
      ],
      "metadata": {
        "id": "wxsd6SPT-QaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels, values = zip(*english_unigram_freqs[:10])\n",
        "indexes = arange(len(labels))\n",
        "width = 0.8 # width = 1 would give bars that overlap because they are too close.\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))                                                               \n",
        "ax = fig.gca() # Get current axis\n",
        "rects = ax.bar(indexes, values, width)\n",
        "\n",
        "# Add title and axis labels\n",
        "fig.suptitle('Top 10 English word unigrams', fontsize=20)\n",
        "plt.xlabel('Word unigram', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Display value of each bar on bar\n",
        "for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x() + rect.get_width() / 2., 50 + height, '%d' % int(height), ha='center', va='bottom') # Can also add color and fontweight arguments.\n",
        "\n",
        "# Remove the default x-axis tick numbers and use tick numbers of your own choosing:\n",
        "ax.set_xticks(indexes)\n",
        "# Replace the tick numbers with strings:\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "plt.show()\n",
        "# plt.savefig('top10EnglishWordUnigrams.png')"
      ],
      "metadata": {
        "id": "rq4j7gVm-TtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J3vTuX7b9-iz"
      }
    }
  ]
}