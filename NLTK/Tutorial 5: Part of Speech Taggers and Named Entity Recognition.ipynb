{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 3-3: Part of Speech Taggers and Named Entity Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMV48s8TJi89EnRdBQadkM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Tutorial%205%3A%20Part%20of%20Speech%20Taggers%20and%20Named%20Entity%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial 5: Part of Speech Taggers and Named Entity Recognition"
      ],
      "metadata": {
        "id": "OjaJkuTM7U1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Creating a POS Tagger:*** Create a tagger that will identify parts of speech in a given sentence. \n",
        "\n",
        "Train a classifier to work out which suffixes are most informative for POS tagging. \n",
        "\n",
        "We can begin by finding out what the most common suffixes are"
      ],
      "metadata": {
        "id": "KMvKHMH378LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import brown corpus and frequency distribution module\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "from nltk import FreqDist\n",
        "\n",
        "#Determine most frequent suffixes in brown corpus (frequency of last 1, 2, 3 characters in words in brown corpus)\n",
        "suffix_fdist = FreqDist()\n",
        "for word in brown.words():\n",
        "    word = word.lower()\n",
        "    suffix_fdist[word[-1:]] += 1\n",
        "    suffix_fdist[word[-2:]] += 1\n",
        "    suffix_fdist[word[-3:]] += 1\n",
        "    \n",
        "suffix_fdist"
      ],
      "metadata": {
        "id": "poVKLWHW7TwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxGLoBgc7Fc2",
        "outputId": "d021f11a-908f-4ded-bd50-450b4475204e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Put 100 most common suffixes into list and print the top 10\n",
        "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
        "common_suffixes[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
      ],
      "metadata": {
        "id": "KlGFqYd3-M0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_features(word):\n",
        "    features = {}\n",
        "    for suffix in common_suffixes:\n",
        "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
        "    return features\n",
        "\n",
        "pos_features('test')"
      ],
      "metadata": {
        "id": "9o4K50Cm-PfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've defined our feature extractor, we can use it to train a new decision tree classifier:"
      ],
      "metadata": {
        "id": "eOnbZVJ5-nuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = brown.tagged_words(categories='news')\n",
        "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
        "featuresets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x2mgkFt-ogI",
        "outputId": "d6051b3c-83a6-42e2-e779-0a3260bbf1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"endswith('')\": False,\n",
              "  \"endswith(')\": False,\n",
              "  \"endswith('s)\": False,\n",
              "  'endswith(()': False,\n",
              "  'endswith())': False,\n",
              "  'endswith(,)': False,\n",
              "  'endswith(--)': False,\n",
              "  'endswith(.)': False,\n",
              "  'endswith(:)': False,\n",
              "  'endswith(;)': False,\n",
              "  'endswith(?)': False,\n",
              "  'endswith(`)': False,\n",
              "  'endswith(``)': False,\n",
              "  'endswith(a)': False,\n",
              "  'endswith(ad)': False,\n",
              "  'endswith(al)': False,\n",
              "  'endswith(an)': False,\n",
              "  'endswith(and)': False,\n",
              "  'endswith(are)': False,\n",
              "  'endswith(as)': False,\n",
              "  'endswith(at)': False,\n",
              "  'endswith(ay)': False,\n",
              "  'endswith(be)': False,\n",
              "  'endswith(by)': False,\n",
              "  'endswith(c)': False,\n",
              "  'endswith(ce)': False,\n",
              "  'endswith(ch)': False,\n",
              "  'endswith(d)': False,\n",
              "  'endswith(e)': True,\n",
              "  'endswith(ed)': False,\n",
              "  'endswith(en)': False,\n",
              "  'endswith(ent)': False,\n",
              "  'endswith(er)': False,\n",
              "  'endswith(ere)': False,\n",
              "  'endswith(ers)': False,\n",
              "  'endswith(es)': False,\n",
              "  'endswith(ey)': False,\n",
              "  'endswith(f)': False,\n",
              "  'endswith(for)': False,\n",
              "  'endswith(g)': False,\n",
              "  'endswith(h)': False,\n",
              "  'endswith(had)': False,\n",
              "  'endswith(hat)': False,\n",
              "  'endswith(he)': True,\n",
              "  'endswith(her)': False,\n",
              "  'endswith(his)': False,\n",
              "  'endswith(i)': False,\n",
              "  'endswith(in)': False,\n",
              "  'endswith(ing)': False,\n",
              "  'endswith(ion)': False,\n",
              "  'endswith(is)': False,\n",
              "  'endswith(it)': False,\n",
              "  'endswith(ith)': False,\n",
              "  'endswith(k)': False,\n",
              "  'endswith(l)': False,\n",
              "  'endswith(ld)': False,\n",
              "  'endswith(le)': False,\n",
              "  'endswith(ll)': False,\n",
              "  'endswith(ly)': False,\n",
              "  'endswith(m)': False,\n",
              "  'endswith(me)': False,\n",
              "  'endswith(n)': False,\n",
              "  'endswith(nce)': False,\n",
              "  'endswith(nd)': False,\n",
              "  'endswith(ne)': False,\n",
              "  'endswith(ng)': False,\n",
              "  'endswith(ns)': False,\n",
              "  'endswith(nt)': False,\n",
              "  'endswith(o)': False,\n",
              "  'endswith(of)': False,\n",
              "  'endswith(om)': False,\n",
              "  'endswith(on)': False,\n",
              "  'endswith(or)': False,\n",
              "  'endswith(ot)': False,\n",
              "  'endswith(p)': False,\n",
              "  'endswith(r)': False,\n",
              "  'endswith(re)': False,\n",
              "  'endswith(rs)': False,\n",
              "  'endswith(ry)': False,\n",
              "  'endswith(s)': False,\n",
              "  'endswith(se)': False,\n",
              "  'endswith(so)': False,\n",
              "  'endswith(ss)': False,\n",
              "  'endswith(st)': False,\n",
              "  'endswith(t)': False,\n",
              "  'endswith(te)': False,\n",
              "  'endswith(ted)': False,\n",
              "  'endswith(ter)': False,\n",
              "  'endswith(th)': False,\n",
              "  'endswith(the)': True,\n",
              "  'endswith(to)': False,\n",
              "  'endswith(ts)': False,\n",
              "  'endswith(ty)': False,\n",
              "  'endswith(uld)': False,\n",
              "  'endswith(ut)': False,\n",
              "  'endswith(ve)': False,\n",
              "  'endswith(w)': False,\n",
              "  'endswith(was)': False,\n",
              "  'endswith(we)': False,\n",
              "  'endswith(y)': False},\n",
              " 'AT')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import decision tree classifier and accuracy\n",
        "from nltk import DecisionTreeClassifier\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "#Set cutoff limit for classifier and training and test set variables\n",
        "cutoff = int(len(featuresets) * 0.1)\n",
        "train_set, test_set = featuresets[cutoff:], featuresets[:cutoff]"
      ],
      "metadata": {
        "id": "iTYi2mDW--3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run classifer on training set\n",
        "#NLTK is a teaching toolkit which is not really optimized for speed. \n",
        "#Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n",
        "classifier = DecisionTreeClassifier.train(train_set) "
      ],
      "metadata": {
        "id": "7mXC0-ml_U2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "rLYIzFZn_wHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(classifier, test_set)"
      ],
      "metadata": {
        "id": "bhJbLlax_hW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(pos_features('cats'))"
      ],
      "metadata": {
        "id": "VW9J9lMz_lBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.pseudocode(depth=4)"
      ],
      "metadata": {
        "id": "_AEN3dGkAgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the classifier, we can add contextual features:\n",
        "\n",
        "def pos_features(sentence, i): [1]\n",
        "    features = {\"suffix(1)\": sentence[i][-1:],\n",
        "                \"suffix(2)\": sentence[i][-2:],\n",
        "                \"suffix(3)\": sentence[i][-3:]}\n",
        "    if i == 0:\n",
        "        features[\"prev-word\"] = \"<START>\"\n",
        "    else:\n",
        "        features[\"prev-word\"] = sentence[i-1]\n",
        "    return features\n",
        "Then, instead of working with tagged words, we work with tagged sentences:\n",
        "\n",
        "tagged_sents = brown.tagged_sents(categories='news')\n",
        "We can then improve this further by adding more features such as prev-tag etc."
      ],
      "metadata": {
        "id": "IMtv_V1-AlEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parts of Speech and Meaning (English Only)***"
      ],
      "metadata": {
        "id": "W3EF0gKkAuiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create string\n",
        "t = \"Cyprus, officially the Republic of Cyprus, is an island country in the Eastern Mediterranean and the third largest and third most populous island in the Mediterranean. Cyprus is located south of Turkey, west of Syria and Lebanon, northwest of Israel, north of Egypt, and southeast of Greece. Cyprus is a major tourist destination in the Mediterranean. With an advanced, high-income economy and a very high Human Development Index, the Republic of Cyprus has been a member of the Commonwealth since 1961 and was a founding member of the Non-Aligned Movement until it joined the European Union on 1 May 2004. On 1 January 2008, the Republic of Cyprus joined the eurozone.\"\n",
        "\n",
        "#Import word tokenizer\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "sentences = sent_tokenize(t.lower())\n",
        "sentences\n",
        "\n",
        "#Tokenize words in t and print tokens in second sentence\n",
        "tokens = word_tokenize(sentences[2])\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOZRzfGgBC-X",
        "outputId": "7fd0a043-027c-4777-8d17-a233d49578cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cyprus',\n",
              " 'is',\n",
              " 'a',\n",
              " 'major',\n",
              " 'tourist',\n",
              " 'destination',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mediterranean',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import part of speech tagger from nltk and tag tokens in string t \n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tags = pos_tag(tokens)\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfGq1L7nB5lZ",
        "outputId": "3ba14aa8-0dfa-4759-a979-de3e8c1ea08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cyprus', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('major', 'JJ'),\n",
              " ('tourist', 'NN'),\n",
              " ('destination', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('mediterranean', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Access documentation for tags, for example for NN:\n",
        "import nltk.help\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhEmLVvQCUQK",
        "outputId": "f8bf6243-ff8c-4dc5-b876-168d31f9e19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word senses for homonyms***\n",
        "\n",
        "WordNet is a lexical database for the English language in the form of a semantic graph.\n",
        "\n",
        "WordNet groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members.\n",
        "\n",
        "NLTK provides an interface to the WordNet API."
      ],
      "metadata": {
        "id": "0KvtmtcUHVOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download wordnet and list set of synonyms (synset) for \"human\"\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('human')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcL1dW76HUsH",
        "outputId": "e97d8f25-2333-4765-edd8-c9f62353ef61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('homo.n.02'),\n",
              " Synset('human.a.01'),\n",
              " Synset('human.a.02'),\n",
              " Synset('human.a.03')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get first definition human from synset\n",
        "wn.synsets('human')[0].definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "exkSfSrhH3RX",
        "outputId": "06ecb9db-912a-433d-9a7f-988dc0c1011f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'any living or extinct member of the family Hominidae characterized by superior intelligence, articulate speech, and erect carriage'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get second definition human from synset\n",
        "wn.synsets('human')[1].definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AaQEs8rRH6Ot",
        "outputId": "96ccc1ba-fea9-4be7-dc7a-7e9e71bac23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'characteristic of humanity'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define variable \"human\" as \"human\" in synset\n",
        "human = wn.synsets('Human', pos=wn.NOUN)[0]\n",
        "human"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqYmIyitIJ_j",
        "outputId": "b5e1b951-29bb-4e1a-90d9-68b5305f4858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('homo.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A hypernym is a word with a broad meaning constituting a category into which words with more specific meanings fall a superordinate. \n",
        "# For example, colour is a hypernym of red.\n",
        "human.hypernyms() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFs0JpDnIe04",
        "outputId": "aad37ba5-e0c2-4398-9764-1eab1c35992a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('hominid.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human.hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4qvPPzKImG_",
        "outputId": "db30bf24-797e-4587-d66c-d73cdeaafb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('homo_erectus.n.01'),\n",
              " Synset('homo_habilis.n.01'),\n",
              " Synset('homo_sapiens.n.01'),\n",
              " Synset('homo_soloensis.n.01'),\n",
              " Synset('neandertal_man.n.01'),\n",
              " Synset('rhodesian_man.n.01'),\n",
              " Synset('world.n.08')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bike = wn.synsets('bicycle')[0]\n",
        "bike"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wt79NhtIonc",
        "outputId": "f3591326-5e20-4a0b-a825-0a7fef92be5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('bicycle.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "girl = wn.synsets('girl')[1]\n",
        "girl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9wnUKrvIqWG",
        "outputId": "8e5f2eff-c0d5-4ef2-f8eb-e256921832cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('female_child.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Wu-Palmer metric (WUP) is a measure of similarity based on distance in the graph. There are many other metrics too.\n",
        "#Get similarity between bike and human\n",
        "bike.wup_similarity(human) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-d3aVqvIrCY",
        "outputId": "8ebe1005-f01d-46d0-9fd7-bcaf8a685569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34782608695652173"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get similarity between girl and human\n",
        "girl.wup_similarity(human)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZzN1zVHIwI8",
        "outputId": "19925ac5-a3e2-4de0-bbbc-74648878492f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5217391304347826"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get synonyms for 'girl'\n",
        "synonyms = []\n",
        "for syn in wn.synsets('girl'):\n",
        "    # A lemma is basically the dictionary form or base form of a word, as opposed to the various inflected forms of a word. \n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.append(lemma.name())\n",
        "synonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J28Jh2dEI5CN",
        "outputId": "fa3e7049-a0ff-44d2-a2a7-ad7087c2a562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['girl',\n",
              " 'miss',\n",
              " 'missy',\n",
              " 'young_lady',\n",
              " 'young_woman',\n",
              " 'fille',\n",
              " 'female_child',\n",
              " 'girl',\n",
              " 'little_girl',\n",
              " 'daughter',\n",
              " 'girl',\n",
              " 'girlfriend',\n",
              " 'girl',\n",
              " 'lady_friend',\n",
              " 'girl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get antonyms for 'girl'\n",
        "antonyms = []\n",
        "for syn in wn.synsets(\"girl\"):\n",
        "    for l in syn.lemmas():\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "antonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwW28ifjI99E",
        "outputId": "9e0b9a69-0cc5-42e1-904d-7a6c3dc7ed05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['male_child', 'boy', 'son', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Chunking and Entity Recognition:***\n",
        "\n",
        "**Chunking:** Divide a sentence into chunks. Usually each chunk contains a head and (optionally) additional words and modifiers. Examples of chunks include noun groups and verb groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q20lcuLPJK0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser"
      ],
      "metadata": {
        "id": "mbvgaxULL9Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create a chunker, we need to first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n",
        "\n",
        "We can define a simple grammar for a noun phrase (NP) chunker with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN).\n",
        "\n",
        "Note how grammatical structures which are not noun phrases are not chunked, which is totally fine:"
      ],
      "metadata": {
        "id": "i4ascxpKMDV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "metadata": {
        "id": "wcem0OCoMLT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###DOES NOT WORK: no display name and no $DISPLAY environment variable\n",
        "\n",
        "chunker = RegexpParser(grammar)\n",
        "result = chunker.parse(tags)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "xVtdsAj6MMEx",
        "outputId": "018486a5-5923-426a-9d47-dd0d7be0f72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('cyprus', 'NN')]), ('is', 'VBZ'), Tree('NP', [('a', 'DT'), ('major', 'JJ'), ('tourist', 'NN')]), Tree('NP', [('destination', 'NN')]), ('in', 'IN'), Tree('NP', [('the', 'DT'), ('mediterranean', 'NN')]), ('.', '.')])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Entity Recognition:*** The goal of entity recogintion is to detect entities such as Person, Location, Time, etc."
      ],
      "metadata": {
        "id": "__WOUL17Mjv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###DOES NOT WORK: no display name and no $DISPLAY environment variable\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.chunk import ne_chunk # ne = named entity\n",
        "ne_chunk(tags)"
      ],
      "metadata": {
        "id": "UTqRaDP0Mj6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note ne_chunk was unable to detect any entities in our sentence. That is because it is quite limited, being able to recognize only the following entities:\n",
        "\n",
        "FACILITY, GPE (Geo-Political Entity), GSP (Geo-Socio-Political group), LOCATION, ORGANIZATION, PERSON"
      ],
      "metadata": {
        "id": "aG1DariZNEhQ"
      }
    }
  ]
}