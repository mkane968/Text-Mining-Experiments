{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK Master Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNC0+Ofi++B73Ps+6cjx09T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Intro_to_NLTK_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intro to NLTK Tutorial"
      ],
      "metadata": {
        "id": "DvUDzDlniodd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P9as4d4O1SR"
      },
      "source": [
        "##Download  Libraries, Packages and Corpora\n",
        "\n",
        "Install and import necessary libraries, packages and corpora to prepare for text analysis (need to run ! pip install for those not in Colab by default)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all libraries"
      ],
      "metadata": {
        "id": "COu_MvlZB_JI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUjQB9_ZNeTS"
      },
      "source": [
        "import nltk # https://www.nltk.org/install.html\n",
        "import numpy # https://www.scipy.org/install.html\n",
        "import matplotlib.pyplot as plt # https://matplotlib.org/downloads.html #Need to standardize this\n",
        "!pip install unidecode\n",
        "import unidecode # https://pypi.python.org/pypi/Unidecode\n",
        "!pip install langdetect\n",
        "import langdetect # https://pypi.python.org/pypi/langdetect\n",
        "!pip install langid\n",
        "import langid # https://github.com/saffsd/langid.py\n",
        "import gensim # https://radimrehurek.com/gensim/install.html\n",
        "import sys\n",
        "import operator\n",
        "import pickle\n",
        "import string \n",
        "# Contains string constants eg. ascii_lowercase which is 'a...z', string formatting functions, other string functions like .capwords() and .translate().\n",
        "import os\n",
        "import nltk.help\n",
        "import random\n",
        "import collections\n",
        "%matplotlib inline\n",
        "import itertools\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages"
      ],
      "metadata": {
        "id": "QgpYFCv0lqw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "from nltk.chunk import RegexpParser, ne_chunk # ne = named entity\n",
        "from nltk.classify import accuracy\n",
        "from nltk.corpus import brown, gutenberg, movie_reviews, names, stopwords, subjectivity, words\n",
        "from nltk.metrics.scores import (precision, recall, f_measure)\n",
        "from nltk.metrics import edit_distance\n",
        "from nltk.sentiment.util import (mark_negation, extract_unigram_feats) \n",
        "# mark_negation(): Append _NEG suffix to words that appear in the scope between a negation and a punctuation mark. \n",
        "# extract_unigram_feats(): Populate a dictionary of unigram features, reflecting the presence/absence in the document of each of the tokens in unigrams.\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\n",
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "from nltk.text import Text\n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
        "from nltk import bigrams, ngrams, DecisionTreeClassifier, everygrams, FreqDist, MaxentClassifier, NaiveBayesClassifier, pos_tag, sent_tokenize, WordNetLemmatizer\n",
        "from numpy import arange\n",
        "from operator import itemgetter\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "import nltk.classify.util # Utility functions and classes for classifiers. Contains functions such as accuracy(classifier, gold)"
      ],
      "metadata": {
        "id": "aY5Bm7YflpAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download NLTK corpora"
      ],
      "metadata": {
        "id": "NL9QYinozkF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('names')\n",
        "nltk.download('punkt')\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('subjectivity')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora/."
      ],
      "metadata": {
        "id": "Ueih18iMzj6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMVw3A92Pe2H"
      },
      "source": [
        "## Plain Text Analysis\n",
        "\n",
        "Conduct basic tokenization and text analysis using NLTK.text module (concordancing, collocations, counting and indexing, distributional similarity, dispersion plot, plot, vocab, common contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split string into tokens"
      ],
      "metadata": {
        "id": "dK8CVFsIlx-b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuFNSF8Pt1k"
      },
      "source": [
        "my_string = \"Two plus two is four, minus one that's three â€” quick maths. Every day man's on the block. Smoke trees. See your girl in the park, that girl is an uckers. When the thing went quack quack quack, your men were ducking! Hold tight Asznee, my brother. He's got a pumpy. Hold tight my man, my guy. He's got a frisbee. I trap, trap, trap on the phone. Moving that cornflakes, rice crispies. Hold tight my girl Whitney.\"\n",
        "tokens = word_tokenize(my_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercase tokens, then print first five tokens"
      ],
      "metadata": {
        "id": "JtIP4-74rEKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word.lower() for word in tokens]\n",
        "tokens[:5]"
      ],
      "metadata": {
        "id": "5cpM626lrEYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign text of tokens to variable t"
      ],
      "metadata": {
        "id": "f6HOF6pml27G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItLI4yBZQJ4G"
      },
      "source": [
        "t = Text(tokens)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOQ9TwmCQQ4y"
      },
      "source": [
        "This method of converting raw strings to NLTK Text instances can be used when reading text from a file. For instance: f = open('my-file.txt','rU') \n",
        "\n",
        "Opening a file with the mode 'U' or 'rU' will open a file for reading in universal newline mode. All three line ending conventions will be translated to a \"\\n\"\n",
        "\n",
        "raw = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jgKl9qmSjon"
      },
      "source": [
        "###Concordances \n",
        "Concordance() is a method of the Text class of NLTK. It finds words and displays a context window. Word matching is not case-sensitive.\n",
        "\n",
        "`Concordance() `is defined as follows: concordance(self, word, width=79, lines=25). Note default values for optional params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lE23TQbQOuP"
      },
      "source": [
        "t.concordance('uckers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYtLx_RpTAVW"
      },
      "source": [
        "###Collocations\n",
        "Find multiple words which commonly co-occur\n",
        "\n",
        "def collocations(self, num=20, window_size=2). num is the max no. of collocations to print."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO1vS3I5RhYt"
      },
      "source": [
        "t.collocations() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNB-d6iiU0nT"
      },
      "source": [
        "###Count\n",
        "Find frequency of word appearance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWbp36ZTqWJ"
      },
      "source": [
        "t.count('quack')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###List\n",
        "\n",
        "Make text a list of words"
      ],
      "metadata": {
        "id": "2RFJJJ0xOMJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']"
      ],
      "metadata": {
        "id": "EYL3sdE2OZuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you can get the length of the list, append a value to the list or add two lists together"
      ],
      "metadata": {
        "id": "FqAcH3zfOYmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent1))\n",
        "\n",
        "print(sent1.append(\"Some\"))\n",
        "\n",
        "['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail']"
      ],
      "metadata": {
        "id": "GX40ZLVeO7BF",
        "outputId": "dac80042-b7a8-4b71-8083-540e5cd13eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Index \n",
        "Find position of word in text"
      ],
      "metadata": {
        "id": "swMoLQARmXLt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3px_kHdUVEmd"
      },
      "source": [
        "t.index('two')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHFfz7UfTmRM"
      },
      "source": [
        "###Distributional Similarity\n",
        "Find other words which appear in the same contexts as the specified word; list most similar words first.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhGnuWmvTlY8"
      },
      "source": [
        "t.similar('brother') #similar(self, word, num=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drw2k8x0VXkV"
      },
      "source": [
        "###Dispersion Plot\n",
        "Find and plot instances of word(s) as distributed across text. Reveals patterns in word positions. Each stripe represents an instance of a word, and each row represents the entire text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx3zdwgfVwCI"
      },
      "source": [
        "t.dispersion_plot(['man', 'thing', 'quack']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBOtNw-1WMQY"
      },
      "source": [
        "###Frequency Plot\n",
        "Find and plot specified number of most common tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SInmqqkWJ7s"
      },
      "source": [
        "t.plot(20) # plots 20 most common tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQRXrFITWqTX"
      },
      "source": [
        "###Most Frequent Vocab\n",
        "Find frequency of each token in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSg-mOdaWmLw"
      },
      "source": [
        "t.vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXRsbd08YbEJ"
      },
      "source": [
        "###Common Contexts\n",
        "Given two words used similarly, displays where in the text they were used similarly\n",
        "\n",
        "Our text is too small, so we will use a bigger one.\n",
        "\n",
        "NLTK comes with several interesting corpora, which are large collections of text. You can check out what kinds of corpora are found at http://www.nltk.org/nltk_data/ \n",
        "\n",
        "reuters is a corpus of news documents. More specifically, reuters is a corpus reader for the Reuters corpus which provides us with methods to access the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iyxa6vLW1Oh"
      },
      "source": [
        "text = Text(reuters.words()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that .common_contexts() takes 2 words which are used similarly and displays where they are used similarly. It also seems that '_' indicates where the words would be in the text.\n"
      ],
      "metadata": {
        "id": "6bQXcSrsm5X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.common_contexts(['August', 'June']) "
      ],
      "metadata": {
        "id": "-D24KAPRm5et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Processing and Wrangling"
      ],
      "metadata": {
        "id": "YJxCT_K3GOoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "J_j8KgoL9Ewv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin by exploring reuters corpus by retrieving file ids and categories. Reuters is a corpus of news documents. More specifically, reuters is a corpus reader for the Reuters corpus which provides us with methods to access the corpus:"
      ],
      "metadata": {
        "id": "IMX1ry0n-Oh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace \\n with spaces in reuters corpus"
      ],
      "metadata": {
        "id": "ilZbXKygZQov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "Sf1WY-d_80wj",
        "outputId": "a38fd45d-085c-4daa-f772-34f8d3110268"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-396dbc14499c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reuters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ],
      "source": [
        "reuters.readme().replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve file ids in reuter's corpus"
      ],
      "metadata": {
        "id": "ru3rCBW0ZT1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.fileids()"
      ],
      "metadata": {
        "id": "ZStmIYoe-SuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve file id in specific position in reuters corpus"
      ],
      "metadata": {
        "id": "yU9L6izPZWgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.fileids()[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GhVXa_4p-XaO",
        "outputId": "e3551c54-a11c-499b-dbf8-65812cec33cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'training/9995'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get length of files ids (number of files) in reuters corpus\n"
      ],
      "metadata": {
        "id": "YGHnnaU8ZZph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(reuters.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otaOaV9n-YK0",
        "outputId": "c582b450-2e5b-45bf-87a7-2b21733cc4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10788"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve reuters categories, delete # to view."
      ],
      "metadata": {
        "id": "mUhmysDQZb8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reuters.categories()"
      ],
      "metadata": {
        "id": "7ANjRxx6-7b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve sentences in specific file in reuters corpus"
      ],
      "metadata": {
        "id": "xFRA1zfTZeIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.sents('test/14826')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34dV_5P-_wuC",
        "outputId": "3ec6d4da-af28-4b2a-8476-877cbc30499d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving bigrams from files in Reuters corpus\n",
        "\n",
        "Load words in files under \"trade\" category into new variable and get length"
      ],
      "metadata": {
        "id": "mBskuQeI-dzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trade_words = reuters.words(categories='trade')\n",
        "len(trade_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2peV1kswAOf4",
        "outputId": "bd9e3dcd-a92b-4392-c893-b58629152291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142723"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put first 100 trade words in new variable"
      ],
      "metadata": {
        "id": "5SDRpX6HZj5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trade_words_condensed = trade_words[:100]\n",
        "trade_words_condensed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxgd0e3jAec8",
        "outputId": "5f2967f1-21fe-42a8-d2b3-513380b73590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stopwords from trade_words_condensed and lower case it"
      ],
      "metadata": {
        "id": "vKBsgd3OZrse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trade_words_condensed = [w.lower() for w in trade_words_condensed if w.lower() not in stopwords.words('english')]\n",
        "trade_words_condensed[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIRwerRrAl7V",
        "outputId": "2faed043-3328-4de4-a83f-7754d26289c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['asian',\n",
              " 'exporters',\n",
              " 'fear',\n",
              " 'damage',\n",
              " 'u',\n",
              " '.',\n",
              " '.-',\n",
              " 'japan',\n",
              " 'rift',\n",
              " 'mounting']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctuation"
      ],
      "metadata": {
        "id": "FHhRk5vtZxZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trade_words_condensed = [w for w in trade_words_condensed if w not in string.punctuation]\n",
        "punct_combo = [c + \"\\\"\" for c in string.punctuation ] + [\"\\\"\" + c for c in string.punctuation] + [\".-\", \":-\", \"..\", \"...\"]\n",
        "trade_words_condensed = [w for w in trade_words_condensed if w not in string.punctuation and w not in punct_combo]\n",
        "#trade_words_condensed\n"
      ],
      "metadata": {
        "id": "3HOYcJ3UA1IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get bigrams for words in cleaned trade word list"
      ],
      "metadata": {
        "id": "4xILoihvZz9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bi_trade_words_condensed = list(bigrams(trade_words_condensed))\n",
        "bi_trade_words_condensed[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qL-UjECA8g8",
        "outputId": "191446de-497d-4597-9f80-abbda226c701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('asian', 'exporters'),\n",
              " ('exporters', 'fear'),\n",
              " ('fear', 'damage'),\n",
              " ('damage', 'u'),\n",
              " ('u', 'japan')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create frequency distribution for bigrams in trade word list\n",
        "\n",
        "Print frequency of each word bigram"
      ],
      "metadata": {
        "id": "_5lXC4dgBMUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bi_fdist = FreqDist(bi_trade_words_condensed)\n",
        "\n",
        "for word, frequency in bi_fdist.most_common(3):\n",
        "    print(word, frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUInWWtrBPun",
        "outputId": "57821105-5399-4822-eb88-b3b3e17f30e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('u', 'japan') 2\n",
            "('asian', 'exporters') 1\n",
            "('exporters', 'fear') 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot frequency distribution of word bigrams"
      ],
      "metadata": {
        "id": "AfUn5bY5Z4qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bi_fdist.plot(3, cumulative=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Hq1yxG8ICRf2",
        "outputId": "b90388bd-6abd-46a0-dcaa-c9080be28d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stemming:*** Reducing inflected words to their stem, based on root form \n",
        "\n",
        "Load each stemmer as variable and specify language\n",
        "\n",
        "Test how each imported stemmer stems sample text"
      ],
      "metadata": {
        "id": "BpDW257HCnPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "print(porter.stem('Re-testing'), lancaster.stem('Re-testing'), snowball.stem('Re-testing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCZx2vf1CuJ3",
        "outputId": "e979ca15-5f54-4020-b5c9-e02a2d6a7019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re-test re-testing re-test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fun fact: SnowballStemmer can stem several other languages beside English.\n",
        "\n",
        "To make, for instance, a French stemmer, we can do the following: french_stemmer = SnowballStemmer('french')"
      ],
      "metadata": {
        "id": "Wu4ss5fzaDjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr2SjWOFDU_k",
        "outputId": "fbd954ce-4ced-4150-b010-17ab71f8dd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an alternative that creates a dictionary mapping of every character from string.punctuation to None (this will also work but creates a whole dictionary so is slower)\n",
        "\n",
        "This uses the 3-argument version of str.maketrans with arguments (x, y, z) where 'x' and 'y' must be equal-length strings and characters in 'x' are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character in the string is mapped to None"
      ],
      "metadata": {
        "id": "R_PFrLVfaH-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#translator = str.maketrans(dict.fromkeys(string.punctuation)\n",
        "\n",
        "sentence = \"So, we'll go no more a-roving. So late into the night, Though the heart be still as loving, And the moon be still as bright.\"\n",
        "\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYcPq_9WDYf0",
        "outputId": "88065ea3-febb-499b-dc79-a27af8d4b204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{33: None,\n",
              " 34: None,\n",
              " 35: None,\n",
              " 36: None,\n",
              " 37: None,\n",
              " 38: None,\n",
              " 39: None,\n",
              " 40: None,\n",
              " 41: None,\n",
              " 42: None,\n",
              " 43: None,\n",
              " 44: None,\n",
              " 45: None,\n",
              " 46: None,\n",
              " 47: None,\n",
              " 58: None,\n",
              " 59: None,\n",
              " 60: None,\n",
              " 61: None,\n",
              " 62: None,\n",
              " 63: None,\n",
              " 64: None,\n",
              " 91: None,\n",
              " 92: None,\n",
              " 93: None,\n",
              " 94: None,\n",
              " 95: None,\n",
              " 96: None,\n",
              " 123: None,\n",
              " 124: None,\n",
              " 125: None,\n",
              " 126: None}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(sentence.translate(translator))\n",
        "tokens[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke4-zZ4NDviB",
        "outputId": "a37bdd14-6cd1-4404-d466-85a4f9cc6016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So', 'well', 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for stemmer in [porter, lancaster, snowball]:\n",
        "    print([stemmer.stem(t) for t in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni08n2GdDzEV",
        "outputId": "71bc2a55-f021-4f7b-e72b-038fd6c89cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['So', 'well', 'go', 'no', 'more', 'arov', 'So', 'late', 'into', 'the', 'night', 'though', 'the', 'heart', 'be', 'still', 'as', 'love', 'and', 'the', 'moon', 'be', 'still', 'as', 'bright']\n",
            "['so', 'wel', 'go', 'no', 'mor', 'arov', 'so', 'lat', 'into', 'the', 'night', 'though', 'the', 'heart', 'be', 'stil', 'as', 'lov', 'and', 'the', 'moon', 'be', 'stil', 'as', 'bright']\n",
            "['so', 'well', 'go', 'no', 'more', 'arov', 'so', 'late', 'into', 'the', 'night', 'though', 'the', 'heart', 'be', 'still', 'as', 'love', 'and', 'the', 'moon', 'be', 'still', 'as', 'bright']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Lemmatization:*** Aims to achieve a similar base \"stem\" for a word, but aims to derive the genuine dictionary root word, not just a truncated version of the word."
      ],
      "metadata": {
        "id": "ogFONe2bD3-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default lemmatization method with the Python NLTK is the WordNet lemmatizer.\n"
      ],
      "metadata": {
        "id": "PkEcBVv2aO0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(wnl.lemmatize('brightening'), wnl.lemmatize('boxes'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEGjqDUBEIVQ",
        "outputId": "aed6011c-0172-4ba2-a6f3-1f3368732a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "brightening box\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw above, sometimes, if we try to lemmatize a word, it will end up with the same word. This is because the default part of speech is nouns.\n"
      ],
      "metadata": {
        "id": "tTT6WwHDaQvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl.lemmatize('brightening', pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fkBbTEAUEO0X",
        "outputId": "887d431f-4a3a-42fb-c159-fd1b36daa3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'brighten'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Derive N-Grams from Text\n",
        "\n",
        "Based on [N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)\n",
        "\n",
        "What are n-grams? See [here](https://cloudmark.github.io/Language-Detection/)."
      ],
      "metadata": {
        "id": "GTJI2-ALU05f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** divides strings of text into substrings of letters and apostrophes ONLY to prepare for n-gram analysis.\n",
        "\n",
        "First, lowercase text in string."
      ],
      "metadata": {
        "id": "RjaYkNAEVDOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLZvk_4ZUu1H"
      },
      "outputs": [],
      "source": [
        "s = \"Le temps est un grand maÃ®tre, dit-on, le malheur est qu'il tue ses Ã©lÃ¨ves.\"\n",
        "s = s.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use RegEx tokenizer to tokenize string."
      ],
      "metadata": {
        "id": "LMHInJLOoQDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(\"[a-zA-Z'`Ã©Ã¨Ã®]+\")\n",
        "s_tokenized = tokenizer.tokenize(s)\n",
        "s_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVBiQkhpVdzt",
        "outputId": "ad660060-b35e-43b9-c7f7-27d546ef1d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['le',\n",
              " 'temps',\n",
              " 'est',\n",
              " 'un',\n",
              " 'grand',\n",
              " 'maÃ®tre',\n",
              " 'dit',\n",
              " 'on',\n",
              " 'le',\n",
              " 'malheur',\n",
              " 'est',\n",
              " \"qu'il\",\n",
              " 'tue',\n",
              " 'ses',\n",
              " 'Ã©lÃ¨ves']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, **generating n-Grams**: find n-length slices of a longer string, typically overlapping/in sequence; can be used for language detection"
      ],
      "metadata": {
        "id": "wkuxxJDxVwuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list for generated ngrams"
      ],
      "metadata": {
        "id": "88NcW0AfVKlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_4grams = []\n",
        "\n"
      ],
      "metadata": {
        "id": "8MozGI4aV3LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate ngram for each word in tokenized string"
      ],
      "metadata": {
        "id": "ydXRLBwXVU_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in s_tokenized:\n",
        "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
        "#generated_4grams"
      ],
      "metadata": {
        "id": "8QgaST1fVVgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that generated_4grams needs flattening since it's supposed to be a list of 4-grams:"
      ],
      "metadata": {
        "id": "YfeR9ZBqV2l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
        "#generated_4grams[:10]"
      ],
      "metadata": {
        "id": "DTqLwYALY84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining n-grams (n = 4)\n",
        "Join 4grams into list of strings"
      ],
      "metadata": {
        "id": "-8lnKPwzaOGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ng_list_4grams = generated_4grams\n",
        "for idx, val in enumerate(generated_4grams):\n",
        "    ng_list_4grams[idx] = ''.join(val)\n",
        "#ng_list_4grams"
      ],
      "metadata": {
        "id": "N3540KisaNMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort n-grams by **how frequently** they appear within the text. \n",
        "\n",
        "First, create list for n-grams sorted by frequency, iterate through ngrams and add to freq_4grams list as many times as appearing in list of strings"
      ],
      "metadata": {
        "id": "--vvU1CZavQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_4grams = {}\n",
        "\n",
        "for ngram in ng_list_4grams:\n",
        "    if ngram not in freq_4grams:\n",
        "        freq_4grams.update({ngram: 1})\n",
        "    else:\n",
        "        ngram_occurrences = freq_4grams[ngram]\n",
        "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
        "      "
      ],
      "metadata": {
        "id": "FIvRnUepavYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The operator module (downloaded above) exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n"
      ],
      "metadata": {
        "id": "nF5zfRXzVrFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams."
      ],
      "metadata": {
        "id": "jytGIHefVve2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] \n",
        "#freq_4grams_sorted"
      ],
      "metadata": {
        "id": "hZWLI1QAVvl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain n-grams for multiple values of n (n = 1, 2, 3, 4)\n",
        "For the code below we need the raw sentence as opposed to the tokens."
      ],
      "metadata": {
        "id": "dPt3vftZbO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_clean = ' '.join(s_tokenized) \n",
        "s_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcvlXoFtbWk2",
        "outputId": "dba35f82-1848-41b7-b916-932677ab5feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"le temps est un grand maÃ®tre dit on le malheur est qu'il tue ses Ã©lÃ¨ves\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define ngram extractor generating uni-grams, bigrams, trigrams and 4-grams of string (range set to 1-4)"
      ],
      "metadata": {
        "id": "oRg1ND0AV2Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_extractor(sent):\n",
        "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
        "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
        "\n",
        "#ngram_extractor(s_clean)"
      ],
      "metadata": {
        "id": "imOf0UoEbrEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Finding Unusual Words in a Given Language"
      ],
      "metadata": {
        "id": "Wb-djpkr37F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create string and tokenize and lowercase string"
      ],
      "metadata": {
        "id": "_J33FZWyawaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kjsh10X3zHm",
        "outputId": "a1cd04cd-9b9d-4509-e909-76c36aeae5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['truly',\n",
              " 'kryptic',\n",
              " 'is',\n",
              " 'the',\n",
              " 'best',\n",
              " 'puzzle',\n",
              " 'game',\n",
              " '.',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'browser-based',\n",
              " 'and',\n",
              " 'free',\n",
              " '.',\n",
              " 'google',\n",
              " 'it',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "text = \"Truly Kryptic is the best puzzle game. It's browser-based and free. Google it.\"\n",
        "\n",
        "text_tokenized = word_tokenize(text.lower())\n",
        "text_tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the words corpus"
      ],
      "metadata": {
        "id": "DHEeEcbta2JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words.readme().replace('\\n', ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3ovxQeUE4sLF",
        "outputId": "32ad8576-501a-4faf-fe69-85c9e3b94669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Wordlists  en: English, http://en.wikipedia.org/wiki/Words_(Unix) en-basic: 850 English words: C.K. Ogden in The ABC of Basic English (1932) '"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List path name of words corpus"
      ],
      "metadata": {
        "id": "tiuXCayGa4NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNd1E4YR41lZ",
        "outputId": "c7c88833-94db-410b-d01e-965c33171ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WordListCorpusReader in '/root/nltk_data/corpora/words'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get file ids in words corpus"
      ],
      "metadata": {
        "id": "j3ZarXy9a6dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kowa3VY744Ex",
        "outputId": "cb6470ff-01e8-47dc-db04-950a219dae9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['en', 'en-basic']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get first ten words in en corpus"
      ],
      "metadata": {
        "id": "XkoQvHosa8TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words.words('en')[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK6ORciz47pK",
        "outputId": "7edd51cc-eea7-430f-c027-f9c82e6d5c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'a',\n",
              " 'aa',\n",
              " 'aal',\n",
              " 'aalii',\n",
              " 'aam',\n",
              " 'Aani',\n",
              " 'aardvark',\n",
              " 'aardwolf',\n",
              " 'Aaron']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get first 10 words in en-basic corpus"
      ],
      "metadata": {
        "id": "8YLCuroea-qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words.words('en-basic')[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "talSusAt49be",
        "outputId": "db58f809-b152-4d6c-f601-79072a967271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'a',\n",
              " 'able',\n",
              " 'about',\n",
              " 'account',\n",
              " 'acid',\n",
              " 'across',\n",
              " 'act',\n",
              " 'addition',\n",
              " 'adjustment']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get length of words in en corpus"
      ],
      "metadata": {
        "id": "5CsXy3FlbBC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(words.words('en'))"
      ],
      "metadata": {
        "id": "43ikDlZK5Apd",
        "outputId": "b7f467e4-8f6c-4178-a584-d80bc041fe6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235886"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get length of words in en-basic corpus"
      ],
      "metadata": {
        "id": "nghPi4vObDm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(words.words('en-basic'))"
      ],
      "metadata": {
        "id": "47gOM6qn5CdF",
        "outputId": "f2a5431e-b781-42d6-8d69-ae212b3fec24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "850"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find unusual words in given language"
      ],
      "metadata": {
        "id": "zID177O9bGfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = set(w.lower() for w in words.words())\n",
        "text_vocab = set(w.lower() for w in text_tokenized if w.isalpha()) # Note .isalpha() removes punctuation tokens. However, tokens with a hyphen like 'browser-based' are totally skipped over because .isalpha() would be false.\n",
        "unusual = text_vocab.difference(english_vocab)\n",
        "unusual"
      ],
      "metadata": {
        "id": "XKD3rjHC5EI6",
        "outputId": "0316d6ac-5287-4ef7-8cab-d11dd70c0786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'google'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Enrichment"
      ],
      "metadata": {
        "id": "IARqqu4FDq8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating a POS Tagger:\n",
        "Create a tagger that will identify parts of speech in a given sentence. \n",
        "\n",
        "Train a classifier to work out which suffixes are most informative for POS tagging. \n",
        "\n",
        "We can begin by finding out what the most common suffixes are"
      ],
      "metadata": {
        "id": "KMvKHMH378LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine most frequent suffixes in brown corpus (frequency of last 1, 2, 3 characters in words in brown corpus)\n"
      ],
      "metadata": {
        "id": "bcI9GhaJbX1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suffix_fdist = FreqDist()\n",
        "for word in brown.words():\n",
        "    word = word.lower()\n",
        "    suffix_fdist[word[-1:]] += 1\n",
        "    suffix_fdist[word[-2:]] += 1\n",
        "    suffix_fdist[word[-3:]] += 1\n",
        "    \n",
        "#suffix_fdist"
      ],
      "metadata": {
        "id": "jZsVGg25bX92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put 100 most common suffixes into list and print the top 10\n"
      ],
      "metadata": {
        "id": "-RmDnvTgbb7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxGLoBgc7Fc2",
        "outputId": "e1421228-8c6f-420c-b9fd-75d6c2384159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
        "common_suffixes[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
      ],
      "metadata": {
        "id": "KlGFqYd3-M0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_features(word):\n",
        "    features = {}\n",
        "    for suffix in common_suffixes:\n",
        "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
        "    return features\n",
        "\n",
        "#pos_features('test')"
      ],
      "metadata": {
        "id": "9o4K50Cm-PfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've defined our feature extractor, we can use it to train a new decision tree classifier:"
      ],
      "metadata": {
        "id": "eOnbZVJ5-nuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = brown.tagged_words(categories='news')\n",
        "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
        "#featuresets[0]"
      ],
      "metadata": {
        "id": "1x2mgkFt-ogI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set cutoff limit for classifier and training and test set variables"
      ],
      "metadata": {
        "id": "u6LCjvE7bpDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = int(len(featuresets) * 0.1)\n",
        "train_set, test_set = featuresets[cutoff:], featuresets[:cutoff]"
      ],
      "metadata": {
        "id": "4WPChi4mbpK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run classifer on training set\n",
        "\n",
        "NLTK is a teaching toolkit which is not really optimized for speed.\n",
        "\n",
        "Therefore, this may take forever. For speed, use scikit-learn for the classifiers."
      ],
      "metadata": {
        "id": "qQd68DaFbsHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = DecisionTreeClassifier.train(train_set) "
      ],
      "metadata": {
        "id": "7mXC0-ml_U2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "rLYIzFZn_wHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(classifier, test_set)"
      ],
      "metadata": {
        "id": "bhJbLlax_hW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(pos_features('cats'))"
      ],
      "metadata": {
        "id": "VW9J9lMz_lBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.pseudocode(depth=4)"
      ],
      "metadata": {
        "id": "_AEN3dGkAgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the classifier, we can add contextual features:\n",
        "\n",
        "def pos_features(sentence, i): [1]\n",
        "    features = {\"suffix(1)\": sentence[i][-1:],\n",
        "                \"suffix(2)\": sentence[i][-2:],\n",
        "                \"suffix(3)\": sentence[i][-3:]}\n",
        "    if i == 0:\n",
        "        features[\"prev-word\"] = \"<START>\"\n",
        "    else:\n",
        "        features[\"prev-word\"] = sentence[i-1]\n",
        "    return features\n",
        "Then, instead of working with tagged words, we work with tagged sentences:\n",
        "\n",
        "tagged_sents = brown.tagged_sents(categories='news')\n",
        "We can then improve this further by adding more features such as prev-tag etc."
      ],
      "metadata": {
        "id": "IMtv_V1-AlEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parts of Speech and Meaning (English Only)***"
      ],
      "metadata": {
        "id": "W3EF0gKkAuiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create string, import word tokenizer, tokenize words in t and print tokens in second sentence"
      ],
      "metadata": {
        "id": "tq5yjRexb0w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = \"Cyprus, officially the Republic of Cyprus, is an island country in the Eastern Mediterranean and the third largest and third most populous island in the Mediterranean. Cyprus is located south of Turkey, west of Syria and Lebanon, northwest of Israel, north of Egypt, and southeast of Greece. Cyprus is a major tourist destination in the Mediterranean. With an advanced, high-income economy and a very high Human Development Index, the Republic of Cyprus has been a member of the Commonwealth since 1961 and was a founding member of the Non-Aligned Movement until it joined the European Union on 1 May 2004. On 1 January 2008, the Republic of Cyprus joined the eurozone.\"\n",
        "\n",
        "sentences = sent_tokenize(t.lower())\n",
        "sentences\n",
        "\n",
        "tokens = word_tokenize(sentences[2])\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOZRzfGgBC-X",
        "outputId": "7fd0a043-027c-4777-8d17-a233d49578cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cyprus',\n",
              " 'is',\n",
              " 'a',\n",
              " 'major',\n",
              " 'tourist',\n",
              " 'destination',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mediterranean',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tag tokens in string t using pos tagger"
      ],
      "metadata": {
        "id": "Ryr_Jvtbb7Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pos_tag(tokens)\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfGq1L7nB5lZ",
        "outputId": "3ba14aa8-0dfa-4759-a979-de3e8c1ea08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cyprus', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('major', 'JJ'),\n",
              " ('tourist', 'NN'),\n",
              " ('destination', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('mediterranean', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access documentation for tags, for example for NN:"
      ],
      "metadata": {
        "id": "NdJQ-w1hb9Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhEmLVvQCUQK",
        "outputId": "f8bf6243-ff8c-4dc5-b876-168d31f9e19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word senses for homonyms***\n",
        "\n",
        "WordNet is a lexical database for the English language in the form of a semantic graph.\n",
        "\n",
        "WordNet groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members.\n",
        "\n",
        "NLTK provides an interface to the WordNet API."
      ],
      "metadata": {
        "id": "0KvtmtcUHVOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download wordnet and list set of synonyms (synset) for \"human\""
      ],
      "metadata": {
        "id": "7epspJ1pb_kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('human')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcL1dW76HUsH",
        "outputId": "e97d8f25-2333-4765-edd8-c9f62353ef61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('homo.n.02'),\n",
              " Synset('human.a.01'),\n",
              " Synset('human.a.02'),\n",
              " Synset('human.a.03')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get first definition human from synset"
      ],
      "metadata": {
        "id": "9Jd-5F1bcDJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('human')[0].definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "exkSfSrhH3RX",
        "outputId": "06ecb9db-912a-433d-9a7f-988dc0c1011f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'any living or extinct member of the family Hominidae characterized by superior intelligence, articulate speech, and erect carriage'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get second definition human from synset"
      ],
      "metadata": {
        "id": "SAXOy5dycHZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet.synsets('human')[1].definition()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AaQEs8rRH6Ot",
        "outputId": "96ccc1ba-fea9-4be7-dc7a-7e9e71bac23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'characteristic of humanity'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define variable \"human\" as \"human\" in synset"
      ],
      "metadata": {
        "id": "tIMtWLndcKFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human = wordnet.synsets('Human', pos=wn.NOUN)[0]\n",
        "human"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqYmIyitIJ_j",
        "outputId": "b5e1b951-29bb-4e1a-90d9-68b5305f4858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('homo.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A hypernym is a word with a broad meaning constituting a category into which words with more specific meanings fall a superordinate. \n"
      ],
      "metadata": {
        "id": "Uh7DqNYXcPtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, colour is a hypernym of red.\n",
        "human.hypernyms() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFs0JpDnIe04",
        "outputId": "aad37ba5-e0c2-4398-9764-1eab1c35992a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('hominid.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human.hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4qvPPzKImG_",
        "outputId": "db30bf24-797e-4587-d66c-d73cdeaafb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('homo_erectus.n.01'),\n",
              " Synset('homo_habilis.n.01'),\n",
              " Synset('homo_sapiens.n.01'),\n",
              " Synset('homo_soloensis.n.01'),\n",
              " Synset('neandertal_man.n.01'),\n",
              " Synset('rhodesian_man.n.01'),\n",
              " Synset('world.n.08')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bike = wordnet.synsets('bicycle')[0]\n",
        "bike"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wt79NhtIonc",
        "outputId": "f3591326-5e20-4a0b-a825-0a7fef92be5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('bicycle.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "girl = wordnet.synsets('girl')[1]\n",
        "girl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9wnUKrvIqWG",
        "outputId": "8e5f2eff-c0d5-4ef2-f8eb-e256921832cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('female_child.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wu-Palmer metric (WUP) is a measure of similarity based on distance in the graph. There are many other metrics too.\n",
        "\n",
        "Get similarity between bike and human"
      ],
      "metadata": {
        "id": "gog72RtIcVYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike.wup_similarity(human) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-d3aVqvIrCY",
        "outputId": "8ebe1005-f01d-46d0-9fd7-bcaf8a685569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34782608695652173"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get similarity between girl and human"
      ],
      "metadata": {
        "id": "NvCG8oYjcbNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "girl.wup_similarity(human)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZzN1zVHIwI8",
        "outputId": "19925ac5-a3e2-4de0-bbbc-74648878492f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5217391304347826"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get synonyms for 'girl'"
      ],
      "metadata": {
        "id": "gaM5vkXfcicS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = []\n",
        "for syn in wordnet.synsets('girl'):\n",
        "    # A lemma is basically the dictionary form or base form of a word, as opposed to the various inflected forms of a word. \n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.append(lemma.name())\n",
        "synonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J28Jh2dEI5CN",
        "outputId": "fa3e7049-a0ff-44d2-a2a7-ad7087c2a562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['girl',\n",
              " 'miss',\n",
              " 'missy',\n",
              " 'young_lady',\n",
              " 'young_woman',\n",
              " 'fille',\n",
              " 'female_child',\n",
              " 'girl',\n",
              " 'little_girl',\n",
              " 'daughter',\n",
              " 'girl',\n",
              " 'girlfriend',\n",
              " 'girl',\n",
              " 'lady_friend',\n",
              " 'girl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get antonyms for 'girl'"
      ],
      "metadata": {
        "id": "B5X3WtNYckLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "antonyms = []\n",
        "for syn in wordnet.synsets(\"girl\"):\n",
        "    for l in syn.lemmas():\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "antonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwW28ifjI99E",
        "outputId": "9e0b9a69-0cc5-42e1-904d-7a6c3dc7ed05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['male_child', 'boy', 'son', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chunking and Entity Recognition:\n",
        "\n",
        "**Chunking:** Divide a sentence into chunks. Usually each chunk contains a head and (optionally) additional words and modifiers. Examples of chunks include noun groups and verb groups.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q20lcuLPJK0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create a chunker, we need to first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n",
        "\n",
        "We can define a simple grammar for a noun phrase (NP) chunker with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN).\n",
        "\n",
        "Note how grammatical structures which are not noun phrases are not chunked, which is totally fine:"
      ],
      "metadata": {
        "id": "i4ascxpKMDV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "matplotlib.use('Agg')"
      ],
      "metadata": {
        "id": "wcem0OCoMLT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###DOES NOT WORK: no display name and no $DISPLAY environment variable\n",
        "\n",
        "chunker = RegexpParser(grammar)\n",
        "result = chunker.parse(tags)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "xVtdsAj6MMEx",
        "outputId": "018486a5-5923-426a-9d47-dd0d7be0f72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('cyprus', 'NN')]), ('is', 'VBZ'), Tree('NP', [('a', 'DT'), ('major', 'JJ'), ('tourist', 'NN')]), Tree('NP', [('destination', 'NN')]), ('in', 'IN'), Tree('NP', [('the', 'DT'), ('mediterranean', 'NN')]), ('.', '.')])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Entity Recognition:*** The goal of entity recogintion is to detect entities such as Person, Location, Time, etc."
      ],
      "metadata": {
        "id": "__WOUL17Mjv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###DOES NOT WORK: no display name and no $DISPLAY environment variable\n",
        "ne_chunk(tags)"
      ],
      "metadata": {
        "id": "UTqRaDP0Mj6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note ne_chunk was unable to detect any entities in our sentence. That is because it is quite limited, being able to recognize only the following entities:\n",
        "\n",
        "FACILITY, GPE (Geo-Political Entity), GSP (Geo-Socio-Political group), LOCATION, ORGANIZATION, PERSON"
      ],
      "metadata": {
        "id": "aG1DariZNEhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifiers"
      ],
      "metadata": {
        "id": "dCLLp6uWFA54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name Gender Identifier"
      ],
      "metadata": {
        "id": "vMmbC3Fdpft9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Building a feature extractor***\n",
        "\n",
        "An idea is to use the last letter of the name to predict the gender. For instance, names ending in a, e and i are likely to be female, while names ending in k, o, r, s and t are likely to be male."
      ],
      "metadata": {
        "id": "NO4My2LJplDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extractor which returns the last letter of a word"
      ],
      "metadata": {
        "id": "f7rtv2SrdFPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyRo-wL0pLUE",
        "outputId": "1667aac4-7361-493e-af5d-53d93e83fcde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'last_letter': 'n'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "def gender_features(word):\n",
        "    return {'last_letter': word[-1]}\n",
        "\n",
        "gender_features('John')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The returned dictionary is known as a feature set."
      ],
      "metadata": {
        "id": "H3Q1nzabpzwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the names corpus"
      ],
      "metadata": {
        "id": "EVBqXON8dI9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names.readme().replace('\\n', ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "0wEXt9xCpy5P",
        "outputId": "85536762-e9ed-4807-fbf6-e50429163c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Names Corpus, Version 1.3 (1994-03-29) Copyright (C) 1991 Mark Kantrowitz Additions by Bill Ross  This corpus contains 5001 female names and 2943 male names, sorted alphabetically, one per line.  You may use the lists of names for any purpose, so long as credit is given in any published work. You may also redistribute the list if you provide the recipients with a copy of this README file. The lists are not in the public domain (I retain the copyright on the lists) but are freely redistributable.  If you have any additions to the lists of names, I would appreciate receiving them.  Mark Kantrowitz <mkant+@cs.cmu.edu> http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the file ids in the names corpus"
      ],
      "metadata": {
        "id": "4AXgfnTxdMcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BelKDAvbqAsW",
        "outputId": "3590da9a-20f8-4356-8431-a17f77384776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['female.txt', 'male.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the first five words in the female text file in corpus"
      ],
      "metadata": {
        "id": "9kXia0eWdPpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names.words('female.txt')[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhCJ43SgqOjE",
        "outputId": "9e52ecdd-7fe0-4c9a-b5c4-8e83fa7dce8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the classifier, we need to prepare a list of examples and corresponding class labels."
      ],
      "metadata": {
        "id": "S7rXA_R8qXtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create list of labeled names where names in female.tx file are labeled female and male.txt names labeled male, print first five in labeled names list"
      ],
      "metadata": {
        "id": "8SvHdppddQl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_names = ([(name, 'female') for name in names.words('female.txt')] + [(name, 'male') for name in names.words('male.txt')])\n",
        "labeled_names[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSNbIqwdqX2n",
        "outputId": "a7c20f2d-f9ae-4a67-a919-4049811da94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Abagael', 'female'),\n",
              " ('Abagail', 'female'),\n",
              " ('Abbe', 'female'),\n",
              " ('Abbey', 'female'),\n",
              " ('Abbi', 'female')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shuffle the data so that we can split it by index into training and test data.\n"
      ],
      "metadata": {
        "id": "Sw_OLvZPdUlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(labeled_names) \n",
        "labeled_names[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkSwSWzyqpKL",
        "outputId": "20a5c4fc-e953-43c6-d3c0-c5a11afd40b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Norbert', 'male'),\n",
              " ('Stoddard', 'female'),\n",
              " ('Silvan', 'male'),\n",
              " ('Joete', 'female'),\n",
              " ('Nance', 'female')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create list of the last letter of each name in labeled names and corresponding gender, print first five\n"
      ],
      "metadata": {
        "id": "bVhfWK-sdW3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
        "featuresets[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqfCS2dkqpUF",
        "outputId": "933d0fee-579a-4754-baa0-76da69afe2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7944"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print length of feature sets"
      ],
      "metadata": {
        "id": "XrW7PvG3daCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(featuresets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZdjFioq-rQ",
        "outputId": "e526d11f-7d4d-43e6-aa0a-3771075cc295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7944"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into a training (80%) and test (20%) set:\n"
      ],
      "metadata": {
        "id": "ns6lv6wDdb7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SET_SIZE = round(len(featuresets) * .8)\n",
        "train_set, test_set = featuresets[:TRAIN_SET_SIZE], featuresets[TRAIN_SET_SIZE:]"
      ],
      "metadata": {
        "id": "kLKnVG7ddcC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also get the names in the test set, to be used later:"
      ],
      "metadata": {
        "id": "pjw4FYd0diEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_names = labeled_names[TRAIN_SET_SIZE:]"
      ],
      "metadata": {
        "id": "uJTvQ3KtdiMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classifier"
      ],
      "metadata": {
        "id": "kwfV5lswdmbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "rcr_L4I2rER0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " When working with large corpora, constructing a single list that contains the features of every instance can use up a large amount of memory. \n",
        " \n",
        "In these cases, use the function nltk.classify.apply_features, which returns an object that acts like a list but does not store all the feature sets in memory: \n",
        "\n",
        "from nltk.classify import apply_features\n",
        "\n",
        "train_names, test_names = labeled_names[:round(len(featuresets) * .8)], labeled_names[round(len(featuresets) * .8):]\n",
        "\n",
        "train_set = apply_features(gender_features, labeled_names[500:])\n",
        "\n",
        "test_set = apply_features(gender_features, labeled_names[:500])"
      ],
      "metadata": {
        "id": "jCbc5okldrYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prints likelihood ratios for most informative features"
      ],
      "metadata": {
        "id": "mmoCXBsxdx9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQd5cG2urOdY",
        "outputId": "42041e7a-e4d9-4418-95c4-909ea418bd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'a'            female : male   =     31.8 : 1.0\n",
            "             last_letter = 'k'              male : female =     27.6 : 1.0\n",
            "             last_letter = 'v'              male : female =     10.4 : 1.0\n",
            "             last_letter = 'p'              male : female =      9.7 : 1.0\n",
            "             last_letter = 'd'              male : female =      8.8 : 1.0\n",
            "             last_letter = 'o'              male : female =      8.6 : 1.0\n",
            "             last_letter = 'm'              male : female =      7.6 : 1.0\n",
            "             last_letter = 'r'              male : female =      6.9 : 1.0\n",
            "             last_letter = 'g'              male : female =      4.9 : 1.0\n",
            "             last_letter = 'z'              male : female =      4.6 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the classifer:"
      ],
      "metadata": {
        "id": "Kg01BMFurNoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get labels from classifer"
      ],
      "metadata": {
        "id": "FMohaVH-d1B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.labels()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p3CeEXBrXee",
        "outputId": "ada7c6e4-9681-4ea4-b646-89373dfa17dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['male', 'female']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get accuracy of classifer"
      ],
      "metadata": {
        "id": "HiJODipHd3C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(accuracy(classifier, test_set), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__oCJGdvreVQ",
        "outputId": "83f1d1b3-9965-43d7-bff9-b8805aabff73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classifier on female name based on last letter of name"
      ],
      "metadata": {
        "id": "aOd8Gxoqd6JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(gender_features('Aphrodite'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "58-rmN-1rjCS",
        "outputId": "6b20805c-bd0f-4e58-9399-d604d5571c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'female'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classifier on male name based on last letter of name"
      ],
      "metadata": {
        "id": "vb2nJFHpd71Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(gender_features('Zeus'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cWgeoyThroGP",
        "outputId": "68120fdf-4d2f-42df-8315-0a9888eb8289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'male'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a classifier with more features:"
      ],
      "metadata": {
        "id": "W_2a5I9rrtNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a classifier which lowercases first and last letter of word and identifies which letters are contained in word and at what frequency"
      ],
      "metadata": {
        "id": "PYKrKTtEd9Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features2(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "#gender_features2('John')"
      ],
      "metadata": {
        "id": "RXkJ422iryOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get features above for list of gendered names and put in list, print first item in list\n"
      ],
      "metadata": {
        "id": "l6b9sS6ueEMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets2 = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
        "#featuresets2[0]"
      ],
      "metadata": {
        "id": "bcpKrKMfsb5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train new classifier on same set of male and female names above and get accuracy\n"
      ],
      "metadata": {
        "id": "HMwva_UUeGW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set2, test_set2 = featuresets2[:TRAIN_SET_SIZE], featuresets2[TRAIN_SET_SIZE:]\n",
        "classifier2 = NaiveBayesClassifier.train(train_set2)\n",
        "round(accuracy(classifier2, test_set2), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZkjXjtusOtF",
        "outputId": "cec67aab-abb9-4cd3-cfbc-2c50ce02ed35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.79"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would have expected that having too many specific features on a small dataset would lead to overfitting, but it seems the classifier was good at avoiding that since its performance is slightly better.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gz97mbUDs1xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the most informative features for the new classifer\n"
      ],
      "metadata": {
        "id": "Ht57QYjHeHq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier2.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SIJEN7Gs1RQ",
        "outputId": "91b512e5-35c4-4165-c03c-103191270cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'a'            female : male   =     31.8 : 1.0\n",
            "             last_letter = 'k'              male : female =     27.6 : 1.0\n",
            "             last_letter = 'v'              male : female =     10.4 : 1.0\n",
            "             last_letter = 'p'              male : female =      9.7 : 1.0\n",
            "                count(v) = 2              female : male   =      8.9 : 1.0\n",
            "             last_letter = 'd'              male : female =      8.8 : 1.0\n",
            "             last_letter = 'o'              male : female =      8.6 : 1.0\n",
            "             last_letter = 'm'              male : female =      7.6 : 1.0\n",
            "             last_letter = 'r'              male : female =      6.9 : 1.0\n",
            "            first_letter = 'w'              male : female =      4.9 : 1.0\n",
            "             last_letter = 'g'              male : female =      4.9 : 1.0\n",
            "             last_letter = 'z'              male : female =      4.6 : 1.0\n",
            "             last_letter = 'b'              male : female =      4.4 : 1.0\n",
            "                count(a) = 3              female : male   =      4.3 : 1.0\n",
            "                count(w) = 1                male : female =      4.2 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, it seems the classifier is mainly using the last letter, along with some other features that happen to improve the accuracy."
      ],
      "metadata": {
        "id": "BBBM_QCLtlnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comparing the two classifiers using nltk.metrics***"
      ],
      "metadata": {
        "id": "Dqz0WE4Ctq2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, here's a useful function for comparing strings:\n",
        "\n",
        "Edit distance is the number of characters that need to be substituted, inserted, or deleted, to transform s1 into s2.\n"
      ],
      "metadata": {
        "id": "HF6jWhBfuBDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edit_distance(\"John\", \"Joan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQeLK08TtqFb",
        "outputId": "376cc7db-5091-4baf-a42e-71f2881be112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK metrics module provides functions for calculating metrics beyond mere accuracy. But in order to do so, we need to build 2 sets for each classification label: a reference set of correct values, and a test set of observed values."
      ],
      "metadata": {
        "id": "7HxTDP7it2QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier 1\n",
        "refsets = collections.defaultdict(set) # For what this is: https://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "# Classifier 2\n",
        "refsets2 = collections.defaultdict(set)\n",
        "testsets2 = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set2):\n",
        "    refsets2[label].add(i)\n",
        "    observed = classifier2.classify(feats)\n",
        "    testsets2[observed].add(i)"
      ],
      "metadata": {
        "id": "5cGcq6DAt1r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refsets"
      ],
      "metadata": {
        "id": "1IanDshYuWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsets"
      ],
      "metadata": {
        "id": "trsn3MnvuZ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can proceed to print the metrics for each classifier. \n",
        "\n",
        "However, we cannot get the accuracy in this manner because nltk.metrics.scores.accuracy(reference, test) works by comparing test[i] == reference[i] and our reference and test are not formatted in a way that allows for this.\n",
        "\n",
        "It's the same for the confusion matrix."
      ],
      "metadata": {
        "id": "FRlJlHvjeREN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = (\n",
        "    round(precision(refsets['female'], testsets['female']), 2),\n",
        "    round(precision(refsets['male'], testsets['male']), 2),\n",
        "    round(recall(refsets['female'], testsets['female']), 2),\n",
        "    round(recall(refsets['male'], testsets['male']), 2),\n",
        "    round(f_measure(refsets['female'], testsets['female']), 2),\n",
        "    round(f_measure(refsets['male'], testsets['male']), 2)\n",
        ")\n",
        "\n",
        "args2 = (\n",
        "    round(precision(refsets2['female'], testsets2['female']), 2),\n",
        "    round(precision(refsets2['male'], testsets2['male']), 2),\n",
        "    round(recall(refsets2['female'], testsets2['female']), 2),\n",
        "    round(recall(refsets2['male'], testsets2['male']), 2),\n",
        "    round(f_measure(refsets2['female'], testsets2['female']), 2),\n",
        "    round(f_measure(refsets2['male'], testsets2['male']), 2)\n",
        ")\n",
        "\n",
        "print('''\n",
        "CLASSIFIER 1\n",
        "------------ \n",
        "Female precision: {0}\n",
        "Male precision: {1}\n",
        "Female recall: {2}\n",
        "Male recall: {3}\n",
        "Female F1 score: {4}\n",
        "Male F1 score: {5}\n",
        "\n",
        "CLASSIFIER 2\n",
        "------------ \n",
        "Female precision: {6}\n",
        "Male precision: {7}\n",
        "Female recall: {8}\n",
        "Male recall: {9}\n",
        "Female F1 score: {10}\n",
        "Male F1 score: {11}\n",
        "'''.format(*args, *args2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyRlMKCzuwdk",
        "outputId": "6722874a-caf4-4a7e-c320-e0f56db2b15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CLASSIFIER 1\n",
            "------------ \n",
            "Female precision: 0.81\n",
            "Male precision: 0.67\n",
            "Female recall: 0.82\n",
            "Male recall: 0.66\n",
            "Female F1 score: 0.81\n",
            "Male F1 score: 0.67\n",
            "\n",
            "CLASSIFIER 2\n",
            "------------ \n",
            "Female precision: 0.83\n",
            "Male precision: 0.72\n",
            "Female recall: 0.85\n",
            "Male recall: 0.68\n",
            "Female F1 score: 0.84\n",
            "Male F1 score: 0.7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Error analysis:*** Investigating errors of classifier (names whose gender was misclassified)"
      ],
      "metadata": {
        "id": "MHJ7NzkUvAqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make list for errors and load in classifications where guess does not equal gender tag, print first five\n"
      ],
      "metadata": {
        "id": "I6IQErZleWNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []\n",
        "for (name, tag) in test_names:\n",
        "    guess = classifier2.classify(gender_features(name))\n",
        "    if guess != tag:\n",
        "        errors.append((tag, guess, name))\n",
        "\n",
        "errors[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SW13TLBvAQO",
        "outputId": "144d923b-9d28-434e-c161-f6e4a1767ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('female', 'male', 'Christean'),\n",
              " ('female', 'male', 'Charis'),\n",
              " ('male', 'female', 'Cody'),\n",
              " ('male', 'female', 'Micah'),\n",
              " ('male', 'female', 'Tracie')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print three columns (correct gender of name, guessed gender, and name itself)"
      ],
      "metadata": {
        "id": "VIWGyka0eY_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (tag, guess, name) in sorted(errors):\n",
        "    #print('Correct = {:8} guess = {:8} name = {}'.format(tag, guess, name)) # :8 creates spaces between columns."
      ],
      "metadata": {
        "id": "3_uC-MRLvo8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking through this list of errors, it seems that some suffixes that are more than one letter long can be indicative of name genders. For example, names ending in yn appear to be predominantly female, despite the fact that names ending in n tend to be male; and names ending in ch are usually male, even though names that end in h tend to be female."
      ],
      "metadata": {
        "id": "vaLHTKNOv5yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a classifier with even more features in response to errors"
      ],
      "metadata": {
        "id": "nshIowSTwFH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define new classifier which counts first letter and last two letters of word"
      ],
      "metadata": {
        "id": "DHM_oE0eegu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features3(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"suffix1\"] = name[-1].lower()\n",
        "    features[\"suffix2\"] = name[-2:].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "#gender_features3('John')"
      ],
      "metadata": {
        "id": "GVSb4Hj9wCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get features above for list of gendered names and put in list, print first item in list\n"
      ],
      "metadata": {
        "id": "6o_JeX8PesM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets3 = [(gender_features3(n), gender) for (n, gender) in labeled_names]\n",
        "featuresets3[0]"
      ],
      "metadata": {
        "id": "MSc2YjOzwj2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train new classifier on same set of male and female names above and get accuracy\n"
      ],
      "metadata": {
        "id": "IUDZtxgFey7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set3, test_set3 = featuresets3[:TRAIN_SET_SIZE], featuresets3[TRAIN_SET_SIZE:]\n",
        "classifier3 = NaiveBayesClassifier.train(train_set3)\n",
        "round(accuracy(classifier3, test_set3), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKXAflRsw3jt",
        "outputId": "63854065-c214-44b4-d05c-1171f96ea916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get 15 most informative features for classifier3"
      ],
      "metadata": {
        "id": "o_5k6q5We1OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier3.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2j_6qzxw-JC",
        "outputId": "7bb9396b-f956-425f-9290-edc577b7d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                 suffix2 = 'na'           female : male   =     84.0 : 1.0\n",
            "                 suffix2 = 'la'           female : male   =     67.8 : 1.0\n",
            "                 suffix2 = 'ra'           female : male   =     53.7 : 1.0\n",
            "                 suffix2 = 'ia'           female : male   =     49.4 : 1.0\n",
            "                 suffix2 = 'us'             male : female =     33.3 : 1.0\n",
            "                 suffix1 = 'a'            female : male   =     31.8 : 1.0\n",
            "                 suffix2 = 'rd'             male : female =     29.9 : 1.0\n",
            "                 suffix1 = 'k'              male : female =     27.6 : 1.0\n",
            "                 suffix2 = 'sa'           female : male   =     27.3 : 1.0\n",
            "                 suffix2 = 'ta'           female : male   =     21.9 : 1.0\n",
            "                 suffix2 = 'do'             male : female =     21.4 : 1.0\n",
            "                 suffix2 = 'ld'             male : female =     20.7 : 1.0\n",
            "                 suffix2 = 'rt'             male : female =     16.7 : 1.0\n",
            "                 suffix2 = 'ka'           female : male   =     14.8 : 1.0\n",
            "                 suffix2 = 'io'             male : female =     13.5 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Maximum entropy classifier:*** The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge is the one with largest entropy.\n",
        "\n",
        "The principle of maximum entropy is invoked when we have some piece(s) of information about a probability distribution, but not enough to characterize it completelyâ€”likely because we do not have the means or resources to do so. As an example, if all we know about a distribution is its average, we can imagine infinite shapes that yield a particular average. The principle of maximum entropy says that we should humbly choose the distribution that maximizes the amount of unpredictability contained in the distribution.\n",
        "\n",
        "Taking the idea to the extreme, it wouldnâ€™t be scientific to choose a distribution that simply yields the average value 100% of the time.\n",
        "\n",
        "From all the models that fit our training data, the Maximum Entropy classifier selects the one which has the largest entropy. Due to the minimum assumptions that the Maximum Entropy classifier makes, it is usually used when we donâ€™t know anything about the prior distributions and when it is unsafe to make any assumptions. Also, the maximum entropy classifier is used when we canâ€™t assume the conditional independence of the features."
      ],
      "metadata": {
        "id": "S2mZgGDBxKTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the performance in terms of accuracy on the test set starts significantly improving beyond the previous model's at around 25 iterations.\n"
      ],
      "metadata": {
        "id": "3KYdrUoZe5VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_iter has default value 100. \n",
        "me_classifier = MaxentClassifier.train(train_set3, max_iter=25) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvjL3c6ExigH",
        "outputId": "6359fa6b-f6a4-41ae-ae2e-5f48204c8960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (25 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.373\n",
            "             2          -0.60435        0.627\n",
            "             3          -0.58273        0.627\n",
            "             4          -0.56287        0.633\n",
            "             5          -0.54470        0.668\n",
            "             6          -0.52810        0.703\n",
            "             7          -0.51296        0.730\n",
            "             8          -0.49913        0.752\n",
            "             9          -0.48651        0.767\n",
            "            10          -0.47497        0.779\n",
            "            11          -0.46440        0.787\n",
            "            12          -0.45471        0.792\n",
            "            13          -0.44580        0.795\n",
            "            14          -0.43760        0.795\n",
            "            15          -0.43004        0.798\n",
            "            16          -0.42304        0.799\n",
            "            17          -0.41656        0.801\n",
            "            18          -0.41055        0.802\n",
            "            19          -0.40495        0.805\n",
            "            20          -0.39974        0.806\n",
            "            21          -0.39487        0.807\n",
            "            22          -0.39032        0.808\n",
            "            23          -0.38606        0.809\n",
            "            24          -0.38206        0.810\n",
            "         Final          -0.37830        0.811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the accuracy of the me classifier. The accuracies above were on the training set so this is what matters.\n"
      ],
      "metadata": {
        "id": "AAlYbod-e7vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(accuracy(me_classifier, test_set3), 2) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTpysUsvx-AR",
        "outputId": "9a63df04-bb25-4410-c831-2086b05165c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get 10 most informative features for me classifier\n"
      ],
      "metadata": {
        "id": "6lo8Bt0Be9p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "me_classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s-6k6FJyKtC",
        "outputId": "42281ea8-b053-4281-dc92-8458a2ff0bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -1.938 suffix2=='na' and label is 'male'\n",
            "  -1.922 suffix2=='la' and label is 'male'\n",
            "  -1.886 suffix2=='ra' and label is 'male'\n",
            "  -1.658 suffix2=='ia' and label is 'male'\n",
            "  -1.430 suffix2=='sa' and label is 'male'\n",
            "  -1.387 suffix1=='a' and label is 'male'\n",
            "  -1.346 suffix2=='us' and label is 'female'\n",
            "  -1.277 suffix1=='k' and label is 'female'\n",
            "  -1.217 suffix2=='ta' and label is 'male'\n",
            "  -1.213 suffix2=='rd' and label is 'female'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***More Classifiers:***\n",
        "Scikit-learn (sklearn) is a popular library which features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN.\n",
        "\n",
        "NLTK provides an API to quickly use sklearn classifiers in nltk.classify.scikitlearn. The other option is to import and use sklearn directly.\n",
        "\n",
        "For an example of integrating sklearn with NLTK, you can check out [this notebook on Kaggle.](https://www.kaggle.com/alvations/basic-nlp-with-nltk) Kaggle is a great website for NLP and machine learning in general, creating an account is highly recommended."
      ],
      "metadata": {
        "id": "kLwG96ZHyTp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Classifying News Documents into Categories\n",
        "\n",
        "Based on Another Excercise: Classifying News Documents in Categories: sport, humor, adventure, science fiction, etc... in [Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ],
      "metadata": {
        "id": "lKOqWc42zipI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring the Brown corpus**\n",
        "\n",
        "The Corpus consists of 500 samples, distributed across 15 genres. Each sample began at a random sentence-boundary in the article or other unit chosen, and continued up to the first sentence boundary after 2,000 words.\n",
        "\n",
        "A. PRESS: Reportage (44 texts)\n",
        "\n",
        "B. PRESS: Editorial (27 texts)\n",
        "\n",
        "C. PRESS: Reviews (17 texts)\n",
        "\n",
        "D. RELIGION (17 texts)\n",
        "\n",
        "E. SKILL AND HOBBIES (36 texts)\n",
        "\n",
        "F. POPULAR LORE (48 texts)\n",
        "\n",
        "G. BELLES-LETTRES - Biography, Memoirs, etc. (75 texts)\n",
        "\n",
        "H. MISCELLANEOUS: US Government & House Organs (30 texts)\n",
        "\n",
        "J. LEARNED - Natural sciences, Medicine, Mathematics, etc. (80 texts)\n",
        "\n",
        "K. FICTION: General (29 texts)\n",
        "\n",
        "L. FICTION: Mystery and Detective Fiction (24 texts)\n",
        "\n",
        "M. FICTION: Science (6 texts)\n",
        "\n",
        "N. FICTION: Adventure and Western (29 texts)\n",
        "\n",
        "P. FICTION: Romance and Love Story (29 texts)\n",
        "\n",
        "R. HUMOR (9 texts)"
      ],
      "metadata": {
        "id": "GKj3q9-Dz852"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean spacing in brown corpus"
      ],
      "metadata": {
        "id": "fn__CLdFfKjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "M0YDRqAazEh1",
        "outputId": "5ca5a9e3-1ea5-40c6-e81d-f70a3aa4e796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'BROWN CORPUS  A Standard Corpus of Present-Day Edited American English, for use with Digital Computers.  by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA  Revised 1971, Revised and Amplified 1979  http://www.hit.uib.no/icame/brown/bcm.html  Distributed with the permission of the copyright holder, redistribution permitted. '"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "brown.readme().replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print file ids in Brown corpus"
      ],
      "metadata": {
        "id": "h4L0dUWRfM8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.fileids()"
      ],
      "metadata": {
        "id": "VE3wqzpj0i9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get categories (genres of text) in Brown corpus"
      ],
      "metadata": {
        "id": "RYliITtefPV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IEyOteC0rkD",
        "outputId": "e97f59e3-5d8c-4b7f-c35c-47ccbc847e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first sentence in specified file of brown corpus"
      ],
      "metadata": {
        "id": "82yw2YGmfXJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown.sents('ca01')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atzpifoH0t73",
        "outputId": "c6f2f94a-99b2-4417-d2f4-da52a3bcc6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of',\n",
              " \"Atlanta's\",\n",
              " 'recent',\n",
              " 'primary',\n",
              " 'election',\n",
              " 'produced',\n",
              " '``',\n",
              " 'no',\n",
              " 'evidence',\n",
              " \"''\",\n",
              " 'that',\n",
              " 'any',\n",
              " 'irregularities',\n",
              " 'took',\n",
              " 'place',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile a list of most popular words in the corpus\n",
        "\n",
        "Takes a bunch of tokens and returns the frequencies of all unique cases."
      ],
      "metadata": {
        "id": "c1rg4JBW020n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the word is alphabetical avoids including stuff like `` and '' which are actually pretty common. \n",
        "# Note that it also omits words such as 1 (very common), aug., 1913, $30, 13th, over-all etc. Another option would have been .isalnum().\n",
        "words_in_corpora = FreqDist(w.lower() for w in brown.words() if w.isalpha()) \n",
        "#words_in_corpora"
      ],
      "metadata": {
        "id": "B6fiKm5u047P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this instead of sorted() to sort dictionary into a (mutable) list in order to delete the second column as opposed to into a tuple (immutable).\n"
      ],
      "metadata": {
        "id": "QcxK8N3Bffnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_corpora_freq_sorted = list(map(list, words_in_corpora.items()))\n",
        "#words_in_corpora_freq_sorted"
      ],
      "metadata": {
        "id": "mZJxzbwW1FpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort words in corpus based on frequency"
      ],
      "metadata": {
        "id": "phNhzkhDfjUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_corpora_freq_sorted.sort(key=lambda x: x[1], reverse=True) # Using a lambda function is an alternative to using the operator library.\n",
        "words_in_corpora_freq_sorted"
      ],
      "metadata": {
        "id": "sYXSGlC71Si1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put 1500 most frequent words in list into variable and delete word count (list item 1)\n"
      ],
      "metadata": {
        "id": "YsKiguyaflUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best1500 = words_in_corpora_freq_sorted[:1500]\n",
        "\n",
        "for list_item in best1500:\n",
        "    del list_item[1]\n",
        "\n",
        "#best1500"
      ],
      "metadata": {
        "id": "E4PU67jv1XVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since best1500 is now a list of words, it should be flattened. \n",
        "\n",
        "Break down the list into its individual sublists and then chain them. \n",
        "\n",
        "Chain further breaks down each sublist into its individual components so this approach can be used to flatten any list of lists."
      ],
      "metadata": {
        "id": "1BbKRgcRfyK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = itertools.chain(*best1500) \n",
        "best1500 = list(chain) # chain is of type itertools.chain so we need the cast\n",
        "#best1500"
      ],
      "metadata": {
        "id": "EFAxauxc1ioy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Receives a list of words and removes stop words from list"
      ],
      "metadata": {
        "id": "qVwANZqhf7dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopw = stopwords.words('english')\n",
        "\n",
        "def nonstop(listwords):\n",
        "    return [word for word in listwords if word not in stopw]\n",
        "\n",
        "best1500_words_corpora = nonstop(best1500) # Note how this will probably contain less than 1500 words.\n",
        "#best1500_words_corpora"
      ],
      "metadata": {
        "id": "j096l7hX1uFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Converting corpus to form suitable for classification:*** Each file in the corpus will eventually be represented by a dictionary showing the presence of the corpusâ€™ most popular words in the particular file."
      ],
      "metadata": {
        "id": "OV9lfJZL13cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = [(nonstop(brown.words(fileid)), category) for category in brown.categories() for fileid in brown.fileids(category)]\n",
        "# documents # Note how documents is a list of tuples.\n",
        "\n",
        "# The code above generates a representation of the corpus but without removing punctuation. This is better:\n",
        "documents = [([item.lower() for item in nonstop(brown.words(fileid)) if item.isalpha()], category)\n",
        "             for category in brown.categories()\n",
        "             for fileid in brown.fileids(category)]\n",
        "documents # Note how documents is a list of tuples."
      ],
      "metadata": {
        "id": "ZTPt6D6312cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle items in list of tuples"
      ],
      "metadata": {
        "id": "UzCvFVuhgBIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle(documents)\n",
        "documents"
      ],
      "metadata": {
        "id": "GfZ4KzwM2KWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a document extract features (the presence or not of the 1500 most frequent words of the corpus)"
      ],
      "metadata": {
        "id": "U-_m2i9ygCQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(doc):\n",
        "    doc_set_words = set(doc) # Checking whether a word occurs in a set is much faster than checking whether it occurs in a list.\n",
        "    features_dic = {} # Features is a dictionary\n",
        "    for word in best1500_words_corpora:\n",
        "        features_dic['has(%s)' % word] = (word in doc_set_words)\n",
        "    return features_dic\n",
        "\n",
        "doc_features_set = [(document_features(d),c) for (d,c) in documents]\n",
        "doc_features_set[0]"
      ],
      "metadata": {
        "id": "b94GlVcq2TFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now build the classifer to determine what category documents fall into based on most frequent words"
      ],
      "metadata": {
        "id": "UoJRCaGG2gXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = doc_features_set[:350] # Since the total is 500\n",
        "test_set  = doc_features_set[150:]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIDW05Zx2oS0",
        "outputId": "0e060bbe-975c-43e9-e9a1-8d4ea07da3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             has(walked) = True           myster : learne =     29.7 : 1.0\n",
            "              has(music) = True           review : learne =     28.9 : 1.0\n",
            "                has(ran) = True           advent : learne =     28.5 : 1.0\n",
            "          has(afternoon) = True           fictio : learne =     27.2 : 1.0\n",
            "               has(road) = True           myster : learne =     27.1 : 1.0\n",
            "            has(playing) = True           review : learne =     26.2 : 1.0\n",
            "                has(god) = True           religi : learne =     25.8 : 1.0\n",
            "                has(car) = True            humor : learne =     25.3 : 1.0\n",
            "               has(hair) = True           romanc : learne =     23.8 : 1.0\n",
            "              has(maybe) = True           romanc : learne =     23.8 : 1.0\n",
            "            has(watched) = True           advent : learne =     22.6 : 1.0\n",
            "            has(kitchen) = True            humor : belles =     22.4 : 1.0\n",
            "          has(communism) = True           editor : learne =     22.1 : 1.0\n",
            "             has(berlin) = True           editor : learne =     22.1 : 1.0\n",
            "           has(watching) = True           romanc : learne =     21.7 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get accuracy of classifier"
      ],
      "metadata": {
        "id": "UW1mLxb1gE8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nzpfE_12uL8",
        "outputId": "42a60d0b-5964-4208-ff7f-50e4874263bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7371428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classification of documet 'ca01' (it is under the 'news' category)"
      ],
      "metadata": {
        "id": "lYGTeBoLgGqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(brown.words('ca01')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kr1sLuUo20TW",
        "outputId": "780afe3a-af05-4be4-f98d-22b220fc3271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'news'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The test text needs to be long enough in order to contain a significant amount of the 1500 most common words in our training corpus.\n",
        "text = \"1 God, infinitely perfect and blessed in himself, in a plan of sheer goodness freely created man to make him share in his own blessed life. For this reason, at every time and in every place, God draws close to man. He calls man to seek him, to know him, to love him with all his strength. He calls together all men, scattered and divided by sin, into the unity of his family, the Church. To accomplish this, when the fullness of time had come, God sent his Son as Redeemer and Saviour. In his Son and through him, he invites men to become, in the Holy Spirit, his adopted children and thus heirs of his blessed life. 2 So that this call should resound throughout the world, Christ sent forth the apostles he had chosen, commissioning them to proclaim the gospel: \\\"Go therefore and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, teaching them to observe all that I have commanded you; and lo, I am with you always, to the close of the age.\\\"4 Strengthened by this mission, the apostles \\\"went forth and preached everywhere, while the Lord worked with them and confirmed the message by the signs that attended it.\\\" 3 Those who with God's help have welcomed Christ's call and freely responded to it are urged on by love of Christ to proclaim the Good News everywhere in the world. This treasure, received from the apostles, has been faithfully guarded by their successors. All Christ's faithful are called to hand it on from generation to generation, by professing the faith, by living it in fraternal sharing, and by celebrating it in liturgy and prayer. 4 Quite early on, the name catechesis was given to the totality of the Church's efforts to make disciples, to help men believe that Jesus is the Son of God so that believing they might have life in his name, and to educate and instruct them in this life, thus building up the body of Christ. Catechesis is an education in the faith of children, young people and adults which includes especially the teaching of Christian doctrine imparted, generally speaking, in an organic and systematic way, with a view to initiating the hearers into the fullness of Christian life. While not being formally identified with them, catechesis is built on a certain number of elements of the Church's pastoral mission which have a catechetical aspect, that prepare for catechesis, or spring from it. They are: the initial proclamation of the Gospel or missionary preaching to arouse faith; examination of the reasons for belief; experience of Christian living; celebration of the sacraments; integration into the ecclesial community; and apostolic and missionary witness. Catechesis is intimately bound up with the whole of the Church's life. Not only her geographical extension and numerical increase, but even more her inner growth and correspondence with God's plan depend essentially on catechesis. Periods of renewal in the Church are also intense moments of catechesis. In the great era of the Fathers of the Church, saintly bishops devoted an important part of their ministry to catechesis. St. Cyril of Jerusalem and St. John Chrysostom, St. Ambrose and St. Augustine, and many other Fathers wrote catechetical works that remain models for us. The ministry of catechesis draws ever fresh energy from the councils. the Council of Trent is a noteworthy example of this. It gave catechesis priority in its constitutions and decrees. It lies at the origin of the Roman Catechism, which is also known by the name of that council and which is a work of the first rank as a summary of Christian teaching. The Council of Trent initiated a remarkable organization of the Church's catechesis. Thanks to the work of holy bishops and theologians such as St. Peter Canisius, St. Charles Borromeo, St. Turibius of Mongrovejo or St. Robert Bellarmine, it occasioned the publication of numerous catechisms. It is therefore no surprise that catechesis in the Church has again attracted attention in the wake of the Second Vatican Council, which Pope Paul Vl considered the great catechism of modern times. the General Catechetical Directory (1971) the sessions of the Synod of Bishops devoted to evangelization (1974) and catechesis (1977), the apostolic exhortations Evangelii nuntiandi (1975) and Catechesi tradendae (1979), attest to this. the Extraordinary Synod of Bishops in 1985 asked that a catechism or compendium of all Catholic doctrine regarding both faith and morals be composed. The Holy Father, Pope John Paul II, made the Synod's wish his own, acknowledging that this desire wholly corresponds to a real need of the universal Church and of the particular Churches. He set in motion everything needed to carry out the Synod Fathers' wish.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') # Picks out sequences of alphanumeric characters as tokens and drops everything else\n",
        "text_tokens = nonstop(tokenizer.tokenize(text.lower()))\n",
        "text_tokens = [w for w in text_tokens if w.isalpha()]\n",
        "#text_tokens"
      ],
      "metadata": {
        "id": "M-EMaQVd270T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine whether list of tokens contain most frequent words set above"
      ],
      "metadata": {
        "id": "ZlOQHMLAgTNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = document_features(text_tokens)\n",
        "#text_features"
      ],
      "metadata": {
        "id": "IzMMcZ9T3CBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifies new document based on presence of frequent words in brown corpus categories"
      ],
      "metadata": {
        "id": "aD_NtICOgV1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(text_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JcWp17aY3LzC",
        "outputId": "ec23b1fe-2215-4643-ec91-e72dc8334177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'belles_lettres'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Intro to Sentiment Analysis"
      ],
      "metadata": {
        "id": "Bbr0cfEraN10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis is the practice of using algorithms to classify various samples of related text into overall positive and negative categories. With NLTK, you can employ these algorithms through powerful built-in machine learning operations to obtain insights from linguistic data.\n",
        "\n",
        "Based on [Exercise B: Sentiment Analysis in Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ],
      "metadata": {
        "id": "NyovRjxyaevV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Explore the movie_reviews corpus**"
      ],
      "metadata": {
        "id": "DvYftuKwaxDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean spacing from NLTK movie reviews corpus"
      ],
      "metadata": {
        "id": "V0Nx_mEsbVKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "X3Qsvmn0aGiT",
        "outputId": "066a5646-9e13-4ee8-c5d4-157ae2f74dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sentiment Polarity Dataset Version 2.0 Bo Pang and Lillian Lee  http://www.cs.cornell.edu/people/pabo/movie-review-data/  Distributed with NLTK with permission from the authors.  =======  Introduction  This README v2.0 (June, 2004) for the v2.0 polarity dataset comes from the URL http://www.cs.cornell.edu/people/pabo/movie-review-data .  =======  What\\'s New -- June, 2004  This dataset represents an enhancement of the review corpus v1.0 described in README v1.1: it contains more reviews, and labels were created with an improved rating-extraction system.  =======  Citation Info   This data was first used in Bo Pang and Lillian Lee, \"A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization  Based on Minimum Cuts\",  Proceedings of the ACL, 2004.  @InProceedings{Pang+Lee:04a,   author =       {Bo Pang and Lillian Lee},   title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},   booktitle =    \"Proceedings of the ACL\",   year =         2004 }  =======  Data Format Summary   - review_polarity.tar.gz: contains this readme and  data used in   the experiments described in Pang/Lee ACL 2004.    Specifically:    Within the folder \"txt_sentoken\" are the 2000 processed down-cased   text files used in Pang/Lee ACL 2004; the names of the two   subdirectories in that folder, \"pos\" and \"neg\", indicate the true   classification (sentiment) of the component files according to our   automatic rating classifier (see section \"Rating Decision\" below).    File names consist of a cross-validation tag plus the name of the   original html file.  The ten folds used in the Pang/Lee ACL 2004 paper\\'s   experiments were:       fold 1: files tagged cv000 through cv099, in numerical order      fold 2: files tagged cv100 through cv199, in numerical order           ...      fold 10: files tagged cv900 through cv999, in numerical order    Hence, the file neg/cv114_19501.txt, for example, was labeled as   negative, served as a member of fold 2, and was extracted from the   file 19501.html in polarity_html.zip (see below).    Each line in each text file corresponds to a single sentence, as   determined by Adwait Ratnaparkhi\\'s sentence boundary detector   MXTERMINATOR.     Preliminary steps were taken to remove rating information from the   text files, but only the rating information upon which the rating   decision was based is guaranteed to have been removed. Thus, if the   original review contains several instances of rating information,   potentially given in different forms, those not recognized as valid   ratings remain part of the review text.  - polarity_html.zip: The original source files from which the   processed, labeled, and (randomly) selected data in   review_polarity.tar.gz was derived.    Specifically:      This data consists of unprocessed, unlabeled html files from the   IMDb archive of the rec.arts.movies.reviews newsgroup,   http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz   represent a processed subset of these files.   =======  Rating Decision (Appendix A)  This section describes how we determined whether a review was positive or negative.  The original html files do not have consistent formats -- a review may not have the author\\'s rating with it, and when it does, the rating can appear at different places in the file in different forms.  We only recognize some of the more explicit ratings, which are extracted via a set of ad-hoc rules.  In essence, a file\\'s classification is determined based on the first rating we were able to identify.   - In order to obtain more accurate rating decisions, the maximum rating must be specified explicitly, both for numerical ratings and star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF ****: ***\" are examples of rating indications we recognize.)  - With a five-star system (or compatible number systems): three-and-a-half stars and up are considered positive,  two stars and below are considered negative. - With a four-star system (or compatible number system): three stars and up are considered positive,  one-and-a-half stars and below are considered negative.   - With a letter grade system: B or above is considered positive, C- or below is considered negative.  We attempted to recognize half stars, but they are specified in an especially free way, which makes them difficult to recognize.  Hence, we may lose a half star very occasionally; but this only results in 2.5 stars in five star system being categorized as negative, which is  still reasonable.   '"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "movie_reviews.readme().replace('\\n', ' ').replace('\\t', '').replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want, you can print the file ids from movie_reviews; it generates a very long list. But you can see the structure of the ids and how the label includes \"pos\" or \"neg\""
      ],
      "metadata": {
        "id": "Pr30athMbfr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#movie_reviews.fileids()"
      ],
      "metadata": {
        "id": "lUszG-ZZbgCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine how many movie reviews are in the corpus, print the length of the list of file ids"
      ],
      "metadata": {
        "id": "0ucelACgcPtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(movie_reviews.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YFBq_EcVXu",
        "outputId": "946b0f23-d98e-485d-d25d-94f84862845c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an additional cleaning trick to get rid of \\' in text - but only if there were no \" used. See how it works with just one file."
      ],
      "metadata": {
        "id": "Gzrs1XiCcaNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews.raw(\"neg/cv000_29416.txt\").replace(\"\\n\", \"\").replace(\"'\", '\"').replace('\"', \"'\") "
      ],
      "metadata": {
        "id": "7atoJ3dEcYGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Building and testing the classifier**"
      ],
      "metadata": {
        "id": "LpekNwyLc55v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building the classifier, you'll want to generate a list of stopwords which will NOT be considered when making lists of positive and negative words. We'll import English stopwords from NLTK and put them in \"stops,\" then add additional features we don't want to include in classification using stops.extend. To see check full list of stopwords, print stops."
      ],
      "metadata": {
        "id": "xDZrX3P9dSXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stops = stopwords.words('english')\n",
        "stops.extend('.,[,],(,),;,/,-,\\',?,\",:,<,>,n\\'t,|,#,\\'s,\\\",\\'re,\\'ve,\\'ll,\\'d,\\'re'.split(','))\n",
        "stops.extend(',')\n",
        "#stops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7oDaru7dqA7",
        "outputId": "45042f4e-d5cf-4afc-ec73-9f4f5da859c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll work with the NaiveBayes Classifier. Learn more about Naive Bayes [here](https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/). "
      ],
      "metadata": {
        "id": "tVFeM1Azd7L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function which, given a word, returns a dict `{word: True}.` This will be our feature in the classifier. "
      ],
      "metadata": {
        "id": "Rv2kNMtEeWww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_feats(words):\n",
        "    return dict([(word, True) for word in words if word not in stops and word.isalpha()])"
      ],
      "metadata": {
        "id": "tSikwcVCeWI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new variables for all positive and all negative movie reviews and get combined length (should be same as  length of original file ids list)."
      ],
      "metadata": {
        "id": "RFhcNDA_e-4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = movie_reviews.fileids('pos')\n",
        "neg_ids = movie_reviews.fileids('neg')\n",
        "\n",
        "len(pos_ids) + len(neg_ids) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pcNw3kGe-gL",
        "outputId": "8593a121-3b63-4749-9e49-a83321d8b998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take the positive/negative words, create the feature for such words, and store it in a positive/negative features list. You can print pos_feats to check list of words has loaded correctly; it will print VERY long list, since it will include words from every positive review.\n"
      ],
      "metadata": {
        "id": "IrrKfwbifS2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]\n",
        "neg_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]\n",
        "\n",
        "#pos_feats"
      ],
      "metadata": {
        "id": "qZTBXV8vfTBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store 3/4 of features for training the classifier and check length of positive training features. "
      ],
      "metadata": {
        "id": "IMtJeNTjgNCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_len_train = int(len(pos_feats) * 3 / 4)\n",
        "neg_len_train = int(len(neg_feats) * 3 / 4)\n",
        "\n",
        "pos_len_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_7g_OV6gOFl",
        "outputId": "6a312f4e-2cc8-4b54-b1f6-d0a06d4db5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "750"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine positive and negative training features into one set and put the rest in \"test features\" "
      ],
      "metadata": {
        "id": "FvC41GKMgbXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_feats = neg_feats[:neg_len_train] + pos_feats[:pos_len_train]\n",
        "test_feats = neg_feats[neg_len_train:] + pos_feats[pos_len_train:]"
      ],
      "metadata": {
        "id": "cokZ_I7igacQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a NaiveBayesClassifier with our training feature words."
      ],
      "metadata": {
        "id": "TmfvBYSAgjkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_feats)"
      ],
      "metadata": {
        "id": "_ODzgc6cgjHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get accuracy of the classifier we have just trained."
      ],
      "metadata": {
        "id": "lBzproVxgonT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: ', nltk.classify.util.accuracy(classifier, test_feats))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAnPOblEgovV",
        "outputId": "7f38200a-34ad-4c21-ae23-fbbea4a77499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see which words fit best in each class by getting the classifier's most informative features. "
      ],
      "metadata": {
        "id": "FQIFYUr_gu3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ZeR30Wgu9x",
        "outputId": "5fe079a8-a8d6-4c3c-9d09-d3de0023e694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
            "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
            "               insulting = True              neg : pos    =     13.0 : 1.0\n",
            "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
            "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
            "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
            "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
            "              astounding = True              pos : neg    =     10.3 : 1.0\n",
            "             fascination = True              pos : neg    =     10.3 : 1.0\n",
            "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Classifying new data**"
      ],
      "metadata": {
        "id": "OqYtDEXkg7MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a new sentence to test our classifier and tokenize it, adding features to tokens that are NOT in \"stops\" we defined above."
      ],
      "metadata": {
        "id": "QtIJfk7-hEU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I feel so miserable, it makes me amazing\"\n",
        "tokens = [word for word in word_tokenize(sentence) if word not in stops]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORWI_qhUhD3j",
        "outputId": "b08327f7-37c0-4f2a-eabf-06368ef195d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'feel', 'miserable', 'makes', 'amazing']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make tokens into features using word_feats function defined above."
      ],
      "metadata": {
        "id": "DnsXTKNPhZuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feats = word_feats(word for word in tokens)\n",
        "feats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obKYKmY1hZ1h",
        "outputId": "c297149d-1bad-43e5-e5ea-b752157e236a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'I': True, 'amazing': True, 'feel': True, 'makes': True, 'miserable': True}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use classifier to classify new sentence as either positive or negative. The result may not be what you expect!"
      ],
      "metadata": {
        "id": "wk-DyJoGhzsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aYV-7yIZhy2l",
        "outputId": "195eb0bb-0d23-411c-ef45-ced21561f3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try classifying another sentence - go through the same tokenizing process."
      ],
      "metadata": {
        "id": "7hyPwIASiAQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2 = \"You are a pathetic fool, a terrible excuse for a human being.\"\n",
        "tokens2 = [word for word in word_tokenize(sentence2) if word not in stops]\n",
        "tokens2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I3Ci43UiAah",
        "outputId": "222bf238-cfb5-49d4-feb7-f2d8763da2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['You', 'pathetic', 'fool', 'terrible', 'excuse', 'human']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tokens into new variable - instead of retaining all tokens, just capture the adjectives using `if pos[] == JJ`"
      ],
      "metadata": {
        "id": "NO2dphAviilC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags2 = [pos for pos in pos_tag(tokens2) if pos[1] == 'JJ']\n",
        "pos_tags2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxAhIxPBiMXG",
        "outputId": "6ccb05f2-0955-4fae-baa8-9ba2c9667c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pathetic', 'JJ'), ('terrible', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put reduced list of tokens into variable for classificaiton"
      ],
      "metadata": {
        "id": "AoZcDaGqjEgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feats2 = word_feats([word for (word,_) in pos_tags2])\n",
        "feats2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDO4_fJcjFFo",
        "outputId": "98b3ea67-e9d2-4449-889e-aa67659a8877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pathetic': True, 'terrible': True}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use classifier to classify new sentence as either positive or negative."
      ],
      "metadata": {
        "id": "cc61rB-WiLYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(feats2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OEAYqGgkjMb4",
        "outputId": "cc935e48-b730-4a34-da40-954588ee7ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'neg'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Incorporating bigram features**\n",
        "In order to improve the classifier, bigram features can be examined using `nltk.util.ngrams`. This is because, for instance, 'not funny' is very different from 'funny'."
      ],
      "metadata": {
        "id": "QzR7R_NhjTOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentiment Analysis with `nltk.sentiment.SentimentAnalyzer` and VADER tools"
      ],
      "metadata": {
        "id": "COYEfQzej8-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 1: Exploring the `subjectivity` corpus***\n",
        "\n",
        "The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences. Learn more about the subjectivity corpus [here](https://www.nltk.org/howto/corpus.html).\n",
        "\n",
        "From the NLTK subjectivity corpus, get the file ids."
      ],
      "metadata": {
        "id": "HHAKVRRnlGxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ6vs51Tj8F_",
        "outputId": "01c93ac4-ecfc-4560-e115-cf8d0cf8a6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plot.tok.gt9.5000', 'quote.tok.gt9.5000']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "subjectivity.fileids()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in plot.tok file"
      ],
      "metadata": {
        "id": "bbPO1cNqmD_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents('plot.tok.gt9.5000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OvcGQBpmE5T",
        "outputId": "dc348af1-9a46-49cd-c84d-a543818e4ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in quote.tok file"
      ],
      "metadata": {
        "id": "BspFeKyCmomm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents('quote.tok.gt9.5000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZ_rKt8mvzk",
        "outputId": "fc29a2ca-68f7-4355-c2af-b5a827e9dbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the categories in subjectivity corpus (objective and subjective sentences)."
      ],
      "metadata": {
        "id": "MUq_FHPgmzcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.categories() # The mapping between documents and categories does not depend on the file structure."
      ],
      "metadata": {
        "id": "iFSv2oNqmy5c",
        "outputId": "260e22f2-bb3b-46ef-9248-4651ecf2dccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['obj', 'subj']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in subjectivity that are categorized as \"objective\""
      ],
      "metadata": {
        "id": "CcnY55Pjnhsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents(categories='obj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTtbpLjnh2p",
        "outputId": "41ecd593-d959-4331-debb-facce1e8328a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens in subjectivity that are categorized as \"subjective\""
      ],
      "metadata": {
        "id": "S-aYzaRQoOw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity.sents(categories='subj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J36LTi4PoO4b",
        "outputId": "81341808-62ea-46ee-d8d8-f31a5dc8d1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Building and testing a classifier with `SentimentAnalyzer`***"
      ],
      "metadata": {
        "id": "-kh_y5Sloiln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set number of instances at 100; then create two new lists for objective and subjective docs and put sentences up to number of n_instancse (100) in each list. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings.\n",
        "\n",
        "Print length of each list to check they both contain 100 sentences."
      ],
      "metadata": {
        "id": "_Ob_727eXrYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_instances = 100\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "len(obj_docs), len(subj_docs)"
      ],
      "metadata": {
        "id": "6ifXBgMoXrja",
        "outputId": "2f39cea0-56df-428b-ab9d-463555fbba5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sentence in obj_docs list to check:"
      ],
      "metadata": {
        "id": "r23Kr1xxYcjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_docs[0]"
      ],
      "metadata": {
        "id": "U1idOEHVYcq3",
        "outputId": "ddfd78df-e00c-4942-d243-9f95ef07f87c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['the',\n",
              "  'movie',\n",
              "  'begins',\n",
              "  'in',\n",
              "  'the',\n",
              "  'past',\n",
              "  'where',\n",
              "  'a',\n",
              "  'young',\n",
              "  'boy',\n",
              "  'named',\n",
              "  'sam',\n",
              "  'attempts',\n",
              "  'to',\n",
              "  'save',\n",
              "  'celebi',\n",
              "  'from',\n",
              "  'a',\n",
              "  'hunter',\n",
              "  '.'],\n",
              " 'obj')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divde sentences into training and testing groups; first 80 sentences of each are for training, last 20 for testing. Split evenly for objective and subjective docs, then combine into two larger groups (all training and all testing)."
      ],
      "metadata": {
        "id": "2LfDV64CYj7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_obj_docs = obj_docs[:80]\n",
        "test_obj_docs = obj_docs[80:100]\n",
        "train_subj_docs = subj_docs[:80]\n",
        "test_subj_docs = subj_docs[80:100]\n",
        "\n",
        "training_docs = train_obj_docs + train_subj_docs\n",
        "testing_docs = test_obj_docs + test_subj_docs"
      ],
      "metadata": {
        "id": "jg1aXbTsYkCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define sentiment analyzer as `SentimentAnalyzer()` and use it to append _NEG suffix to words that appear between a sensed negation and a punctuation mark."
      ],
      "metadata": {
        "id": "cvg-fUBHY3sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
        "#all_words_neg"
      ],
      "metadata": {
        "id": "Nt3vhXFgY3Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the list of most common 1-word features in all_words_neg, with a minimum frequency of 4 appearances."
      ],
      "metadata": {
        "id": "ICLzdAz_Zk9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "len(unigram_feats)"
      ],
      "metadata": {
        "id": "W0sx3HcoZldZ",
        "outputId": "d55523ba-a6f4-4b5e-ba46-043dfbf64de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add unigram_features to list of features that the sentiment analyzer will extract from the data."
      ],
      "metadata": {
        "id": "I_x6dJbicnFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
      ],
      "metadata": {
        "id": "0l2UG9qNcg6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redefine training and test set to include whether or not sents include the `unigram_feats`"
      ],
      "metadata": {
        "id": "wcIWyc4YcTl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = sentim_analyzer.apply_features(training_docs)\n",
        "test_set = sentim_analyzer.apply_features(testing_docs)\n",
        "#training_set[0]"
      ],
      "metadata": {
        "id": "Rc_HWRZPbiJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train our classifier on the training set, and subsequently output the evaluation results. "
      ],
      "metadata": {
        "id": "OmOgpJ21bh_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)\n"
      ],
      "metadata": {
        "id": "I1ZDFgfccTtW",
        "outputId": "97ab854b-b70d-4e57-8074-809a7fd1bbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of results from [Python NLTK Cookbook:](https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/)\n",
        "\n",
        "*  **Accuracy** measures the number of elements correctly identified in a data set.\n",
        "*  **F-measure** is the weighted harmonic mean of precision and recall. \n",
        "*  **Precision** measures the exactness of a classifier. A higher precision means less false positives, while a lower precision means more false positives.\n",
        "*   **Recall** measures the completeness, or sensitivity, of a classifier. Higher recall means less false negatives, while lower recall means more false negatives. Often improves inverse of precision.\n"
      ],
      "metadata": {
        "id": "HsO80ZOveRbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "    print('{0}: {1}'.format(key, value))"
      ],
      "metadata": {
        "id": "iZgR6_eSd-TJ",
        "outputId": "9fa43753-8902-433d-b1c3-b2b6533d4229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating NaiveBayesClassifier results...\n",
            "Accuracy: 0.8\n",
            "F-measure [obj]: 0.8\n",
            "F-measure [subj]: 0.8\n",
            "Precision [obj]: 0.8\n",
            "Precision [subj]: 0.8\n",
            "Recall [obj]: 0.8\n",
            "Recall [subj]: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: Building and testing a classifier with `nltk.sentiment.vader.SentimentIntensityAnalyzer`***"
      ],
      "metadata": {
        "id": "houZ2eJQfyrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use `SentimentIntensityAnalyzer `from [Vader](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). This will assign an \"intensity score\" to each sentence based on its identified sentiment."
      ],
      "metadata": {
        "id": "y74QTxjAgDIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add list of sentences for analysis."
      ],
      "metadata": {
        "id": "HaN71m4Xgr0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"You are a jerk, and I will step on you.\",\n",
        "    \"THIS SUX!!!\",\n",
        "    \"This kinda sux...\",\n",
        "    \"You're good, man\",\n",
        "    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n",
        "            ]"
      ],
      "metadata": {
        "id": "2BsReT4tgsCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SentimentIntesnityAnalyzer (defined as sid) to get \"intensity\" of each sentence in list"
      ],
      "metadata": {
        "id": "-hXXSPfng3PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "for sentence in sentences:\n",
        "    print('\\n' + sentence)\n",
        "    ss = sid.polarity_scores(sentence)\n",
        "    for k in sorted(ss):\n",
        "        print('{0}: {1}, '.format(k, ss[k]), end='')"
      ],
      "metadata": {
        "id": "8UCeoLBXg3bI",
        "outputId": "77849890-c5dc-4de2-910c-53e209ecef68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "\n",
            "You are a jerk, and I will step on you.\n",
            "compound: -0.34, neg: 0.255, neu: 0.745, pos: 0.0, \n",
            "THIS SUX!!!\n",
            "compound: -0.5229, neg: 0.771, neu: 0.229, pos: 0.0, \n",
            "This kinda sux...\n",
            "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
            "You're good, man\n",
            "compound: 0.4404, neg: 0.0, neu: 0.408, pos: 0.592, \n",
            "HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\n",
            "compound: 0.8386, neg: 0.0, neu: 0.386, pos: 0.614, "
          ]
        }
      ]
    }
  ]
}