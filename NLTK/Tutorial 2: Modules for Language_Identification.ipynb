{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 2: Modules for Language Identification.ipynb",
      "provenance": [],
      "mount_file_id": "1EYZrjmDTNcNuJh8i71ou0FbQnc0zPZF8",
      "authorship_tag": "ABX9TyM4Mer9yi6+IsV/xeE0yjNM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Tutorial%202%3A%20Modules%20for%20Language_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Modules for Language Identification \n",
        "# (N-Gram, Stopword and Word Bigram Analysis)"
      ],
      "metadata": {
        "id": "9pJU0rMMcGtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.1: Deriving N-Grams from Text**\n",
        "\n",
        "Based on [N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)\n",
        "\n",
        "What are n-grams? See [here](https://cloudmark.github.io/Language-Detection/)."
      ],
      "metadata": {
        "id": "GTJI2-ALU05f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization:*** Divides strings of text into substrings of letters and apostrophes ONLY to prepare for n-gram analysis"
      ],
      "metadata": {
        "id": "RjaYkNAEVDOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cLZvk_4ZUu1H"
      },
      "outputs": [],
      "source": [
        "#Lowercase text in string\n",
        "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
        "s = s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import regular expressions tokenizer and tokenize string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
        "s_tokenized = tokenizer.tokenize(s)\n",
        "s_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVBiQkhpVdzt",
        "outputId": "1e4955b5-43dd-4b3d-8b21-9a48eb779021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['le',\n",
              " 'temps',\n",
              " 'est',\n",
              " 'un',\n",
              " 'grand',\n",
              " 'maître',\n",
              " 'dit',\n",
              " 'on',\n",
              " 'le',\n",
              " 'malheur',\n",
              " 'est',\n",
              " \"qu'il\",\n",
              " 'tue',\n",
              " 'ses',\n",
              " 'élèves']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generating N-Grams:*** Finds n-length slices of a longer string, typically overlapping/in sequence; can be used for language detection"
      ],
      "metadata": {
        "id": "wkuxxJDxVwuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import ngrams module and create list for generated ngrams\n",
        "from nltk.util import ngrams\n",
        "generated_4grams = []\n",
        "\n",
        "#Generate ngram for each word in tokenized string\n",
        "for word in s_tokenized:\n",
        "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
        "generated_4grams"
      ],
      "metadata": {
        "id": "8MozGI4aV3LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that generated_4grams needs flattening since it's supposed to be a list of 4-grams:"
      ],
      "metadata": {
        "id": "YfeR9ZBqV2l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates list in which ????\n",
        "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
        "generated_4grams[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqLwYALY84E",
        "outputId": "64a05f5e-5291-4e43-bc52-8b8cd269b790"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_', '_', '_', 'l'),\n",
              " ('_', '_', 'l', 'e'),\n",
              " ('_', 'l', 'e', '_'),\n",
              " ('l', 'e', '_', '_'),\n",
              " ('e', '_', '_', '_'),\n",
              " ('_', '_', '_', 't'),\n",
              " ('_', '_', 't', 'e'),\n",
              " ('_', 't', 'e', 'm'),\n",
              " ('t', 'e', 'm', 'p'),\n",
              " ('e', 'm', 'p', 's')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining n-grams (n = 4)"
      ],
      "metadata": {
        "id": "-8lnKPwzaOGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Join 4grams into list of strings\n",
        "ng_list_4grams = generated_4grams\n",
        "for idx, val in enumerate(generated_4grams):\n",
        "    ng_list_4grams[idx] = ''.join(val)\n",
        "ng_list_4grams"
      ],
      "metadata": {
        "id": "N3540KisaNMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort n-grams by how frequently they appear within the text"
      ],
      "metadata": {
        "id": "--vvU1CZavQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create list for n-grams sorted by frequency\n",
        "freq_4grams = {}\n",
        "\n",
        "#Iterate through ngrams and add to freq_4grams list as many times as appearing in list of strings\n",
        "for ngram in ng_list_4grams:\n",
        "    if ngram not in freq_4grams:\n",
        "        freq_4grams.update({ngram: 1})\n",
        "    else:\n",
        "        ngram_occurrences = freq_4grams[ngram]\n",
        "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
        "        \n",
        "# The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
        "from operator import itemgetter \n",
        "\n",
        "# We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
        "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] \n",
        "freq_4grams_sorted"
      ],
      "metadata": {
        "id": "FIvRnUepavYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain n-grams for multiple values of n (n = 1, 2, 3, 4)"
      ],
      "metadata": {
        "id": "dPt3vftZbO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import everygrams\n",
        "\n",
        "# For the code below we need the raw sentence as opposed to the tokens.\n",
        "s_clean = ' '.join(s_tokenized) \n",
        "s_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcvlXoFtbWk2",
        "outputId": "acd053ba-ba85-48d3-aacd-0407c2ce0293"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define ngram extractor generating uni-grams, bigrams, trigrams and 4-grams of string (range set to 1-4)\n",
        "def ngram_extractor(sent):\n",
        "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
        "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
        "\n",
        "ngram_extractor(s_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imOf0UoEbrEC",
        "outputId": "2d5a64fb-ab0d-4a59-db59-7cbf2f331142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l',\n",
              " 'e',\n",
              " 't',\n",
              " 'e',\n",
              " 'm',\n",
              " 'p',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'n',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'm',\n",
              " 'a',\n",
              " 'î',\n",
              " 't',\n",
              " 'r',\n",
              " 'e',\n",
              " 'd',\n",
              " 'i',\n",
              " 't',\n",
              " 'o',\n",
              " 'n',\n",
              " 'l',\n",
              " 'e',\n",
              " 'm',\n",
              " 'a',\n",
              " 'l',\n",
              " 'h',\n",
              " 'e',\n",
              " 'u',\n",
              " 'r',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " 'q',\n",
              " 'u',\n",
              " \"'\",\n",
              " 'i',\n",
              " 'l',\n",
              " 't',\n",
              " 'u',\n",
              " 'e',\n",
              " 's',\n",
              " 'e',\n",
              " 's',\n",
              " 'é',\n",
              " 'l',\n",
              " 'è',\n",
              " 'v',\n",
              " 'e',\n",
              " 's',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_t',\n",
              " 'te',\n",
              " 'em',\n",
              " 'mp',\n",
              " 'ps',\n",
              " 's_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_u',\n",
              " 'un',\n",
              " 'n_',\n",
              " '_g',\n",
              " 'gr',\n",
              " 'ra',\n",
              " 'an',\n",
              " 'nd',\n",
              " 'd_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'aî',\n",
              " 'ît',\n",
              " 'tr',\n",
              " 're',\n",
              " 'e_',\n",
              " '_d',\n",
              " 'di',\n",
              " 'it',\n",
              " 't_',\n",
              " '_o',\n",
              " 'on',\n",
              " 'n_',\n",
              " '_l',\n",
              " 'le',\n",
              " 'e_',\n",
              " '_m',\n",
              " 'ma',\n",
              " 'al',\n",
              " 'lh',\n",
              " 'he',\n",
              " 'eu',\n",
              " 'ur',\n",
              " 'r_',\n",
              " '_e',\n",
              " 'es',\n",
              " 'st',\n",
              " 't_',\n",
              " '_q',\n",
              " 'qu',\n",
              " \"u'\",\n",
              " \"'i\",\n",
              " 'il',\n",
              " 'l_',\n",
              " '_t',\n",
              " 'tu',\n",
              " 'ue',\n",
              " 'e_',\n",
              " '_s',\n",
              " 'se',\n",
              " 'es',\n",
              " 's_',\n",
              " '_é',\n",
              " 'él',\n",
              " 'lè',\n",
              " 'èv',\n",
              " 've',\n",
              " 'es',\n",
              " 'le_',\n",
              " '_te',\n",
              " 'tem',\n",
              " 'emp',\n",
              " 'mps',\n",
              " 'ps_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_un',\n",
              " 'un_',\n",
              " '_gr',\n",
              " 'gra',\n",
              " 'ran',\n",
              " 'and',\n",
              " 'nd_',\n",
              " '_ma',\n",
              " 'maî',\n",
              " 'aît',\n",
              " 'îtr',\n",
              " 'tre',\n",
              " 're_',\n",
              " '_di',\n",
              " 'dit',\n",
              " 'it_',\n",
              " '_on',\n",
              " 'on_',\n",
              " '_le',\n",
              " 'le_',\n",
              " '_ma',\n",
              " 'mal',\n",
              " 'alh',\n",
              " 'lhe',\n",
              " 'heu',\n",
              " 'eur',\n",
              " 'ur_',\n",
              " '_es',\n",
              " 'est',\n",
              " 'st_',\n",
              " '_qu',\n",
              " \"qu'\",\n",
              " \"u'i\",\n",
              " \"'il\",\n",
              " 'il_',\n",
              " '_tu',\n",
              " 'tue',\n",
              " 'ue_',\n",
              " '_se',\n",
              " 'ses',\n",
              " 'es_',\n",
              " '_él',\n",
              " 'élè',\n",
              " 'lèv',\n",
              " 'ève',\n",
              " 'ves',\n",
              " '_tem',\n",
              " 'temp',\n",
              " 'emps',\n",
              " 'mps_',\n",
              " '_est',\n",
              " 'est_',\n",
              " '_un_',\n",
              " '_gra',\n",
              " 'gran',\n",
              " 'rand',\n",
              " 'and_',\n",
              " '_maî',\n",
              " 'maît',\n",
              " 'aîtr',\n",
              " 'ître',\n",
              " 'tre_',\n",
              " '_dit',\n",
              " 'dit_',\n",
              " '_on_',\n",
              " '_le_',\n",
              " '_mal',\n",
              " 'malh',\n",
              " 'alhe',\n",
              " 'lheu',\n",
              " 'heur',\n",
              " 'eur_',\n",
              " '_est',\n",
              " 'est_',\n",
              " \"_qu'\",\n",
              " \"qu'i\",\n",
              " \"u'il\",\n",
              " \"'il_\",\n",
              " '_tue',\n",
              " 'tue_',\n",
              " '_ses',\n",
              " 'ses_',\n",
              " '_élè',\n",
              " 'élèv',\n",
              " 'lève',\n",
              " 'èves']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.2: Detecting Text Language by Counting Stopwords** \n",
        "\n",
        "Based on [Detecting Text Language With Python and NLTK by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/)\n",
        "\n",
        "Stop words are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature (e.g. search engines remove words like 'want')"
      ],
      "metadata": {
        "id": "gvbJKxQRdPrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize text, regex-based tokenizer splitting text on whitespace and punctuation (except for underscore)\n",
        "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\"\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize import wordpunct_tokenize \n",
        "except ImportError:\n",
        "    print('[!] You need to install nltk (http://nltk.org/index.html)')\n",
        "test_tokens = wordpunct_tokenize(text)\n",
        "test_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-ImW1h0dPNJ",
        "outputId": "800a8e03-2f3f-4944-a54d-1aec3b80e0c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yo',\n",
              " 'man',\n",
              " ',',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'time',\n",
              " 'for',\n",
              " 'you',\n",
              " 'to',\n",
              " 'shut',\n",
              " 'yo',\n",
              " \"'\",\n",
              " 'mouth',\n",
              " '!',\n",
              " 'I',\n",
              " 'ain',\n",
              " \"'\",\n",
              " 't',\n",
              " 'even',\n",
              " 'messin',\n",
              " \"'\",\n",
              " 'dawg',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other tokenizers e.g. RegexpTokenizer where you can enter your own regexp, WhitespaceTokenizer (similar to Python's string.split()) and BlanklineTokenizer."
      ],
      "metadata": {
        "id": "_D-Ytvpy1v8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring NLTK Stopword Corpus** \n",
        "\n",
        "NLTK comes with a corpus of stop words in various languages."
      ],
      "metadata": {
        "id": "kl9cLYTb11Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Since this is raw text, we need to replace \\n's with spaces for it to be readable.\n",
        "stopwords.readme().replace('\\n', ' ') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "hvpqeg0R2DJr",
        "outputId": "199e14bd-9946-40f7-975c-61bb7fd8efcd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files is accessed via fileids(). Here you can see the list of languages the corpus contains\n",
        "stopwords.fileids() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RzNbpi72VQD",
        "outputId": "e2ddc915-e793-4095-9d61-1b76d2903607"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'bengali',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore raw corpus of Greek stopwords\n",
        "stopwords.raw('greek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Fy_-YGTs2wSi",
        "outputId": "9e40fb0c-b7cb-4bee-bf37-d0c2cced542e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"αλλα\\nαν\\nαντι\\nαπο\\nαυτα\\nαυτεσ\\nαυτη\\nαυτο\\nαυτοι\\nαυτοσ\\nαυτουσ\\nαυτων\\nαἱ\\nαἳ\\nαἵ\\nαὐτόσ\\nαὐτὸς\\nαὖ\\nγάρ\\nγα\\nγα^\\nγε\\nγια\\nγοῦν\\nγὰρ\\nδ'\\nδέ\\nδή\\nδαί\\nδαίσ\\nδαὶ\\nδαὶς\\nδε\\nδεν\\nδι'\\nδιά\\nδιὰ\\nδὲ\\nδὴ\\nδ’\\nεαν\\nειμαι\\nειμαστε\\nειναι\\nεισαι\\nειστε\\nεκεινα\\nεκεινεσ\\nεκεινη\\nεκεινο\\nεκεινοι\\nεκεινοσ\\nεκεινουσ\\nεκεινων\\nενω\\nεπ\\nεπι\\nεἰ\\nεἰμί\\nεἰμὶ\\nεἰς\\nεἰσ\\nεἴ\\nεἴμι\\nεἴτε\\nη\\nθα\\nισωσ\\nκ\\nκαί\\nκαίτοι\\nκαθ\\nκαι\\nκατ\\nκατά\\nκατα\\nκατὰ\\nκαὶ\\nκι\\nκἀν\\nκἂν\\nμέν\\nμή\\nμήτε\\nμα\\nμε\\nμεθ\\nμετ\\nμετά\\nμετα\\nμετὰ\\nμη\\nμην\\nμἐν\\nμὲν\\nμὴ\\nμὴν\\nνα\\nο\\nοι\\nομωσ\\nοπωσ\\nοσο\\nοτι\\nοἱ\\nοἳ\\nοἷς\\nοὐ\\nοὐδ\\nοὐδέ\\nοὐδείσ\\nοὐδεὶς\\nοὐδὲ\\nοὐδὲν\\nοὐκ\\nοὐχ\\nοὐχὶ\\nοὓς\\nοὔτε\\nοὕτω\\nοὕτως\\nοὕτωσ\\nοὖν\\nοὗ\\nοὗτος\\nοὗτοσ\\nπαρ\\nπαρά\\nπαρα\\nπαρὰ\\nπερί\\nπερὶ\\nποια\\nποιεσ\\nποιο\\nποιοι\\nποιοσ\\nποιουσ\\nποιων\\nποτε\\nπου\\nποῦ\\nπρο\\nπροσ\\nπρόσ\\nπρὸ\\nπρὸς\\nπως\\nπωσ\\nσε\\nστη\\nστην\\nστο\\nστον\\nσόσ\\nσύ\\nσύν\\nσὸς\\nσὺ\\nσὺν\\nτά\\nτήν\\nτί\\nτίς\\nτίσ\\nτα\\nταῖς\\nτε\\nτην\\nτησ\\nτι\\nτινα\\nτις\\nτισ\\nτο\\nτοί\\nτοι\\nτοιοῦτος\\nτοιοῦτοσ\\nτον\\nτοτε\\nτου\\nτούσ\\nτοὺς\\nτοῖς\\nτοῦ\\nτων\\nτό\\nτόν\\nτότε\\nτὰ\\nτὰς\\nτὴν\\nτὸ\\nτὸν\\nτῆς\\nτῆσ\\nτῇ\\nτῶν\\nτῷ\\nωσ\\nἀλλ'\\nἀλλά\\nἀλλὰ\\nἀλλ’\\nἀπ\\nἀπό\\nἀπὸ\\nἀφ\\nἂν\\nἃ\\nἄλλος\\nἄλλοσ\\nἄν\\nἄρα\\nἅμα\\nἐάν\\nἐγώ\\nἐγὼ\\nἐκ\\nἐμόσ\\nἐμὸς\\nἐν\\nἐξ\\nἐπί\\nἐπεὶ\\nἐπὶ\\nἐστι\\nἐφ\\nἐὰν\\nἑαυτοῦ\\nἔτι\\nἡ\\nἢ\\nἣ\\nἤ\\nἥ\\nἧς\\nἵνα\\nὁ\\nὃ\\nὃν\\nὃς\\nὅ\\nὅδε\\nὅθεν\\nὅπερ\\nὅς\\nὅσ\\nὅστις\\nὅστισ\\nὅτε\\nὅτι\\nὑμόσ\\nὑπ\\nὑπέρ\\nὑπό\\nὑπὲρ\\nὑπὸ\\nὡς\\nὡσ\\nὥς\\nὥστε\\nὦ\\nᾧ\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Greek stopwards by \\n with space\n",
        "stopwords.raw('greek').replace('\\n', ' ') # Better"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "vb2e4r3f21zI",
        "outputId": "39b45e80-50fc-4968-c07f-11622132f898"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"αλλα αν αντι απο αυτα αυτεσ αυτη αυτο αυτοι αυτοσ αυτουσ αυτων αἱ αἳ αἵ αὐτόσ αὐτὸς αὖ γάρ γα γα^ γε για γοῦν γὰρ δ' δέ δή δαί δαίσ δαὶ δαὶς δε δεν δι' διά διὰ δὲ δὴ δ’ εαν ειμαι ειμαστε ειναι εισαι ειστε εκεινα εκεινεσ εκεινη εκεινο εκεινοι εκεινοσ εκεινουσ εκεινων ενω επ επι εἰ εἰμί εἰμὶ εἰς εἰσ εἴ εἴμι εἴτε η θα ισωσ κ καί καίτοι καθ και κατ κατά κατα κατὰ καὶ κι κἀν κἂν μέν μή μήτε μα με μεθ μετ μετά μετα μετὰ μη μην μἐν μὲν μὴ μὴν να ο οι ομωσ οπωσ οσο οτι οἱ οἳ οἷς οὐ οὐδ οὐδέ οὐδείσ οὐδεὶς οὐδὲ οὐδὲν οὐκ οὐχ οὐχὶ οὓς οὔτε οὕτω οὕτως οὕτωσ οὖν οὗ οὗτος οὗτοσ παρ παρά παρα παρὰ περί περὶ ποια ποιεσ ποιο ποιοι ποιοσ ποιουσ ποιων ποτε που ποῦ προ προσ πρόσ πρὸ πρὸς πως πωσ σε στη στην στο στον σόσ σύ σύν σὸς σὺ σὺν τά τήν τί τίς τίσ τα ταῖς τε την τησ τι τινα τις τισ το τοί τοι τοιοῦτος τοιοῦτοσ τον τοτε του τούσ τοὺς τοῖς τοῦ των τό τόν τότε τὰ τὰς τὴν τὸ τὸν τῆς τῆσ τῇ τῶν τῷ ωσ ἀλλ' ἀλλά ἀλλὰ ἀλλ’ ἀπ ἀπό ἀπὸ ἀφ ἂν ἃ ἄλλος ἄλλοσ ἄν ἄρα ἅμα ἐάν ἐγώ ἐγὼ ἐκ ἐμόσ ἐμὸς ἐν ἐξ ἐπί ἐπεὶ ἐπὶ ἐστι ἐφ ἐὰν ἑαυτοῦ ἔτι ἡ ἢ ἣ ἤ ἥ ἧς ἵνα ὁ ὃ ὃν ὃς ὅ ὅδε ὅθεν ὅπερ ὅς ὅσ ὅστις ὅστισ ὅτε ὅτι ὑμόσ ὑπ ὑπέρ ὑπό ὑπὲρ ὑπὸ ὡς ὡσ ὥς ὥστε ὦ ᾧ \""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#List first 10 English stopwords\n",
        "stopwords.words('english')[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URw5kJmJ29sW",
        "outputId": "0919fc4e-839a-49fb-c7f1-c03f7f72cdd1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use .sents() which returns sentences. However, in our particular case, this will cause an error:"
      ],
      "metadata": {
        "id": "SwhMC1Lt3Gr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.sents('greek')"
      ],
      "metadata": {
        "id": "62ANltCg3D7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error is because the stopwords corpus reader is of type WordListCorpusReader so there are no sentences. It's the same for .paras()."
      ],
      "metadata": {
        "id": "XYocPA8t3mdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of stopwords in English and Greek. There is a total of 444 Greek and English stop words\n",
        "len(stopwords.words(['english', 'greek'])) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bnkkt54N3rG3",
        "outputId": "6f56a5de-4930-44fb-a4fb-eeaaa8c8820e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "444"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Classifying texts based on stopwords:*** Loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words"
      ],
      "metadata": {
        "id": "1cf_06_2303X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create empty list to store ratio of stopwords in each language\n",
        "language_ratios = {}\n",
        "\n",
        "# lowercase all tokens in test_tokens (\"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\")\n",
        "test_words = [word.lower() for word in test_tokens] \n",
        "test_words_set = set(test_words)\n",
        "\n",
        "#Iterate through test_tokens and output number of stopwords in each language the string contains\n",
        "for language in stopwords.fileids():\n",
        "    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
        "    common_elements = test_words_set.intersection(stopwords_set)\n",
        "    language_ratios[language] = len(common_elements) # language \"score\"\n",
        "    \n",
        "language_ratios"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7GSjR5-32Kg",
        "outputId": "d5398f47-d033-497d-e435-e2801ad90b7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arabic': 0,\n",
              " 'azerbaijani': 0,\n",
              " 'bengali': 0,\n",
              " 'danish': 3,\n",
              " 'dutch': 0,\n",
              " 'english': 8,\n",
              " 'finnish': 0,\n",
              " 'french': 2,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 0,\n",
              " 'italian': 1,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 3,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 2,\n",
              " 'russian': 0,\n",
              " 'slovene': 2,\n",
              " 'spanish': 1,\n",
              " 'swedish': 2,\n",
              " 'tajik': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are determining the language for which the most stopwords were found in the string\n",
        "# The key parameter to the max() function is a function that computes a key. \n",
        "# In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\n",
        "most_rated_language = max(language_ratios, key=language_ratios.get) \n",
        "most_rated_language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LmpdSKIo5QUa",
        "outputId": "13ae8ef6-e157-4ed6-f212-eb6d93d20900"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'english'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print stopwords in most rated language appearing in string (here it's English)\n",
        "test_words_set.intersection(set(stopwords.words(most_rated_language)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNeSJ2wj5Ca7",
        "outputId": "cba7b104-56bd-4b12-8b60-5891e31a51a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ain', 'for', 'i', 'it', 's', 't', 'to', 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2.3: Language Identifier Using Word Bigrams** \n",
        "\n",
        "Based on [this language identifier program on Github.](https://github.com/asif31iqbal/language-identifier)"
      ],
      "metadata": {
        "id": "3m6tGv3l6DrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries \n",
        "import pickle\n",
        "import string\n",
        "import os\n",
        "from nltk import ngrams, FreqDist, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from numpy import arange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#Create tokenizer method\n",
        "def ultimate_tokenize(sentence):\n",
        "    # Remove punctuation and digits\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    return word_tokenize(sentence.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmzBMGUK6jSP",
        "outputId": "95560226-75cf-4d16-937d-605573e8757b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generating Word Tokens:*** Divide text into one-word slices; typically followed by frequency calcuations to determine language of origin"
      ],
      "metadata": {
        "id": "TAmj_wzw72OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate word tokens from sample string\n",
        "simple_example_text = 'Oh, then, I see Queen Mab hath been with you.'\n",
        "\n",
        "simple_example_tokens_words = ultimate_tokenize(simple_example_text)\n",
        "simple_example_tokens_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb5qEsaK61As",
        "outputId": "500e242f-c3ef-47d8-eee0-763584e2417f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh', 'then', 'i', 'see', 'queen', 'mab', 'hath', 'been', 'with', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Break word token into characters and print characters in token in position 0\n",
        "simple_example_tokens_chars = list(simple_example_tokens_words[0])\n",
        "simple_example_tokens_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C-wd5Cm7G4s",
        "outputId": "2bd9f38b-6145-4fc6-9cdf-d20109d579a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o', 'h']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate and print list of 1-word word tokens (unigrams)\n",
        "simple_example_tokens_words_unigrams = list(ngrams(simple_example_tokens_words, 1))\n",
        "simple_example_tokens_words_unigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVsn48ad7PNP",
        "outputId": "e7335262-fba4-4aa0-e760-f34f7022ef2a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('oh',),\n",
              " ('then',),\n",
              " ('i',),\n",
              " ('see',),\n",
              " ('queen',),\n",
              " ('mab',),\n",
              " ('hath',),\n",
              " ('been',),\n",
              " ('with',),\n",
              " ('you',)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate and print list of 2-word word tokends (bigrams)\n",
        "simple_example_tokens_words_bigrams = list(ngrams(simple_example_tokens_words, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))\n",
        "simple_example_tokens_words_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJy9tj8t7fOP",
        "outputId": "b5f375df-7443-4763-b4fb-1e2fc8d19050"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_', 'oh'),\n",
              " ('oh', 'then'),\n",
              " ('then', 'i'),\n",
              " ('i', 'see'),\n",
              " ('see', 'queen'),\n",
              " ('queen', 'mab'),\n",
              " ('mab', 'hath'),\n",
              " ('hath', 'been'),\n",
              " ('been', 'with'),\n",
              " ('with', 'you'),\n",
              " ('you', '_')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create frequency distribution of word token unigrams\n",
        "fdist = FreqDist(simple_example_tokens_words_unigrams)\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zNCVNOk8RvU",
        "outputId": "352a3f35-7323-40fb-c003-ee1fc2b71caf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('been',): 1,\n",
              "          ('hath',): 1,\n",
              "          ('i',): 1,\n",
              "          ('mab',): 1,\n",
              "          ('oh',): 1,\n",
              "          ('queen',): 1,\n",
              "          ('see',): 1,\n",
              "          ('then',): 1,\n",
              "          ('with',): 1,\n",
              "          ('you',): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary for the unigrams; keys = word tokens, values = frequency counts\n",
        "unigram_dict = dict()\n",
        "for k, v in fdist.items():\n",
        "        unigram_dict[' '.join(k)] = v\n",
        "unigram_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-KpM5A48f9c",
        "outputId": "6a7cf027-9b40-41e1-90e2-efb965e47abe"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'been': 1,\n",
              " 'hath': 1,\n",
              " 'i': 1,\n",
              " 'mab': 1,\n",
              " 'oh': 1,\n",
              " 'queen': 1,\n",
              " 'see': 1,\n",
              " 'then': 1,\n",
              " 'with': 1,\n",
              " 'you': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Open file (uploaded to Drive) and lowercase, replace \\n with spaces and list first 100 characters\n",
        "file = '/content/drive/MyDrive/hm3_files/LangId.train.English'\n",
        "with open(file, encoding='utf8') as f:\n",
        "        content = f.read().lower()\n",
        "content.replace('\\n', '')[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TfQH3LkV94A0",
        "outputId": "d1f293b7-a1ab-4ef3-f8fa-8a6b24ba31a3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"approval of the minutes of the previous sitting the minutes of yesterday 's sitting have been distri\""
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Open file (uploaded to Drive) and serialize (flatten) English unigram dictionary (???)\n",
        "with open('/content/drive/MyDrive/hm3_files/English.unigram.pickle', 'rb') as handle:\n",
        "    unigram_english_dict = pickle.load(handle)\n",
        "unigram_english_dict"
      ],
      "metadata": {
        "id": "zEL8tw6D9_La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open file (uploaded to Drive) and serialize (flatten) English bigram dictionary (???)\n",
        "with open('/content/drive/MyDrive/hm3_files/English.bigram.pickle', 'rb') as handle:\n",
        "    bigram_english_dict = pickle.load(handle)\n",
        "bigram_english_dict"
      ],
      "metadata": {
        "id": "CmF90L4j-Ecq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count frequency of phrase in bigram english dictionary\n",
        "bigram_english_dict.get('of the')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS6_3qDA-KXT",
        "outputId": "3fb2eb49-bd4a-4f32-ef7c-e9afd79ab2de"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "904"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import operator and sort unigram frequencies from highest to lowest in imported dictionary, list top 10 \n",
        "import operator\n",
        "english_unigram_freqs = sorted(unigram_english_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "english_unigram_freqs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxsd6SPT-QaA",
        "outputId": "69a96e76-90f7-446e-8bba-0d8e6dbd5776"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5698),\n",
              " (',', 3853),\n",
              " ('.', 2829),\n",
              " ('of', 2769),\n",
              " ('to', 2490),\n",
              " ('and', 2040),\n",
              " ('in', 1668),\n",
              " ('is', 1303),\n",
              " ('a', 1301),\n",
              " ('that', 1205)]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create histogram of top 10 English word unigrams"
      ],
      "metadata": {
        "id": "J3vTuX7b9-iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set lables and values as corresponding to top ten most frequent English unigrams\n",
        "#Arrange indexes by length of labels and set height\n",
        "labels, values = zip(*english_unigram_freqs[:10])\n",
        "indexes = arange(len(labels))\n",
        "width = 0.8 # width = 1 would give bars that overlap because they are too close.\n",
        "\n",
        "#Size plot, axes and bars\n",
        "fig = plt.figure(figsize=(10,7))                                                               \n",
        "ax = fig.gca() # Get current axis\n",
        "rects = ax.bar(indexes, values, width)\n",
        "\n",
        "# Add title and axis labels\n",
        "fig.suptitle('Top 10 English word unigrams', fontsize=20)\n",
        "plt.xlabel('Word unigram', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Display value of each bar on bar\n",
        "for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x() + rect.get_width() / 2., 50 + height, '%d' % int(height), ha='center', va='bottom') # Can also add color and fontweight arguments.\n",
        "\n",
        "# Remove the default x-axis tick numbers and use tick numbers of your own choosing:\n",
        "ax.set_xticks(indexes)\n",
        "# Replace the tick numbers with strings:\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "plt.show()\n",
        "# plt.savefig('top10EnglishWordUnigrams.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "rq4j7gVm-TtN",
        "outputId": "d103d433-e14e-483c-cfca-21d4a480e1e9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating unigram and bigram frequencies for English, French and Italian from training files"
      ],
      "metadata": {
        "id": "B9L0PaTOD1Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DOES NOT RUN YET\n",
        "def get_ngram_count_dict(tokens, n):\n",
        "    if n == 1:\n",
        "        n_grams = ngrams(tokens, n)\n",
        "    else:\n",
        "        n_grams = ngrams(tokens, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_') # Fun fact: If I remove padding here and later when testing, and also remove the '_' from the unigram dicts, the accuracy rises slightly. However, it's not statistically significant due to the small size of the data.\n",
        "    fdist = FreqDist(n_grams)\n",
        "    ngram_dict = dict()\n",
        "    for k,v in fdist.items():\n",
        "        ngram_dict[' '.join(k)] = v\n",
        "    return ngram_dict\n",
        "\n",
        "# Calls get_ngram_count_dict to get a unigram and bigram dict from file.\n",
        "def get_unigram_bigram_dicts(file):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        content = f.read()\n",
        "    tokens = ultimate_tokenize(content)\n",
        "    unigram_dict = get_ngram_count_dict(tokens, 1)     \n",
        "    bigram_dict = get_ngram_count_dict(tokens, 2)     \n",
        "    return (unigram_dict, bigram_dict)\n",
        "\n",
        "# Dumps unigram and bigram dictionary of training data of given language to .pickle files.\n",
        "def dump_pickle(language):\n",
        "    file = 'ngram_langid_files/LangId.train.' + language + '.txt'\n",
        "    unigram_dict, bigram_dict = get_unigram_bigram_dicts(file)\n",
        "    with open('ngram_langid_files/' + language + '.unigram.pickle', 'wb') as handle:\n",
        "        pickle.dump(unigram_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) # HIGHEST_PROTOCOL instructs pickle to use the highest protocol version available.\n",
        "    with open('ngram_langid_files/' + language + '.bigram.pickle', 'wb') as handle:\n",
        "        pickle.dump(bigram_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "dump_pickle('English')\n",
        "dump_pickle('French')\n",
        "dump_pickle('Italian')"
      ],
      "metadata": {
        "id": "pUhFFiI_D6gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later, it will also be required to know how many sentences there are in the training data for each language. This is because of the method used to calculate probabilities (incorporating the probability of the bigram among other bigrams starting with the same word) and the fact we use padding for our bigrams.\n",
        "\n",
        "In our training data each line is a sentence, which is very convenient for calculating the number of sentences.\n",
        "\n",
        "We go ahead and get the number of sentences (for more efficiency, the following code could be added to get_unigram_bigram_dicts):"
      ],
      "metadata": {
        "id": "569OuqYJD_ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ngram_langid_files/LangId.train.English.txt', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_en = i + 1\n",
        "with open('ngram_langid_files/LangId.train.French.txt', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_fr = i + 1\n",
        "with open('ngram_langid_files/LangId.train.Italian.txt', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_it = i + 1\n",
        "\n",
        "print('NUMBER OF SENTENCES IN TRAINING DATA')\n",
        "print('English:', number_of_sents_en)\n",
        "print('French:', number_of_sents_fr)\n",
        "print('Italian:', number_of_sents_it)"
      ],
      "metadata": {
        "id": "1cMvu9SIEGfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying language for each line of the test file using bigram probabilities"
      ],
      "metadata": {
        "id": "hBLLKszQEHz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ngram_langid_files/English.unigram.pickle', 'rb') as handle:\n",
        "    unigram_english_dict = pickle.load(handle)\n",
        "    \n",
        "with open('ngram_langid_files/English.bigram.pickle', 'rb') as handle:\n",
        "    bigram_english_dict = pickle.load(handle)\n",
        "    \n",
        "with open('ngram_langid_files/French.unigram.pickle', 'rb') as handle:\n",
        "    unigram_french_dict = pickle.load(handle)\n",
        "    \n",
        "with open('ngram_langid_files/French.bigram.pickle', 'rb') as handle:\n",
        "    bigram_french_dict = pickle.load(handle)\n",
        "    \n",
        "with open('ngram_langid_files/Italian.unigram.pickle', 'rb') as handle:\n",
        "    unigram_italian_dict = pickle.load(handle)\n",
        "    \n",
        "with open('ngram_langid_files/Italian.bigram.pickle', 'rb') as handle:\n",
        "    bigram_italian_dict = pickle.load(handle)\n",
        "    \n",
        "vocabulary_size = len(unigram_english_dict) + len(unigram_french_dict) + len(unigram_italian_dict)\n",
        "vocabulary_size"
      ],
      "metadata": {
        "id": "z9hQ0GJ-EKEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get probability of given bigram belonging to the language which bigram_dict is in\n",
        "def get_bigram_probability(bigram, first_word, bigram_dict, first_word_dict): # first_word is the first word of the word bigram.\n",
        "    bigram_count = bigram_dict.get(bigram)\n",
        "    if bigram_count is None:\n",
        "        bigram_count = 0\n",
        "    \n",
        "    first_word_count = first_word_dict.get(first_word)\n",
        "    if first_word_count is None:\n",
        "        first_word_count = 0\n",
        "    \n",
        "    return (bigram_count + 1) / (first_word_count + vocabulary_size) # To get the logic of this formula, note how the proability is used in the function below. Without the + 1 in the Nr, if you find a bigram which is not in our known bigrams for a language, the probability of it being in that language would become 0. So we would like to assign a small probability of 1 / vocabulary_size in that case. Also note the arbitrariness of this 'probability'. We're saying \"Given a bigram and a language, what is the probability that the bigram is of that language?\" This is arbitrary because to get a meaningful probability we need to know which are the other languages considered and what their bigram frequencies are. That would be another way to do it, but arguable a worse one because it wouldn't be able to give a confidence score for a particular language. The formula just uses common sense to get to a number which works for the purposes. In the denominator, we have both first_word_count and vocabulary_size. Why? We have vocabulary_size for all langs in the denom because the larger this is, the less significant it is that for this particular language the bigram appears so many times. Could we have used a vocab_size of bigrams instead of unigrams? Sure, and the 'probabilities' would end up being much smaller numbers. What about first_word_count? This gives us a way to compare this bigram against other bigrams in this language starting with the same word. In general though, for a given bigram, it's more important to consider how many times it exists than to consider whether it is the usual bigram given a certain first word. The formula achieves that. Take the bigram 'le monseiur' and the English language. Let's say the bigram appears once and 'le' also appears once, while in French 'le monseiur' appears 100 times and le appears 100,000 times. Probability for English = (1 + 1) / (1 + 20,000) = 0.000099995. Probability for French = (100 + 1) / (100,000 + 20,000) = 0.00084166666. Note how the probability for French is still low because 100/100,000 is quite low and maybe it's not French after all if in French le is usually followed by other words. However, it's still significantly higher than the probability for English where both 'le' and 'le monseiur' only appear once.\n",
        "\n",
        "# Get probability that a given bigram list is of a language (specified by its bigram_dict)\n",
        "def get_language_probability(bigram_list, first_words, bigram_dict, first_word_dict):\n",
        "    result = 1.0\n",
        "    index = 0\n",
        "    for bigram in bigram_list:\n",
        "        result *= get_bigram_probability(bigram, first_words[index], bigram_dict, first_word_dict)\n",
        "        index += 1\n",
        "    return result\n",
        "\n",
        "# Load correct solutions\n",
        "solution_dict = dict()\n",
        "with open('ngram_langid_files/LangId.sol.txt') as f:\n",
        "    for line in f:\n",
        "       (key, val) = line.split()\n",
        "       solution_dict[int(key)] = val\n",
        "        \n",
        "line_no = 1\n",
        "result_dict = dict()\n",
        "correct = 0\n",
        "incorrect_line_numbers = []\n",
        "\n",
        "# This needs to be done because I'm using padding for bigrams so the unigram dicts in their raw forms can't be used in get_bigram_probability():\n",
        "unigram_english_dict['_'] = number_of_sents_en\n",
        "unigram_french_dict['_'] = number_of_sents_fr\n",
        "unigram_italian_dict['_'] = number_of_sents_it\n",
        "\n",
        "with open('ngram_langid_files/LangId.test.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        tokens = ultimate_tokenize(line)\n",
        "        bigrams = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')\n",
        "        bigram_list = [] # bigram_list will be exactly like bigrams but instead of [('_', 'this'), ...] it will be ['_ this', ...]. It is required because this is how bigrams are represented in the dictionary.\n",
        "        first_words = [] # The first words of each bigram. This is the similar to making a unigram_list. We use it because we don't want something in the form [(this,), ...]. Also because we want this to include '_'. We want it to include '_' because we're not using the unigrams for classification but as part of a formula to judge bigram frequency based on the starting word.\n",
        "        for b in bigrams:\n",
        "            bigram_list.append(' '.join(b))\n",
        "            first_words.append(b[0])\n",
        "        \n",
        "        english_prob = get_language_probability(bigram_list, first_words, bigram_english_dict, unigram_english_dict)\n",
        "        french_prob = get_language_probability(bigram_list, first_words, bigram_french_dict, unigram_french_dict)\n",
        "        italian_prob = get_language_probability(bigram_list, first_words, bigram_italian_dict, unigram_italian_dict)\n",
        "        \n",
        "        max_prob = max(english_prob, french_prob, italian_prob)\n",
        "        if max_prob == english_prob:\n",
        "            result_dict[line_no] = 'English'\n",
        "        elif max_prob == french_prob:\n",
        "            result_dict[line_no] = 'French'\n",
        "        else:\n",
        "            result_dict[line_no] = 'Italian'\n",
        "        \n",
        "        if solution_dict[line_no] == result_dict[line_no]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect_line_numbers.append(line_no)\n",
        "            \n",
        "        line_no += 1\n",
        "\n",
        "# Storing results from result_dict to file:\n",
        "with open('ngram_langid_files/LangId.result.txt', 'w') as f:\n",
        "    for (key, val) in result_dict.items():\n",
        "        f.write(' '.join([str(key), val]) + '\\n')\n",
        "        \n",
        "print('Accuracy: {:2.2f}%'.format(correct * 100 / len(solution_dict)))"
      ],
      "metadata": {
        "id": "nifhpoxrER8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Line numbers for incorrectly classified languages: {}'.format(str(incorrect_line_numbers)))\n"
      ],
      "metadata": {
        "id": "MYV1kZZqEZq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with our own sentence"
      ],
      "metadata": {
        "id": "K_7jANRoEaFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"This is a sentence.\"\n",
        "sent_tokens = ultimate_tokenize(sent)\n",
        "sent_bigrams_pre = ngrams(sent_tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')\n",
        "sent_bigrams = []\n",
        "sent_bigrams_first_words = []\n",
        "for b in sent_bigrams_pre:\n",
        "    sent_bigrams.append(' '.join(b))\n",
        "    sent_bigrams_first_words.append(b[0])\n",
        "print('Sentence bigrams:', sent_bigrams)\n",
        "print('Sentence bigrams first words:', sent_bigrams_first_words)"
      ],
      "metadata": {
        "id": "C4FFMzrLEcdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_english_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_english_dict, unigram_english_dict)\n",
        "sent_french_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_french_dict, unigram_french_dict)\n",
        "sent_italian_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_italian_dict, unigram_italian_dict)\n",
        "print(\"RAW 'PROBABILITIES'\")\n",
        "print('English:', sent_english_prob)\n",
        "print('French:', sent_french_prob)\n",
        "print('Italian:', sent_italian_prob)\n",
        "# As we can see, these 'probabilities' are arbitrary. We can try to convert them to percentages since we are classifying only among these 3 languages:"
      ],
      "metadata": {
        "id": "LTHcjjlUEfnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalized_probabilities(list_of_probabilities):\n",
        "    sum_of_probabilities = sum(list_of_probabilities)\n",
        "    result = []\n",
        "    for probability in list_of_probabilities:\n",
        "        result.append(probability / sum_of_probabilities)\n",
        "    return result\n",
        "\n",
        "probabilities = [sent_english_prob, sent_french_prob, sent_italian_prob]\n",
        "normalized_probabilities = get_normalized_probabilities(probabilities)\n",
        "\n",
        "print('RELATIVE PROBABILITIES')\n",
        "print('English: ', round(normalized_probabilities[0] * 100, 2), '%', sep='') # I use sep because I don't want a space before the % sign.\n",
        "print('French: ', round(normalized_probabilities[1] * 100, 2), '%', sep='')\n",
        "print('Italian: ', round(normalized_probabilities[2] * 100, 2), '%', sep='')\n"
      ],
      "metadata": {
        "id": "DjN3HJfzEiGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PS: For a state-of-the-art Greek dialect classifier using n-grams, take a look at [Greek Dialect Classifier.](https://github.com/hb20007/greek-dialect-classifier)\n"
      ],
      "metadata": {
        "id": "Rx4rxAa6Ejwc"
      }
    }
  ]
}