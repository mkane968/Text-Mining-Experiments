{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with NLTK",
      "provenance": [],
      "authorship_tag": "ABX9TyOcNORs6vRd1MHAPMrI7GV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/NLTK/Text_Classification_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Classification with NLTK \n",
        "\n",
        "Below, Naive Bayes and maximum entropy classifiers will be used to sort names by gender and texts into genre and sentiment categories. Two other tools for sentiment analysis, SentimentAnalyzer and Vader will also be explored."
      ],
      "metadata": {
        "id": "CGI4z1WQaBpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Name Gender Identification with Naive Bayes Classifers \n",
        "\n",
        "A classifier can be built to predict gender based on the last letter of a name.\n",
        "\n",
        "First, build a feature extractor to retrieve the last letter of a word. The returned dictionary is called a feature set."
      ],
      "metadata": {
        "id": "yPn9LK5VA5E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features(word):  \n",
        "    return {'last_letter': word[-1]}\n",
        "\n",
        "gender_features('John')"
      ],
      "metadata": {
        "id": "w8HLePJIeg1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the classifier, we need to prepare a list of examples and corresponding class labels. Retrieve examples from \"female.txt\" and \"male.txt\" in the NLTK names corpus and create a list of labeled names. "
      ],
      "metadata": {
        "id": "cYZ05zr3Cucr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Open and clean names corpus\n",
        "names.readme().replace('\\n', ' ')\n",
        "\n",
        "#Get files in names corpus\n",
        "names.fileids()\n",
        "\n",
        "#Print first 5 words in female names text file\n",
        "print(names.words('female.txt')[:5])\n",
        "\n",
        "#Create list of labeled names where names in female.txt file are labeled female and male.txt names labeled male\n",
        "labeled_names = ([(name, 'female') for name in names.words('female.txt')] + [(name, 'male') for name in names.words('male.txt')])\n",
        "\n",
        "#Print first five in labeled names list\n",
        "labeled_names[:5]"
      ],
      "metadata": {
        "id": "o_wMsOLhCui4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both training and testing data are needed for the classifier, so shuffle names in labeled_names"
      ],
      "metadata": {
        "id": "IyDrYrnXEKpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(labeled_names) \n",
        "labeled_names[:5]"
      ],
      "metadata": {
        "id": "wHzLMQUdET59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a list of the last letter of each name in labeled names and thecorresponding gender; this is the information will be used to train and test the classifier. "
      ],
      "metadata": {
        "id": "-s5NMLl0EviN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract last letter of each name in labeled_names and place in list along with gender\n",
        "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
        "\n",
        "#Check length of new list\n",
        "print(len(featuresets))\n",
        "\n",
        "#Print first five entires in new list\n",
        "(featuresets[:5])"
      ],
      "metadata": {
        "id": "bb6Wv-20EvpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the featuresets lists into a training set (80% of list) and a test set (20% of list). We also need the names from the training set, to be used later. "
      ],
      "metadata": {
        "id": "ZT_i2mD5E-87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split featuressets into test and training sets\n",
        "TRAIN_SET_SIZE = round(len(featuresets) * .8)\n",
        "train_set, test_set = featuresets[:TRAIN_SET_SIZE], featuresets[TRAIN_SET_SIZE:]\n",
        "\n",
        "#Store names in test set in new variable\n",
        "test_names = labeled_names[TRAIN_SET_SIZE:]"
      ],
      "metadata": {
        "id": "4dZAnusrE_Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to define the classifier. In this case, we will be deploying a [Naive Bayes Classifer ](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c), which will use the data from the training set (last letters of male and female names) to predict whether new names given to it are male or female."
      ],
      "metadata": {
        "id": "PqovDjBxF1x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define classifier as \"trained\" on training set\n",
        "classifier = NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "g9F4n7KWF15p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some notes: \n",
        "\n",
        "When working with large corpora, constructing a single list that contains the features of every instance can use up a large amount of memory.\n",
        "\n",
        "In these cases, use the function nltk.classify.apply_features, which returns an object that acts like a list but does not store all the feature sets in memory:\n",
        "\n",
        "from nltk.classify import apply_features\n",
        "\n",
        "train_names, test_names = labeled_names[:round(len(featuresets) * .8)], labeled_names[round(len(featuresets) * .8):]\n",
        "\n",
        "train_set = apply_features(gender_features, labeled_names[500:])\n",
        "\n",
        "test_set = apply_features(gender_features, labeled_names[:500])"
      ],
      "metadata": {
        "id": "JDrUlFlgG0l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the classifier has \"trained\" on the training data, we can learn what features are most influential to its classification decisions. In this case, the ratios indicate how likely a name with a given last letter would be classified as male or female."
      ],
      "metadata": {
        "id": "1MFrGwMsG91m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10) "
      ],
      "metadata": {
        "id": "sQ53T6h7HFYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, \"test\" the classifier on its ability to classify male and female names in the test set accurately."
      ],
      "metadata": {
        "id": "ghQwrOaoHTxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print labels of classifier (should be male and female)\n",
        "print(classifier.labels())\n",
        "\n",
        "#Print accuracy of classifier when deployed on test set\n",
        "print(round(accuracy(classifier, test_set), 2))"
      ],
      "metadata": {
        "id": "ZyJhYZ31Ha9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual names can also be given to the classifier to judge its accuracy."
      ],
      "metadata": {
        "id": "LHLE0W_bH-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test classifier on female name based on last letter of name\n",
        "print(classifier.classify(gender_features('Aphrodite')))\n",
        "\n",
        "#Test classifier on male name based on last letter of name\n",
        "print(classifier.classify(gender_features('Zeus')))"
      ],
      "metadata": {
        "id": "tnHRcFm4IGdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier accuracy may increase if the classifier was built to consider more features. Let's define a classifier which lowercases the first and last letter of each name and identifies which letters are contained in the name and at what frequency."
      ],
      "metadata": {
        "id": "nyih2GPnIT-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns new classifier\n",
        "def gender_features2(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "#Returns new features for John (long list!)\n",
        "#gender_features2('John')\n",
        "\n",
        "#Retrieve features for list of gendered names and put in new list\n",
        "featuresets2 = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
        "\n",
        "#Print first item in list (long list!)\n",
        "#featuresets2[0]"
      ],
      "metadata": {
        "id": "YuZfvfIuIp0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the new classifier can be trained on the same set of male and female names of above and its accuracy can be obtained. Having too many specific features on a small dataset may lead to overfitting, but it seems the classifier was good at avoiding that since its performance is slightly better."
      ],
      "metadata": {
        "id": "mDG6TUoLJGs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set2, test_set2 = featuresets2[:TRAIN_SET_SIZE], featuresets2[TRAIN_SET_SIZE:]\n",
        "classifier2 = NaiveBayesClassifier.train(train_set2)\n",
        "round(accuracy(classifier2, test_set2), 2)"
      ],
      "metadata": {
        "id": "sQRN70ZWJMRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the most influential features on this classifier's decisions. Indeed, it seems the classifier is still mainly using the last letter, along with some other features that happen to improve the accuracy."
      ],
      "metadata": {
        "id": "Wx4MDPXrJPiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier2.show_most_informative_features(15)"
      ],
      "metadata": {
        "id": "-0GdVwGrJXVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compare the performance of the two classiifers. \n",
        "\n",
        "Before we start, here's a useful function for comparing strings:\n",
        "\n",
        "Edit distance is the number of characters that need to be substituted, inserted, or deleted, to transform s1 into s2."
      ],
      "metadata": {
        "id": "XT_asft2Jq3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edit_distance(\"John\", \"Joan\")"
      ],
      "metadata": {
        "id": "OkJ375JZJwX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK metrics module provides functions for calculating metrics beyond mere accuracy to compare the two classifiers. But in order to do so, we need to build 2 sets for each classification label: a reference set of correct values, and a test set of observed values."
      ],
      "metadata": {
        "id": "LVCQGPStKD5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier 1\n",
        "refsets = collections.defaultdict(set) # For what this is: https://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work\n",
        "testsets = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set):\n",
        "    refsets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    testsets[observed].add(i)\n",
        "    \n",
        "# Classifier 2\n",
        "refsets2 = collections.defaultdict(set)\n",
        "testsets2 = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_set2):\n",
        "    refsets2[label].add(i)\n",
        "    observed = classifier2.classify(feats)\n",
        "    testsets2[observed].add(i)"
      ],
      "metadata": {
        "id": "BfkxT4ZtKG8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can proceed to print the metrics for each classifier (precision, recall, f_measure). \n",
        "\n",
        "Note that we cannot get the accuracy in this manner. nltk.metrics.scores.accuracy(reference, test) works by comparing test[i] == reference[i] and our reference and test are not formatted in a way that allows for this.\n",
        "\n",
        "It's the same for the confusion matrix."
      ],
      "metadata": {
        "id": "5mQnR_YjKey-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = (\n",
        "    round(precision(refsets['female'], testsets['female']), 2),\n",
        "    round(precision(refsets['male'], testsets['male']), 2),\n",
        "    round(recall(refsets['female'], testsets['female']), 2),\n",
        "    round(recall(refsets['male'], testsets['male']), 2),\n",
        "    round(f_measure(refsets['female'], testsets['female']), 2),\n",
        "    round(f_measure(refsets['male'], testsets['male']), 2)\n",
        ")\n",
        "\n",
        "args2 = (\n",
        "    round(precision(refsets2['female'], testsets2['female']), 2),\n",
        "    round(precision(refsets2['male'], testsets2['male']), 2),\n",
        "    round(recall(refsets2['female'], testsets2['female']), 2),\n",
        "    round(recall(refsets2['male'], testsets2['male']), 2),\n",
        "    round(f_measure(refsets2['female'], testsets2['female']), 2),\n",
        "    round(f_measure(refsets2['male'], testsets2['male']), 2)\n",
        ")\n",
        "\n",
        "print('''\n",
        "CLASSIFIER 1\n",
        "------------ \n",
        "Female precision: {0}\n",
        "Male precision: {1}\n",
        "Female recall: {2}\n",
        "Male recall: {3}\n",
        "Female F1 score: {4}\n",
        "Male F1 score: {5}\n",
        "\n",
        "CLASSIFIER 2\n",
        "------------ \n",
        "Female precision: {6}\n",
        "Male precision: {7}\n",
        "Female recall: {8}\n",
        "Male recall: {9}\n",
        "Female F1 score: {10}\n",
        "Male F1 score: {11}\n",
        "'''.format(*args, *args2))"
      ],
      "metadata": {
        "id": "3EoswdERKvc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building classifiers, also conduct error analysis--investigating misclassified data. In this case, make a list for errors and load in classifications where the classifier's guess does not equal the gender tag."
      ],
      "metadata": {
        "id": "VQzyMYxgjuF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []\n",
        "for (name, tag) in test_names:\n",
        "    guess = classifier2.classify(gender_features(name))\n",
        "    if guess != tag:\n",
        "        errors.append((tag, guess, name))\n",
        "\n",
        "#Print first five misclassified names and tags\n",
        "print(errors[:5])\n",
        "\n",
        "#Print three columns (correct gender of name, guessed gender, and name itself)--LONG LIST!\n",
        "#for (tag, guess, name) in sorted(errors):\n",
        "  #print('Correct = {:8} guess = {:8} name = {}'.format(tag, guess, name)) # :8 creates spaces between columns."
      ],
      "metadata": {
        "id": "jdf5EUNxj9Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking through this list of errors, it seems that some suffixes that are more than one letter long can be indicative of name genders. For example, names ending in \"yn\" appear to be predominantly female, despite the fact that names ending in \"n\" tend to be male; also, names ending in \"ch\" are usually male, even though names that end in \"h\" tend to be female."
      ],
      "metadata": {
        "id": "WRXGt7Tylfke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this information to build a new classifier with even more features. Here we'll define a classifier which counts first letter and last two letters of word."
      ],
      "metadata": {
        "id": "fYjYvp1smsP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define new set of features (first letter and last two letters of each word)\n",
        "def gender_features3(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"suffix1\"] = name[-1].lower()\n",
        "    features[\"suffix2\"] = name[-2:].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "#gender_features3('John')\n",
        "\n",
        "#Get features above for list of gendered names and put in list\n",
        "featuresets3 = [(gender_features3(n), gender) for (n, gender) in labeled_names]\n",
        "\n",
        "#print first item in list (Long List!)\n",
        "#featuresets3[0]"
      ],
      "metadata": {
        "id": "5UXExdFym1as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the classifier on same set of male and female names above and find its accuracy and most informative features."
      ],
      "metadata": {
        "id": "JOFeGgZonK8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train new classifer\n",
        "train_set3, test_set3 = featuresets3[:TRAIN_SET_SIZE], featuresets3[TRAIN_SET_SIZE:]\n",
        "classifier3 = NaiveBayesClassifier.train(train_set3)\n",
        "\n",
        "#Get accuracy of classifier\n",
        "print(round(accuracy(classifier3, test_set3), 2))\n",
        "\n",
        "#Print 15 most informative features for classification decisions\n",
        "classifier3.show_most_informative_features(15)"
      ],
      "metadata": {
        "id": "4lSuqkAxnRbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen, this classifer's accuracy is improved, and the 2-letter suffixes are most indicative of the classification decisions. "
      ],
      "metadata": {
        "id": "7YY9fpm7nQwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Name Gender Identification with Maximum Entropy Classifier\n",
        "\n",
        "Another type of classifier that can be deployed is a [maximum entropy classifer.](https://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/#:~:text=What%20is%20the%20Max%20Entropy,conditionally%20independent%20of%20each%20other.)\n",
        "\n",
        "Unlike the Naive Bayes classifier, the Maximum Entropy classifier does not assume that the features (in this case, first letters and suffixes) are conditionally independent from one another. It is based on the Principle of Maximum Entropy and selects, from all models that fit the training data, the one that has the has the largest entropy (is most unpredictable). \n",
        "\n",
        "The principle of maximum entropy is invoked when we have some piece(s) of information about a probability distribution, but not enough to characterize it completely—likely because we do not have the means or resources to do so. As an example, if all we know about a distribution is its average, we can imagine infinite shapes that yield a particular average. The principle of maximum entropy says that we should humbly choose the distribution that maximizes the amount of unpredictability contained in the distribution.\n",
        "\n",
        "Taking the idea to the extreme, it wouldn’t be scientific to choose a distribution that simply yields the average value 100% of the time.\n",
        "\n",
        "Due to the minimum assumptions that the Maximum Entropy classifier makes, it is usually used when we don’t know anything about the prior distributions and when it is unsafe to make any assumptions. As noted above, the maximum entropy classifier is used when we can’t assume the conditional independence of the features."
      ],
      "metadata": {
        "id": "0q9zyEPinP23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a maximum entropy classifier and have it iterate through models that fit the training data. The default value of maximum iterations is 100; in this example, the performance in terms of accuracy on the test set stops significantly improving beyond the previous model's at around 25 iterations."
      ],
      "metadata": {
        "id": "bS2VMAzBpYEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_iter has default value 100. \n",
        "me_classifier = MaxentClassifier.train(train_set3, max_iter=25) "
      ],
      "metadata": {
        "id": "0z6AEw1-pag1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then get the accuracy of the maximum entropy classifier on the TEST set and the most informative features."
      ],
      "metadata": {
        "id": "XexG6xGUp2i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy of ME classifier\n",
        "print(round(accuracy(me_classifier, test_set3), 2))\n",
        "\n",
        "#Most informative features\n",
        "me_classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "id": "wgrrFk5_p2Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes and Maximum Entropy are just two of the classification methods enabled by NLTK. \n",
        "\n",
        "Scikit-learn (sklearn) is a popular library which features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN.\n",
        "\n",
        "NLTK provides an API to quickly use sklearn classifiers in nltk.classify.scikitlearn. The other option is to import and use sklearn directly.\n",
        "\n",
        "For an example of integrating sklearn with NLTK, you can check out [this notebook on Kaggle.](https://www.kaggle.com/alvations/basic-nlp-with-nltk) Kaggle is a great website for NLP and machine learning in general, creating an account is highly recommended."
      ],
      "metadata": {
        "id": "lKUjgYn8qkfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r8CCj_R4aMAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Genre Categorization with Naive Bayes Classifier\n",
        "\n",
        "Based on \"Another Excercise: Classifying News Documents in Categories: sport, humor, adventure, science fiction, etc...\" in [Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ],
      "metadata": {
        "id": "4LIHinCqrB4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim here is to build a classifier to sort texts in the Brown corpus into genre categories based on the most frequent words used. [Learn more about the Brown corpus here. ](https://www.nltk.org/book/ch02.html)\n",
        "\n",
        "First, prepare the Brown corpus for classification analysis."
      ],
      "metadata": {
        "id": "QThPBK6arWJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean spacing\n",
        "brown.readme().replace('\\n', ' ')\n",
        "\n",
        "#Get file ids\n",
        "#brown.fileids()\n",
        "\n",
        "#Get categories (genres of text) in Brown corpus\n",
        "brown.categories()"
      ],
      "metadata": {
        "id": "i7hL2MzNr91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, compile a list of the most popular words in the corpus. Then sort the words into a mutable list and sort them based on frequency"
      ],
      "metadata": {
        "id": "YGR8NOWksQug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the word is alphabetical avoids including stuff like `` and '' which are actually pretty common. \n",
        "# Note that it also omits words such as 1 (very common), aug., 1913, $30, 13th, over-all etc. Another option would have been .isalnum().\n",
        "words_in_corpora = FreqDist(w.lower() for w in brown.words() if w.isalpha()) \n",
        "\n",
        "#Put most frequent words in mutable list\n",
        "words_in_corpora_freq_sorted = list(map(list, words_in_corpora.items()))\n",
        "#words_in_corpora_freq_sorted\n",
        "\n",
        "#Sort words in list based on frequency\n",
        "words_in_corpora_freq_sorted.sort(key=lambda x: x[1], reverse=True) # Using a lambda function is an alternative to using the operator library.\n",
        "\n",
        "#Get 10 most frequent words\n",
        "words_in_corpora_freq_sorted[:10]"
      ],
      "metadata": {
        "id": "AEFrpnwCsV_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, put the 1500 most frequent words in list into a new variable and delete word count (list item 1)"
      ],
      "metadata": {
        "id": "wfhiqg1YtGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new variable for top 1500 words\n",
        "best1500 = words_in_corpora_freq_sorted[:1500]\n",
        "\n",
        "#Remove frequency counts\n",
        "for list_item in best1500:\n",
        "    del list_item[1]\n",
        "\n",
        "#Print top 10 words in new variable\n",
        "best1500[:10]"
      ],
      "metadata": {
        "id": "GbTuO8NYtG1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since best1500 is now a list of words, it should be [flattened](https://stackabuse.com/python-how-to-flatten-list-of-lists/). Break down the list into its individual sublists and then use the chain() function. Chain further breaks down each sublist into its individual components, so this approach can be used to flatten any list of lists."
      ],
      "metadata": {
        "id": "m_F4auFRtbwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = itertools.chain(*best1500) \n",
        "best1500 = list(chain) # chain is of type itertools.chain so we need the cast\n",
        "best1500[:10]"
      ],
      "metadata": {
        "id": "MeTyGdr3tlmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of the top 1500 words were stopwords; these will now be removed from the list."
      ],
      "metadata": {
        "id": "_q-WtxQNuLO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopw = stopwords.words('english')\n",
        "\n",
        "def nonstop(listwords):\n",
        "    return [word for word in listwords if word not in stopw]\n",
        "\n",
        "best1500_words_corpora = nonstop(best1500) # Note how this will probably contain less than 1500 words.\n",
        "best1500_words_corpora[:10]"
      ],
      "metadata": {
        "id": "1NOyMavCuRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the documents in the corpus must be converted to forms suitable for classification. Each file in the corpus will eventually be represented by a dictionary showing the presence of the corpus’ most popular words in the particular file."
      ],
      "metadata": {
        "id": "CDtPuFDkufqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = [(nonstop(brown.words(fileid)), category) for category in brown.categories() for fileid in brown.fileids(category)]\n",
        "# documents # Note how documents is a list of tuples.\n",
        "\n",
        "# The code above generates a representation of the corpus but without removing punctuation. This is better:\n",
        "documents = [([item.lower() for item in nonstop(brown.words(fileid)) if item.isalpha()], category)\n",
        "             for category in brown.categories()\n",
        "             for fileid in brown.fileids(category)]\n",
        "#documents # Long List! Note how documents is a list of tuples\n",
        "\n",
        "#Shuffle items in documents\n",
        "shuffle(documents)"
      ],
      "metadata": {
        "id": "opbBEfaCupAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a list of features to be used to train the classifier - in this case, the presence of the 1500 most frequent words in the corpus"
      ],
      "metadata": {
        "id": "j2WCV1CsveTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(doc):\n",
        "    doc_set_words = set(doc) # Checking whether a word occurs in a set is much faster than checking whether it occurs in a list.\n",
        "    features_dic = {} # Features is a dictionary\n",
        "    for word in best1500_words_corpora:\n",
        "        features_dic['has(%s)' % word] = (word in doc_set_words)\n",
        "    return features_dic\n",
        "\n",
        "doc_features_set = [(document_features(d),c) for (d,c) in documents]\n",
        "#doc_features_set[0]"
      ],
      "metadata": {
        "id": "2PcA-obrvAK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now build the classifer to determine what category documents fall into based on the most frequent words."
      ],
      "metadata": {
        "id": "im9QWxKqv1Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort documents into training and test sets \n",
        "train_set = doc_features_set[:350] # Since the total is 500\n",
        "test_set  = doc_features_set[150:]\n",
        "\n",
        "#Run classifier on training set\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "#Get accuracy of classifier\n",
        "print(accuracy(classifier, test_set))\n",
        "\n",
        "#Show most informative features\n",
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "id": "qRuQuqXAv28c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classification of documet 'ca01' (it is under the 'news' category)"
      ],
      "metadata": {
        "id": "fKlo_yt7wVTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(brown.words('ca01')))"
      ],
      "metadata": {
        "id": "2GUudPvdwVg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New texts can also be prepared for classificiation. "
      ],
      "metadata": {
        "id": "RmmxGh7XL33Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The test text needs to be long enough in order to contain a significant amount of the 1500 most common words in our training corpus.\n",
        "text = \"1 God, infinitely perfect and blessed in himself, in a plan of sheer goodness freely created man to make him share in his own blessed life. For this reason, at every time and in every place, God draws close to man. He calls man to seek him, to know him, to love him with all his strength. He calls together all men, scattered and divided by sin, into the unity of his family, the Church. To accomplish this, when the fullness of time had come, God sent his Son as Redeemer and Saviour. In his Son and through him, he invites men to become, in the Holy Spirit, his adopted children and thus heirs of his blessed life. 2 So that this call should resound throughout the world, Christ sent forth the apostles he had chosen, commissioning them to proclaim the gospel: \\\"Go therefore and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, teaching them to observe all that I have commanded you; and lo, I am with you always, to the close of the age.\\\"4 Strengthened by this mission, the apostles \\\"went forth and preached everywhere, while the Lord worked with them and confirmed the message by the signs that attended it.\\\" 3 Those who with God's help have welcomed Christ's call and freely responded to it are urged on by love of Christ to proclaim the Good News everywhere in the world. This treasure, received from the apostles, has been faithfully guarded by their successors. All Christ's faithful are called to hand it on from generation to generation, by professing the faith, by living it in fraternal sharing, and by celebrating it in liturgy and prayer. 4 Quite early on, the name catechesis was given to the totality of the Church's efforts to make disciples, to help men believe that Jesus is the Son of God so that believing they might have life in his name, and to educate and instruct them in this life, thus building up the body of Christ. Catechesis is an education in the faith of children, young people and adults which includes especially the teaching of Christian doctrine imparted, generally speaking, in an organic and systematic way, with a view to initiating the hearers into the fullness of Christian life. While not being formally identified with them, catechesis is built on a certain number of elements of the Church's pastoral mission which have a catechetical aspect, that prepare for catechesis, or spring from it. They are: the initial proclamation of the Gospel or missionary preaching to arouse faith; examination of the reasons for belief; experience of Christian living; celebration of the sacraments; integration into the ecclesial community; and apostolic and missionary witness. Catechesis is intimately bound up with the whole of the Church's life. Not only her geographical extension and numerical increase, but even more her inner growth and correspondence with God's plan depend essentially on catechesis. Periods of renewal in the Church are also intense moments of catechesis. In the great era of the Fathers of the Church, saintly bishops devoted an important part of their ministry to catechesis. St. Cyril of Jerusalem and St. John Chrysostom, St. Ambrose and St. Augustine, and many other Fathers wrote catechetical works that remain models for us. The ministry of catechesis draws ever fresh energy from the councils. the Council of Trent is a noteworthy example of this. It gave catechesis priority in its constitutions and decrees. It lies at the origin of the Roman Catechism, which is also known by the name of that council and which is a work of the first rank as a summary of Christian teaching. The Council of Trent initiated a remarkable organization of the Church's catechesis. Thanks to the work of holy bishops and theologians such as St. Peter Canisius, St. Charles Borromeo, St. Turibius of Mongrovejo or St. Robert Bellarmine, it occasioned the publication of numerous catechisms. It is therefore no surprise that catechesis in the Church has again attracted attention in the wake of the Second Vatican Council, which Pope Paul Vl considered the great catechism of modern times. the General Catechetical Directory (1971) the sessions of the Synod of Bishops devoted to evangelization (1974) and catechesis (1977), the apostolic exhortations Evangelii nuntiandi (1975) and Catechesi tradendae (1979), attest to this. the Extraordinary Synod of Bishops in 1985 asked that a catechism or compendium of all Catholic doctrine regarding both faith and morals be composed. The Holy Father, Pope John Paul II, made the Synod's wish his own, acknowledging that this desire wholly corresponds to a real need of the universal Church and of the particular Churches. He set in motion everything needed to carry out the Synod Fathers' wish.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') # Picks out sequences of alphanumeric characters as tokens and drops everything else\n",
        "text_tokens = nonstop(tokenizer.tokenize(text.lower()))\n",
        "text_tokens = [w for w in text_tokens if w.isalpha()]\n",
        "#text_tokens\n",
        "\n",
        "#Determine whether list of tokens contain most frequent words set above\n",
        "text_features = document_features(text_tokens)\n",
        "#text_features"
      ],
      "metadata": {
        "id": "FlmQ8-5ZL4Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now classify the new document based on presence of frequent words in brown corpus categories"
      ],
      "metadata": {
        "id": "sokA0DK3MWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(text_tokens))"
      ],
      "metadata": {
        "id": "b-rSNXKyMWfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis with Naive Bayes Classifier\n",
        "\n",
        "Sentiment analysis is the practice of using algorithms to classify various samples of related text into overall positive and negative categories. With NLTK, you can employ these algorithms through powerful built-in machine learning operations to obtain insights from linguistic data.\n",
        "\n",
        "Based on [Exercise B: Sentiment Analysis in Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ],
      "metadata": {
        "id": "8UGTFSAEMu4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the NLTK [movie reviews corpus](https://www.nltk.org/_modules/nltk/corpus/reader/reviews.html) to build the classifier. Files in this corpus are designated as positive and negative. "
      ],
      "metadata": {
        "id": "yEDBGctWNa8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean spacing from NLTK movie reviews corpus\n",
        "movie_reviews.readme().replace('\\n', ' ').replace('\\t', '').replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")\n",
        "\n",
        "#Printing the file ids from movie_reviews generates a very long list. \n",
        "#But you can see the structure of the ids and how the label includes \"pos\" or \"neg\"\n",
        "#movie_reviews.fileids()\n",
        "\n",
        "#To determine how many movie reviews are in the corpus, print the length of the list of file ids\n",
        "len(movie_reviews.fileids())\n",
        "\n",
        "#Here's an additional cleaning trick to get rid of ' in text - but only if there were no \" used. See how it works with just one file.\n",
        "movie_reviews.raw(\"neg/cv000_29416.txt\").replace(\"\\n\", \"\").replace(\"'\", '\"').replace('\"', \"'\")"
      ],
      "metadata": {
        "id": "lOe9JRwpNaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building the classifier, you'll want to generate a list of stopwords which will NOT be considered when making lists of positive and negative words. We'll import English stopwords from NLTK and put them in \"stops,\" then add additional features we don't want to include in classification using stops.extend. To see check full list of stopwords, print stops."
      ],
      "metadata": {
        "id": "ooDlrxyuQpaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stops = stopwords.words('english')\n",
        "stops.extend('.,[,],(,),;,/,-,\\',?,\",:,<,>,n\\'t,|,#,\\'s,\\\",\\'re,\\'ve,\\'ll,\\'d,\\'re'.split(','))\n",
        "stops.extend(',')\n",
        "#stops"
      ],
      "metadata": {
        "id": "p6X4hEP_Qprd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to store the features to be used in the classifer. \n"
      ],
      "metadata": {
        "id": "x6Ny2qfdPr69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_feats(words):\n",
        "    return dict([(word, True) for word in words if word not in stops and word.isalpha()])"
      ],
      "metadata": {
        "id": "TheBds0FM4mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new variables for all positive and all negative movie reviews and get combined length (should be same as  length of original file ids list)."
      ],
      "metadata": {
        "id": "mVvt7gevP6ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = movie_reviews.fileids('pos')\n",
        "neg_ids = movie_reviews.fileids('neg')\n",
        "\n",
        "len(pos_ids) + len(neg_ids) "
      ],
      "metadata": {
        "id": "svFtY0ZQP6gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take the positive/negative words, create the feature for such words, and store it in a positive/negative features list. You can print pos_feats to check list of words has loaded correctly; it will print a VERY long list, since it will include words from every positive review."
      ],
      "metadata": {
        "id": "znBCHHoGQDmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]\n",
        "neg_feats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]\n",
        "\n",
        "#pos_feats"
      ],
      "metadata": {
        "id": "GxnmFhhJQDvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store 3/4 of the positive and negative features to train the classifer; combine them all and store the remaining 1/4 as test features."
      ],
      "metadata": {
        "id": "eRNG6HCxQ0Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store positive and negative features for training\n",
        "pos_len_train = int(len(pos_feats) * 3 / 4)\n",
        "neg_len_train = int(len(neg_feats) * 3 / 4)\n",
        "\n",
        "#Check length of positive features for training\n",
        "print(pos_len_train)\n",
        "\n",
        "#Combine positive and negative training features into one set and put the rest in \"test features\"\n",
        "train_feats = neg_feats[:neg_len_train] + pos_feats[:pos_len_train]\n",
        "test_feats = neg_feats[neg_len_train:] + pos_feats[pos_len_train:]"
      ],
      "metadata": {
        "id": "N7ouCSA4Q0cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train a NaiveBayesClassifier with the positive and negative words, get the accuracy of the classifier and its most informative features"
      ],
      "metadata": {
        "id": "ncxY939RRgXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train classifier\n",
        "classifier = NaiveBayesClassifier.train(train_feats)\n",
        "\n",
        "#Get classifier accuracy\n",
        "print('Accuracy: ', nltk.classify.util.accuracy(classifier, test_feats))\n",
        "\n",
        "#Get most informative features\n",
        "classifier.show_most_informative_features()"
      ],
      "metadata": {
        "id": "B0StQzK5RgqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, add a new sentence to test our classifier. The new sentence must be tokenized and cleaned of stopwords. From here the sentence can be converted to features to be classified."
      ],
      "metadata": {
        "id": "BqU3R6o6R_4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and clean new sentence\n",
        "sentence = \"I feel so miserable, it makes me amazing\"\n",
        "tokens = [word for word in word_tokenize(sentence) if word not in stops]\n",
        "print(tokens)\n",
        "\n",
        "#Make tokens into features using word_feats function defined above\n",
        "feats = word_feats(word for word in tokens)\n",
        "print(feats)"
      ],
      "metadata": {
        "id": "PRTmcfhFSAE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use classifier to classify new sentence as either positive or negative. The result may not be what you expect!"
      ],
      "metadata": {
        "id": "SJTE_CjoSnuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(feats)"
      ],
      "metadata": {
        "id": "jR7soz9ESopz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try classifying another sentence - go through the same tokenizing process and load tokens into new variable. This time, instead of retaining all tokens as features, just capture the adjectives using `if pos[] == JJ`"
      ],
      "metadata": {
        "id": "ESfbYqEmSssq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and clean new sentence\n",
        "sentence2 = \"You are a pathetic fool, a terrible excuse for a human being.\"\n",
        "tokens2 = [word for word in word_tokenize(sentence2) if word not in stops]\n",
        "tokens2\n",
        "\n",
        "#Capture tokens identifiied as adjectives\n",
        "pos_tags2 = [pos for pos in pos_tag(tokens2) if pos[1] == 'JJ']\n",
        "pos_tags2\n",
        "\n",
        "#Put reduced list of tokens into variable for classification\n",
        "feats2 = word_feats([word for (word,_) in pos_tags2])\n",
        "feats2"
      ],
      "metadata": {
        "id": "6vLMzPGOSs2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use classifier to classify new sentence as either positive or negative."
      ],
      "metadata": {
        "id": "Vbpe8VDYTLcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(feats2)"
      ],
      "metadata": {
        "id": "cw2a2ZxFTLoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible extension: In order to improve the classifier, bigram features can be examined using nltk.util.ngrams. This is because, for instance, 'not funny' is very different from 'funny'."
      ],
      "metadata": {
        "id": "k64q5JgPTn46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis with NLTK SentimentAnalyzer\n",
        "\n",
        "Sentiment analysis can also be conducted using NLTK's [SentimentAnalyzer](https://www.nltk.org/api/nltk.sentiment.sentiment_analyzer.html). \n",
        "\n",
        "To do so, draw features from the Subjectivity Dataset. The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences. Learn more about the subjectivity corpus [here](https://www.nltk.org/howto/corpus.html)."
      ],
      "metadata": {
        "id": "cJnQuR2xUghZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#From the NLTK subjectivity corpus, get the file ids\n",
        "subjectivity.fileids()\n",
        "\n",
        "#Get tokens in two of the files\n",
        "print(subjectivity.sents('plot.tok.gt9.5000'))\n",
        "print(subjectivity.sents('quote.tok.gt9.5000'))"
      ],
      "metadata": {
        "id": "ac9hQ8gfVJqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the categories in the subjectivity sentences and get words categorized as \"objective\" and \"subjective.\""
      ],
      "metadata": {
        "id": "BIlCEnCXUqBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve the categories in subjectivity corpus (objective and subjective sentences).\n",
        "subjectivity.categories() # The mapping between documents and categories does not depend on the file structure.\n",
        "\n",
        "#Get tokens in subjectivity that are categorized as \"objective\"\n",
        "subjectivity.sents(categories='obj')\n",
        "\n",
        "#Get tokens in subjectivity that are categorized as \"subjective\"\n",
        "subjectivity.sents(categories='subj')"
      ],
      "metadata": {
        "id": "oFOTofTDWDni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get features for classification, create two new lists for objective and subjective documents. Set the number of instances at 100 and put sentences up to number of n_instances (100) in each list. \n",
        "\n",
        "Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings."
      ],
      "metadata": {
        "id": "WCqRMb1pWNc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_instances = 100\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
        "len(obj_docs), len(subj_docs)"
      ],
      "metadata": {
        "id": "0IaeJPGaWvFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sentence in obj_docs list to check:"
      ],
      "metadata": {
        "id": "2pPXdQv_W4K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_docs[0]"
      ],
      "metadata": {
        "id": "LIvLD2SSW4TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divde sentences into training and testing groups. The first 80 sentences of each are for training, last 20 for testing. Split evenly for objective and subjective docs, then combine into two larger groups (all training and all testing)."
      ],
      "metadata": {
        "id": "U-0Mii6-W7cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Designate 80 objective docs for training , 20 for testing\n",
        "train_obj_docs = obj_docs[:80]\n",
        "test_obj_docs = obj_docs[80:100]\n",
        "\n",
        "#Designate 80 subjective docs for training , 20 for testing\n",
        "train_subj_docs = subj_docs[:80]\n",
        "test_subj_docs = subj_docs[80:100]\n",
        "\n",
        "#Combine objective and subjective lists into larger groups for training and testing\n",
        "training_docs = train_obj_docs + train_subj_docs\n",
        "testing_docs = test_obj_docs + test_subj_docs"
      ],
      "metadata": {
        "id": "htpT_sqyW7k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the new sentiment analyzer and use it to append _NEG suffix to words that appear between a sensed negation and a punctuation mark."
      ],
      "metadata": {
        "id": "INdAc0MbXbcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
        "#all_words_neg"
      ],
      "metadata": {
        "id": "9E079KHOXbsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return the list of most common 1-word features in all_words_neg, with a minimum frequency of 4 appearances."
      ],
      "metadata": {
        "id": "1HHzBlvNXqWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
        "len(unigram_feats)"
      ],
      "metadata": {
        "id": "0NQOe74DXqey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redefine training and test set to include whether or not sents include the unigram_feats"
      ],
      "metadata": {
        "id": "0UI6Xd4BYF9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = sentim_analyzer.apply_features(training_docs)\n",
        "test_set = sentim_analyzer.apply_features(testing_docs)\n",
        "#training_set[0]"
      ],
      "metadata": {
        "id": "nhQyZTNiYGGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train our classifier on the training set, and subsequently output the evaluation results."
      ],
      "metadata": {
        "id": "HYJTqO4IYIrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)"
      ],
      "metadata": {
        "id": "1WguACLnYI34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of results from [Python NLTK Cookbook:](https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/)\n",
        "\n",
        "*  **Accuracy** measures the number of elements correctly identified in a data set.\n",
        "*  **F-measure** is the weighted harmonic mean of precision and recall. \n",
        "*  **Precision** measures the exactness of a classifier. A higher precision means less false positives, while a lower precision means more false positives.\n",
        "*   **Recall** measures the completeness, or sensitivity, of a classifier. Higher recall means less false negatives, while lower recall means more false negatives. Often improves inverse of precision.\n",
        "\n",
        "Get measures for classifier run above."
      ],
      "metadata": {
        "id": "MpAwK1KWYOnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "    print('{0}: {1}'.format(key, value))"
      ],
      "metadata": {
        "id": "ltWVBErrYOyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis with VADER\n",
        "\n",
        "Sentiment analysis can also be conducted using `SentimentIntensityAnalyzer from [VADER](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). This will assign an \"intensity score\" to each sentence based on its identified sentiment.\n",
        "\n",
        "First, add a list of sentences for analysis."
      ],
      "metadata": {
        "id": "Kz2vwtM4Ybco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"You are a jerk, and I will step on you.\",\n",
        "    \"THIS SUX!!!\",\n",
        "    \"This kinda sux...\",\n",
        "    \"You're good, man\",\n",
        "    \"HAHAHA YOU ARE THE BEST!!!!! VERY FUNNY!!!\"\n",
        "            ]"
      ],
      "metadata": {
        "id": "YnGADHISYbk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SentimentIntesnityAnalyzer (defined as sid) to get \"intensity\" of each sentence in list"
      ],
      "metadata": {
        "id": "WOA-TOKnYrqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "for sentence in sentences:\n",
        "    print('\\n' + sentence)\n",
        "    ss = sid.polarity_scores(sentence)\n",
        "    for k in sorted(ss):\n",
        "        print('{0}: {1}, '.format(k, ss[k]), end='')"
      ],
      "metadata": {
        "id": "hP31rNDcYrzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}