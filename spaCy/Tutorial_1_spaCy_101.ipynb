{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 1: spaCy 101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYBzXUuXbUhU8lv3cqOFA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/spaCy/Tutorial_1_spaCy_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 1: spaCy 101"
      ],
      "metadata": {
        "id": "JxGrpOnyQ3-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out features from spaCy 101 page: https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "8xbl7TfFQkCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linguistic Annotations\n",
        "spaCy provides a variety of linguistic annotations to give you insights into a text’s grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. "
      ],
      "metadata": {
        "id": "mhQG-5oQRAVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AszxkC1sQWpk",
        "outputId": "3a712ee8-f983-4648-8058-bd59a7855006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.K. PROPN compound\n",
            "startup NOUN dobj\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "1 NUM compound\n",
            "billion NUM pobj\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each Doc consists of individual tokens, and we can iterate over them:"
      ],
      "metadata": {
        "id": "NkBNT5h6RXnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo33OOVyRgLl",
        "outputId": "9f7125a3-50e6-441b-cd15-1d0d5be158c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about how spaCy’s tokenization rules work in detail, how to customize and replace the default tokenizer and how to add language-specific data, see the usage guides on [language data](https://spacy.io/usage/linguistic-features#language-data) and [customizing the tokenizer.](https://spacy.io/usage/linguistic-features#tokenization)"
      ],
      "metadata": {
        "id": "kfa1JVHJRwmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part of Speech Tags and Dependencies\n",
        "After tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context.\n",
        "\n",
        "Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name:"
      ],
      "metadata": {
        "id": "Vq3jEuzRSVvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zgLoufRSfL2",
        "outputId": "3eb0ab48-7e64-4b57-e4b6-0e2abe410a0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
            "is be AUX VBZ aux xx True True\n",
            "looking look VERB VBG ROOT xxxx True False\n",
            "at at ADP IN prep xx True True\n",
            "buying buy VERB VBG pcomp xxxx True False\n",
            "U.K. U.K. PROPN NNP compound X.X. False False\n",
            "startup startup NOUN NN dobj xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "$ $ SYM $ quantmod $ False False\n",
            "1 1 NUM CD compound d False False\n",
            "billion billion NUM CD pobj xxxx True False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its dependencies look like:"
      ],
      "metadata": {
        "id": "9mraINSvlb8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "metadata": {
        "id": "_P1IrOsJlbKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entity Recognition\n",
        "\n",
        "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction."
      ],
      "metadata": {
        "id": "xNXrxtNYlxng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em61gWAGl45Y",
        "outputId": "14248c53-d09a-436e-c466-385604ff54a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its named entities look like:\n"
      ],
      "metadata": {
        "id": "Q0IzR6MYl4NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "5d6w31vul__i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about entity recognition in spaCy, how to add your own entities to a document and how to train and update the entity predictions of a model, see the usage guides on [named entity recognition](https://spacy.io/usage/linguistic-features#named-entities) and [training pipelines.](https://spacy.io/usage/training)"
      ],
      "metadata": {
        "id": "d6mFmFbumOYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Vectors and Similarity\n",
        "\n",
        "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this:"
      ],
      "metadata": {
        "id": "9Q4o3AwGmfNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
        "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
        "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
        "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
        "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
        "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
        "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
        "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
        "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
        "       # ... and so on ...\n",
        "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
        "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
        "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)"
      ],
      "metadata": {
        "id": "RGazu9EpmvPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline packages that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors."
      ],
      "metadata": {
        "id": "OC1q3guumzs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokens = nlp(\"dog cat banana afskfsd\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpChQcZ7m1AP",
        "outputId": "5737be05-21d5-48a9-d6d0-10e3d901f2f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog True 19.266302 True\n",
            "cat True 19.220264 True\n",
            "banana True 17.748499 True\n",
            "afskfsd True 20.882006 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of 0, which means it’s practically nonexistent."
      ],
      "metadata": {
        "id": "610h4jV9m-lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one."
      ],
      "metadata": {
        "id": "BIwsQet7nJQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each Doc, Span, Token and Lexeme comes with a .similarity method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether two words, spans or documents are similar really depends on how you’re looking at it. spaCy’s similarity implementation usually assumes a pretty general-purpose definition of similarity."
      ],
      "metadata": {
        "id": "7XKcFtCRnHSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # make sure to use larger package!\n",
        "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
        "doc2 = nlp(\"Fast food tastes very good.\")\n",
        "\n",
        "# Similarity of two documents\n",
        "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
        "# Similarity of tokens and spans\n",
        "french_fries = doc1[2:4]\n",
        "burgers = doc1[5]\n",
        "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmXOIUeQnL_2",
        "outputId": "978a3829-12ad-4512-fb43-7ddc44a9b6ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.3679284418194866\n",
            "salty fries <-> hamburgers 0.43668434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vocab, hashes and lexemes\n",
        "\n",
        "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab, that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values – in this case for example, “coffee” has the hash 3197928453018144401. Entity labels like “ORG” and part-of-speech tags like “VERB” are also encoded. Internally, spaCy only “speaks” in hash values."
      ],
      "metadata": {
        "id": "Jw906wlunkCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ULpj5qtns-x",
        "outputId": "20d7c502-c382-49b2-9f7e-6e381640253a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197928453018144401\n",
            "coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all strings are encoded, the entries in the vocabulary don’t need to include the word text themselves. Instead, they can look it up in the StringStore via its hash value. Each entry in the vocabulary, also called Lexeme, contains the context-independent information about a word."
      ],
      "metadata": {
        "id": "fKSHh05Wnw9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGYRN0z5nxdP",
        "outputId": "716cc3bc-24e0-4541-9ee8-cf2a69ce9c20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 4690420944186131903 X I I True False True en\n",
            "love 3702023516439754181 xxxx l ove True False False en\n",
            "coffee 3197928453018144401 xxxx c fee True False False en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mapping of words to hashes doesn’t depend on any state. To make sure each value is unique, spaCy uses a hash function to calculate the hash based on the word string\n",
        "\n",
        "However, hashes cannot be reversed and there’s no way to resolve 3197928453018144401 back to “coffee”. All spaCy can do is look it up in the vocabulary. That’s why you always need to make sure all objects you create have access to the same vocabulary. "
      ],
      "metadata": {
        "id": "yzNQIjEmn5Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")  # Original Doc\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee' \n",
        "\n",
        "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
        "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
        "\n",
        "empty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\n",
        "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' \n",
        "\n",
        "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
        "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qykx2gCcoAAe",
        "outputId": "17685f7a-a936-40d5-9fa6-4a9affe00a79"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197928453018144401\n",
            "coffee\n",
            "coffee\n",
            "coffee\n"
          ]
        }
      ]
    }
  ]
}