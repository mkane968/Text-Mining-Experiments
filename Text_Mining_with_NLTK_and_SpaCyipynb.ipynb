{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Mining with NLTK and SpaCyipynb",
      "provenance": [],
      "collapsed_sections": [
        "ebscZ-AJi9Ks",
        "-IEWeBA6jFry",
        "tUJp55LvjpA_",
        "2OV5OEQjjuP3",
        "6l9c0snjj1qW",
        "W1tig0UtkDds",
        "dQkD3y2ckhU6",
        "PndxDRfGkwtC",
        "HGEjsuGolnbp",
        "mDqUxQrEl6Ae",
        "JLK-Rq_pl8f1",
        "4LIHinCqrB4Y"
      ],
      "authorship_tag": "ABX9TyOSIozsFMG+J424EU9qv9UI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/Text_Mining_with_NLTK_and_SpaCyipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Mining with NLTK and SpaCy\n",
        "\n",
        "This tutorial will provide an introduction to the basics of text mining using the Natural Language Toolkit and SpaCy. \n",
        "\n",
        "INTRO PARAGRAPH ABOUT NLTK\n",
        "\n",
        "INTRO PARAGRAPH ABOUT SPACY"
      ],
      "metadata": {
        "id": "RuRKzfxbhoj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLTK Pipeline\n",
        "\n",
        "Describe this pipeline"
      ],
      "metadata": {
        "id": "naftfDYNizq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Libraries, Packages and Corpora\n",
        "To prepare for text analysis, install and import necessary libraries, packages and corpora. You will need to run ! pip install for those not in Colab by default."
      ],
      "metadata": {
        "id": "ebscZ-AJi9Ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMWIGsUqhbut"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "import nltk.corpus\n",
        "from nltk.corpus import brown, stopwords, names, movie_reviews, subjectivity\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('names')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('subjectivity')\n",
        "import string \n",
        "from string import punctuation\n",
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer, bigrams, trigrams, FreqDist, ngrams, NaiveBayesClassifier, MaxentClassifier, pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "from nltk.text import Text\n",
        "import matplotlib\n",
        "import matplotlib.pyplot\n",
        "import numpy as np\n",
        "import random\n",
        "from nltk.classify import accuracy\n",
        "from nltk.metrics.scores import (precision, recall, f_measure)\n",
        "from nltk.metrics import edit_distance\n",
        "import collections\n",
        "import itertools\n",
        "from random import shuffle\n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import (mark_negation, extract_unigram_feats) \n",
        "from nltk.sentiment import SentimentAnalyzer # SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis.\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Tokenization\n",
        "NLTK analysis involves working with small units of text such as words and sentences. Tokenization is the process used to split up full text into parts."
      ],
      "metadata": {
        "id": "-IEWeBA6jFry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sentence Tokenization\n",
        "Convert a passage into sentence and assign it to variable \"tokens\": "
      ],
      "metadata": {
        "id": "JDpvr3eQjQqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Two plus two is four, minus one that's three â€” quick maths. Every day man's on the block. Smoke trees. See your girl in the park, that girl is an uckers. When the thing went quack quack quack, your men were ducking! Hold tight Asznee, my brother. He's got a pumpy. Hold tight my man, my guy. He's got a frisbee. I trap, trap, trap on the phone. Moving that cornflakes, rice crispies. Hold tight my girl Whitney.\"\n",
        "sent_tokens = sent_tokenize(my_string)\n",
        "sent_tokens"
      ],
      "metadata": {
        "id": "aArPrxZ0jSgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Word Tokenization\n",
        "You can also convert sentences (or whole passages) into word tokens. The first five word tokens of the passage are shown below."
      ],
      "metadata": {
        "id": "prDLGDTljVVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(my_string)\n",
        "word_tokens[:10]"
      ],
      "metadata": {
        "id": "duEqRVtQjW_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that punctuation and 's are recognized as tokens here. Depending on the goals of your analysis, you may want to filter out these tokens and others (see next section for more details). \n",
        "\n",
        "Tokenization can be done for a whole file or corpus of files. Many NLTK corpora have already been tokenized. Let's retrieve the tokenized words from the news documents in the [Brown corpus](https://www.nltk.org/book/ch02.html):"
      ],
      "metadata": {
        "id": "E4Sgu8PPjaGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text = brown.words(categories='news')\n",
        "news_text[:10]"
      ],
      "metadata": {
        "id": "fQ8oydrgjcqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also tokenize a file (or corpus) uploaded from your local machine. Use this code to upload a document file of your choice and convert it into  tokens. "
      ],
      "metadata": {
        "id": "DQGRSZTrjcQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import file importer\n",
        "from google.colab import files\n",
        "\n",
        "#Run file upload (Click \"Choose files\" to browse and select local file)\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "IrOWlCLyjgd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert document into string\n",
        "uploaded= str(uploaded)\n",
        "\n",
        "#Convert document into word tokens and print first ten tokens\n",
        "word_tokens = word_tokenize(uploaded)\n",
        "word_tokens[:10]"
      ],
      "metadata": {
        "id": "PW-IAoeVjjG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Get N-Grams\n",
        "\n",
        "NLTK can detect n-grams, or strings of n items in a text. The most popular n-grams examined are bigrams and trigrams; larger patterns are less common across texts and thus less useful in analysis. Let's retrieve bigrams and trigrams in the Brown corpus text (cleaned for punctuation; stopwords included). "
      ],
      "metadata": {
        "id": "nNRcC2ZvfEeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text_cleaned_punct = [w for w in news_text_cleaned if w not in string.punctuation and w not in punct_combo]\n",
        "\n",
        "bi_news_text = list(bigrams(news_text_cleaned_punct))\n",
        "print(bi_news_text)\n",
        "tri_news_text = list(trigrams(news_text_cleaned_punct))\n",
        "print(tri_news_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHbFpm5TgKWJ",
        "outputId": "0f76e72f-86e0-49a1-e4ba-4ed2ea85b885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'investigation'), ('investigation', \"atlanta's\"), (\"atlanta's\", 'recent'), ('recent', 'primary'), ('primary', 'election'), ('election', 'produced'), ('produced', 'evidence'), ('evidence', 'irregularities'), ('irregularities', 'took'), ('took', 'place'), ('place', 'jury'), ('jury', 'said'), ('said', 'term-end'), ('term-end', 'presentments'), ('presentments', 'city'), ('city', 'executive'), ('executive', 'committee'), ('committee', 'over-all'), ('over-all', 'charge'), ('charge', 'election'), ('election', 'deserves'), ('deserves', 'praise'), ('praise', 'thanks'), ('thanks', 'city'), ('city', 'atlanta'), ('atlanta', 'manner'), ('manner', 'election'), ('election', 'conducted'), ('conducted', 'september-october'), ('september-october', 'term'), ('term', 'jury'), ('jury', 'charged'), ('charged', 'fulton'), ('fulton', 'superior'), ('superior', 'court'), ('court', 'judge'), ('judge', 'durwood'), ('durwood', 'pye'), ('pye', 'investigate'), ('investigate', 'reports'), ('reports', 'possible'), ('possible', 'irregularities'), ('irregularities', 'hard-fought'), ('hard-fought', 'primary'), ('primary', 'mayor-nominate'), ('mayor-nominate', 'ivan')]\n",
            "[('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'investigation'), ('friday', 'investigation', \"atlanta's\"), ('investigation', \"atlanta's\", 'recent'), (\"atlanta's\", 'recent', 'primary'), ('recent', 'primary', 'election'), ('primary', 'election', 'produced'), ('election', 'produced', 'evidence'), ('produced', 'evidence', 'irregularities'), ('evidence', 'irregularities', 'took'), ('irregularities', 'took', 'place'), ('took', 'place', 'jury'), ('place', 'jury', 'said'), ('jury', 'said', 'term-end'), ('said', 'term-end', 'presentments'), ('term-end', 'presentments', 'city'), ('presentments', 'city', 'executive'), ('city', 'executive', 'committee'), ('executive', 'committee', 'over-all'), ('committee', 'over-all', 'charge'), ('over-all', 'charge', 'election'), ('charge', 'election', 'deserves'), ('election', 'deserves', 'praise'), ('deserves', 'praise', 'thanks'), ('praise', 'thanks', 'city'), ('thanks', 'city', 'atlanta'), ('city', 'atlanta', 'manner'), ('atlanta', 'manner', 'election'), ('manner', 'election', 'conducted'), ('election', 'conducted', 'september-october'), ('conducted', 'september-october', 'term'), ('september-october', 'term', 'jury'), ('term', 'jury', 'charged'), ('jury', 'charged', 'fulton'), ('charged', 'fulton', 'superior'), ('fulton', 'superior', 'court'), ('superior', 'court', 'judge'), ('court', 'judge', 'durwood'), ('judge', 'durwood', 'pye'), ('durwood', 'pye', 'investigate'), ('pye', 'investigate', 'reports'), ('investigate', 'reports', 'possible'), ('reports', 'possible', 'irregularities'), ('possible', 'irregularities', 'hard-fought'), ('irregularities', 'hard-fought', 'primary'), ('hard-fought', 'primary', 'mayor-nominate'), ('primary', 'mayor-nominate', 'ivan')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both tokens and n-grams can be examined in basic text analysis. "
      ],
      "metadata": {
        "id": "EAlMK55jgKpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will be working with the tokenized version of the Brown corpus. For the most part, word tokens will be of more use than sentence tokens. \n",
        "\n",
        "TALK ABOUT WHEN SENTENCE TOKENS CAN BE ANALYZED"
      ],
      "metadata": {
        "id": "_kKQnijJjlSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Cleaning and Enrichment\n",
        "\n",
        "NLTK features several tools for cleaning (lowercasing, removing stopwords and punctuation, stemming and lemmatization) and enriching text (part of speech tagging, grammatical chunking, named entity recognition, n-gram generation). "
      ],
      "metadata": {
        "id": "tUJp55LvjpA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Remove Stopwords\n",
        "\n",
        "First, you may clean your text of words that will not be useful to count in analysis. \n",
        "**Stopwords**, very common words which do not add much meaning to the text, are often removed. \n",
        "\n",
        "The code below remove all stopwords from \"my_string.\" The new variable, \"my_string_cleaned,\" includes only words which do NOT appear in the NLTK stopword dictionary (stopwords.words). Words in the new string have also been lowercased."
      ],
      "metadata": {
        "id": "2OV5OEQjjuP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Two plus two is four, minus one that's three â€” quick maths. Every day man's on the block. Smoke trees. See your girl in the park, that girl is an uckers. When the thing went quack quack quack, your men were ducking! Hold tight Asznee, my brother. He's got a pumpy. Hold tight my man, my guy. He's got a frisbee. I trap, trap, trap on the phone. Moving that cornflakes, rice crispies. Hold tight my girl Whitney.\"\n",
        "word_tokens = word_tokenize(my_string)\n",
        "\n",
        "\n",
        "my_string_cleaned = [w.lower() for w in word_tokens if w.lower() not in stopwords.words('english')]\n",
        "my_string_cleaned[:10]"
      ],
      "metadata": {
        "id": "jPxjZMrzjkwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same can be done for a file or collection of files; let's clean the Brown corpus. We'll also lowercase all words in the corpus, since some basic text analysis functions are case-sensitive. "
      ],
      "metadata": {
        "id": "mA3x1IDdjyCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Brown corpus of stopwords and assign to new variable\n",
        "news_text_nostop = [w.lower() for w in news_text if w.lower() not in stopwords.words('english')]\n",
        "news_text_nostop[:10]"
      ],
      "metadata": {
        "id": "ZJLTGsgxjzMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Remove Punctuation\n",
        "\n",
        "Removing punctuation is another common cleaning step. Here all tokens are kept which are NOT in two punctuation lists (string.punctuation and punct_combo)"
      ],
      "metadata": {
        "id": "6l9c0snjj1qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punct_combo = [c + \"\\\"\" for c in string.punctuation ] + [\"\\\"\" + c for c in string.punctuation] + [\"â€”\",\".-\", \":-\", \"..\", \"...\",\"'s\", \"``\", \"''\"]\n",
        "my_string_cleaned= [w for w in my_string_cleaned if w not in string.punctuation and w not in punct_combo]\n",
        "my_string_cleaned[:10]"
      ],
      "metadata": {
        "id": "MqjVIP6Yj9gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do the same for the Brown corpus. We can do this using the original text,and since cleaning methods build on each other, we'll also remove the punctuation from the text we have just cleaned of stopwords. "
      ],
      "metadata": {
        "id": "y96zSjjXj-La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean original Brown text\n",
        "news_text_nopunct= [w for w in news_text if w not in string.punctuation and w not in punct_combo]\n",
        "news_text_nopunct[:10]"
      ],
      "metadata": {
        "id": "DfLespxSj_pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean Brown corpus without stopwords\n",
        "news_text_nostop_nopunct= [w for w in news_text_nostop if w not in string.punctuation and w not in punct_combo]\n",
        "news_text_nostop_nopunct[:10]"
      ],
      "metadata": {
        "id": "Sxvts--gkB93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stemming and Lemmatization\n",
        "\n",
        "Depending on your analysis goals, you may also want to reduce words to their roots (stemming) or core meanings (lemmatization).\n",
        "\n",
        "First, let's test out three stemming tools: the Porter Stemmer, Lancaster Stemmer, and Snowball Stemmer. [Learn more about the differences between them here.](https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/)"
      ],
      "metadata": {
        "id": "W1tig0UtkDds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "#Test on word \"re-test\"\n",
        "print(porter.stem('Re-testing'), lancaster.stem('Re-testing'), snowball.stem('Re-testing'))"
      ],
      "metadata": {
        "id": "YdAH9NwdkKk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use each to stem the first 100 words in the news category of the Brown corpus (using the version of the corpus we have just cleaned of stopwords and punctuation)."
      ],
      "metadata": {
        "id": "YM4VPsRvkMRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for stemmer in [porter, lancaster, snowball]:\n",
        "    print([stemmer.stem(t) for t in news_text_cleaned])"
      ],
      "metadata": {
        "id": "6GgfAcAskLuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to to stemming, lemmatization aims to derive the genuine dictionary root word, not just a truncated version of the word. \n",
        "\n",
        "The default lemmatization method with the Python NLTK is the WordNet lemmatizer. Let's use it to lemmatize the same tokens from the Brown corpus. "
      ],
      "metadata": {
        "id": "kbtHe5okkNxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print([wnl.lemmatize(t) for t in news_text_cleaned])"
      ],
      "metadata": {
        "id": "67JnngX5kQHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, if we try to lemmatize a word, it will end up with the same word. This is because the default part of speech is nouns. Indicate the part of speech for more accurate results."
      ],
      "metadata": {
        "id": "_nkMwNVLkQmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnl.lemmatize('brightening', pos='v')"
      ],
      "metadata": {
        "id": "a3ab28_qkWWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have several corpora to work with for our remaining analysis: \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "news_text = Original versions of news texts in the Brown corpus \n",
        "```\n",
        "```\n",
        "news_text_nostop = Brown corpus news texts cleaned of stopwords\n",
        "```\n",
        "```\n",
        "news_text_nopunct = Brown corpus news texts cleaned of punctuation\n",
        "```\n",
        "```\n",
        "news_text_nostop_nopunct = Brown corpus news texts cleaned of stopwords and punctuation\n",
        "```\n",
        "\n",
        "You will want to choose your corpus carefully, depending on the type of analysis conducted. Below, the reason behind the choice of corpus used in each analysis will be discussed, but feel free to choose different variables and see how it impacts the results. "
      ],
      "metadata": {
        "id": "2LxNNxNEkYnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Part of Speech (POS) Tagging\n",
        "\n",
        "NLTK enables the labeling of all words in a text according to their part of speech. Let's label the file from the brown corpus and print the first 10 tagged tokens. We'll use the file including stop words since stop words comprise meaningful parts of speech. "
      ],
      "metadata": {
        "id": "dQkD3y2ckhU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_pos = nltk.pos_tag(news_text)\n",
        "print(brown_pos[:10])"
      ],
      "metadata": {
        "id": "VNUhVhLAkjd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not sure what the POS tags mean? Retrieve the list of tags and their meanings."
      ],
      "metadata": {
        "id": "vTxNKQcckk03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.help.upenn_tagset()\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "id": "ksf9zmzckk8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chunking\n",
        "\n",
        "NLTK also allows for the identification of phrases in a text through the process of **chunking.** Usually, each chunk contains a head and (optionally) additional words and modifiers. Examples of chunks include noun groups and verb groups.\n",
        "\n",
        "To chunk a text, it must first be labeled by a POS tagger (done above).\n",
        "\n",
        "Then, we need to define a **[chunk grammar,](https://www.nltk.org/book_1ed/ch07.html)** consisting of rules that indicate how sentences should be chunked.\n",
        "\n",
        "Below, a simple grammar for a noun phrase (NP) chunker is defined with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)."
      ],
      "metadata": {
        "id": "PndxDRfGkwtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "9QqIrrLZkyRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a parser to identify chunks in a text based on the grammar defined above."
      ],
      "metadata": {
        "id": "G_oYydMXk0dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "pyORi1W8k0j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll run the chunk parser on our tagged Brown corpus text. "
      ],
      "metadata": {
        "id": "9C2a8G6Hk3k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = chunk_parser.parse(brown_pos)"
      ],
      "metadata": {
        "id": "2xMRTEQwk3rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the chunks using draw(). We first need to create a virtual environment and then display the visualized chunks. "
      ],
      "metadata": {
        "id": "dAQU2HwTk7Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0. "
      ],
      "metadata": {
        "id": "9d21S-6Qk7bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT (Required to display NLTK trees) ###\n",
        "!apt install ghostscript python3-tk\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "TDCKo-k4k-gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(tree)"
      ],
      "metadata": {
        "id": "RbiXy7T2lBqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Named Entity Recognition (NER)\n",
        "\n",
        "NLTK also enables the recognition of **named entities** like people, locations, and times. Let's tag and visualize the named entities in the Brown text"
      ],
      "metadata": {
        "id": "r9i_rlbwk-WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = nltk.ne_chunk(brown_pos)\n",
        "display(tree)"
      ],
      "metadata": {
        "id": "76tfHRtLlG-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also retrieve a list of named entities in a text by creating a function. In this function, four entities were extracted from the Brown corpus text. "
      ],
      "metadata": {
        "id": "edEQD_7zlIK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ne(brown_pos):\n",
        "    tree = nltk.ne_chunk(brown_pos, binary=True)\n",
        "    return set(\n",
        "        \" \".join(i[0] for i in t)\n",
        "        for t in tree\n",
        "        if hasattr(t, \"label\") and t.label() == \"NE\"\n",
        "    )\n",
        "\n",
        "extract_ne(brown_pos)"
      ],
      "metadata": {
        "id": "cJsTETCulIeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Basic Text Analysis \n",
        "\n",
        "A variety of basic text analyses can be conducted through NLTK.\n",
        "\n"
      ],
      "metadata": {
        "id": "HGEjsuGolnbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Frequency Analyses"
      ],
      "metadata": {
        "id": "mDqUxQrEl6Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Len() retrieves length of text or corpus."
      ],
      "metadata": {
        "id": "z7eQaEsBmBE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text = Text(brown.words(categories='news'))\n",
        "len(news_text)"
      ],
      "metadata": {
        "id": "0GxDwOwKmDFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count() retrieves frequency of word appearances. "
      ],
      "metadata": {
        "id": "k6dcpcSEmEYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.count('jury')"
      ],
      "metadata": {
        "id": "g_kNNF5ymEgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FreqDist() retrieves the most common tokens in a text or corpus. "
      ],
      "metadata": {
        "id": "-R2sDitamvcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_distribution = FreqDist(news_text)\n",
        "print(frequency_distribution)\n",
        "#Since the corpus is so large, let's retrieve the 10 most common tokens.\n",
        "frequency_distribution.most_common(10)"
      ],
      "metadata": {
        "id": "wdXQ1HhhmvkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most tokens in this initial analysis are stopwords or punctuation features; let's run FreqDist() on a cleaned corpus. "
      ],
      "metadata": {
        "id": "ZTzDTr6imvyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text_cleaned = [w.lower() for w in news_text_condensed if w.lower() not in stopwords.words('english')]\n",
        "news_text_cleaned = [w for w in news_text_cleaned if w not in string.punctuation and w not in punct_combo]\n",
        "\n",
        "frequency_distribution = FreqDist(news_text_cleaned)\n",
        "frequency_distribution.most_common(10)"
      ],
      "metadata": {
        "id": "LWZ3mHUVmv6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a frequency plot to plot the most common tokens in a text or corpus."
      ],
      "metadata": {
        "id": "IlYzJgkFm3Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_distribution.plot(10, cumulative=True)"
      ],
      "metadata": {
        "id": "WYvIpoGym3YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a plot of the most frequent bigrams in the corpus. "
      ],
      "metadata": {
        "id": "ZnIvnf47m3hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bi_fdist = FreqDist(bi_news_text)\n",
        "\n",
        "for word, frequency in bi_fdist.most_common(3):\n",
        "    print(word, frequency)\n",
        "\n",
        "bi_fdist.plot(3, cumulative=False)"
      ],
      "metadata": {
        "id": "pT-MOyr1m3qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocab() finds the frequency of each word in the text."
      ],
      "metadata": {
        "id": "4GcTOmv7nBt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints a long list of words!\n",
        "#news_text_lemmatized.vocab()"
      ],
      "metadata": {
        "id": "hu6cWan0nB1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Contextual Analyses"
      ],
      "metadata": {
        "id": "JLK-Rq_pl8f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index() finds the position of a word in a text."
      ],
      "metadata": {
        "id": "uLRWB5m7mIt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.index(\"jury\")"
      ],
      "metadata": {
        "id": "u4Stfc0tmI1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concordance() finds and displays each time a word is used and its immediate contexts. Let's perform a concordance on a file in the Brown corpus to explore contexts where \"jury\" is used."
      ],
      "metadata": {
        "id": "f3Td7mjkmT_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.concordance('jury')"
      ],
      "metadata": {
        "id": "LRphVmEhmUIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocation() allows you to find multiple words which commonly co-occur. "
      ],
      "metadata": {
        "id": "Z72-Xj6XmWxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.collocations() "
      ],
      "metadata": {
        "id": "_fqQ7ebGmW4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a lemmatized list of words may pick up more instances of the same root words. "
      ],
      "metadata": {
        "id": "jhYuSUNqmgE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [wnl.lemmatize(word) for word in news_text]\n",
        "news_text_lemmatized = nltk.Text(lemmatized_words)\n",
        "news_text_lemmatized.collocations()"
      ],
      "metadata": {
        "id": "IrUU9SxGmfjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar() retrieves distributional similarity, or other words which appear in the same contexts as the specified word. Most similar words are listed first."
      ],
      "metadata": {
        "id": "KLLRQm3Tml8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text_lemmatized.similar('jury') #similar(self, word, num=20)"
      ],
      "metadata": {
        "id": "JBO2klrGmmT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dispersion plot to find and plot instances of word(s) as distributed across text. Dispersion plots reveal patterns in word positions. Each stripe represents an instance of a word, and each row represents the entire text."
      ],
      "metadata": {
        "id": "JN_sqPvbmo9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.dispersion_plot(['jury', 'judge', 'trial'])"
      ],
      "metadata": {
        "id": "_w14QuCGmsKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given two words in a text, common_contexts() displays where they are used similarly. "
      ],
      "metadata": {
        "id": "yeWq1bctnIaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_text.common_contexts(['county', 'state']) "
      ],
      "metadata": {
        "id": "j8uFbtdQnI7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Classification\n",
        "\n",
        "Genre Categorization with Naive Bayes Classifier\n",
        "\n",
        "Based on \"Another Excercise: Classifying News Documents in Categories: sport, humor, adventure, science fiction, etc...\" in [Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ],
      "metadata": {
        "id": "4LIHinCqrB4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim here is to build a classifier to sort texts in the Brown corpus into genre categories based on the most frequent words used. [Learn more about the Brown corpus here. ](https://www.nltk.org/book/ch02.html)\n",
        "\n",
        "First, prepare the Brown corpus for classification analysis."
      ],
      "metadata": {
        "id": "QThPBK6arWJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean spacing\n",
        "brown.readme().replace('\\n', ' ')\n",
        "\n",
        "#Get file ids\n",
        "#brown.fileids()\n",
        "\n",
        "#Get categories (genres of text) in Brown corpus\n",
        "brown.categories()"
      ],
      "metadata": {
        "id": "i7hL2MzNr91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, compile a list of the most popular words in the corpus. Then sort the words into a mutable list and sort them based on frequency"
      ],
      "metadata": {
        "id": "YGR8NOWksQug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the word is alphabetical avoids including stuff like `` and '' which are actually pretty common. \n",
        "# Note that it also omits words such as 1 (very common), aug., 1913, $30, 13th, over-all etc. Another option would have been .isalnum().\n",
        "words_in_corpora = FreqDist(w.lower() for w in brown.words() if w.isalpha()) \n",
        "\n",
        "#Put most frequent words in mutable list\n",
        "words_in_corpora_freq_sorted = list(map(list, words_in_corpora.items()))\n",
        "#words_in_corpora_freq_sorted\n",
        "\n",
        "#Sort words in list based on frequency\n",
        "words_in_corpora_freq_sorted.sort(key=lambda x: x[1], reverse=True) # Using a lambda function is an alternative to using the operator library.\n",
        "\n",
        "#Get 10 most frequent words\n",
        "words_in_corpora_freq_sorted[:10]"
      ],
      "metadata": {
        "id": "AEFrpnwCsV_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, put the 1500 most frequent words in list into a new variable and delete word count (list item 1)"
      ],
      "metadata": {
        "id": "wfhiqg1YtGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new variable for top 1500 words\n",
        "best1500 = words_in_corpora_freq_sorted[:1500]\n",
        "\n",
        "#Remove frequency counts\n",
        "for list_item in best1500:\n",
        "    del list_item[1]\n",
        "\n",
        "#Print top 10 words in new variable\n",
        "best1500[:10]"
      ],
      "metadata": {
        "id": "GbTuO8NYtG1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since best1500 is now a list of words, it should be [flattened](https://stackabuse.com/python-how-to-flatten-list-of-lists/). Break down the list into its individual sublists and then use the chain() function. Chain further breaks down each sublist into its individual components, so this approach can be used to flatten any list of lists."
      ],
      "metadata": {
        "id": "m_F4auFRtbwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = itertools.chain(*best1500) \n",
        "best1500 = list(chain) # chain is of type itertools.chain so we need the cast\n",
        "best1500[:10]"
      ],
      "metadata": {
        "id": "MeTyGdr3tlmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of the top 1500 words were stopwords; these will now be removed from the list."
      ],
      "metadata": {
        "id": "_q-WtxQNuLO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopw = stopwords.words('english')\n",
        "\n",
        "def nonstop(listwords):\n",
        "    return [word for word in listwords if word not in stopw]\n",
        "\n",
        "best1500_words_corpora = nonstop(best1500) # Note how this will probably contain less than 1500 words.\n",
        "best1500_words_corpora[:10]"
      ],
      "metadata": {
        "id": "1NOyMavCuRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the documents in the corpus must be converted to forms suitable for classification. Each file in the corpus will eventually be represented by a dictionary showing the presence of the corpusâ€™ most popular words in the particular file."
      ],
      "metadata": {
        "id": "CDtPuFDkufqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = [(nonstop(brown.words(fileid)), category) for category in brown.categories() for fileid in brown.fileids(category)]\n",
        "# documents # Note how documents is a list of tuples.\n",
        "\n",
        "# The code above generates a representation of the corpus but without removing punctuation. This is better:\n",
        "documents = [([item.lower() for item in nonstop(brown.words(fileid)) if item.isalpha()], category)\n",
        "             for category in brown.categories()\n",
        "             for fileid in brown.fileids(category)]\n",
        "#documents # Long List! Note how documents is a list of tuples\n",
        "\n",
        "#Shuffle items in documents\n",
        "shuffle(documents)"
      ],
      "metadata": {
        "id": "opbBEfaCupAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a list of features to be used to train the classifier - in this case, the presence of the 1500 most frequent words in the corpus"
      ],
      "metadata": {
        "id": "j2WCV1CsveTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(doc):\n",
        "    doc_set_words = set(doc) # Checking whether a word occurs in a set is much faster than checking whether it occurs in a list.\n",
        "    features_dic = {} # Features is a dictionary\n",
        "    for word in best1500_words_corpora:\n",
        "        features_dic['has(%s)' % word] = (word in doc_set_words)\n",
        "    return features_dic\n",
        "\n",
        "doc_features_set = [(document_features(d),c) for (d,c) in documents]\n",
        "#doc_features_set[0]"
      ],
      "metadata": {
        "id": "2PcA-obrvAK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now build the classifer to determine what category documents fall into based on the most frequent words."
      ],
      "metadata": {
        "id": "im9QWxKqv1Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort documents into training and test sets \n",
        "train_set = doc_features_set[:350] # Since the total is 500\n",
        "test_set  = doc_features_set[150:]\n",
        "\n",
        "#Run classifier on training set\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "#Get accuracy of classifier\n",
        "print(accuracy(classifier, test_set))\n",
        "\n",
        "#Show most informative features\n",
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "id": "qRuQuqXAv28c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test classification of documet 'ca01' (it is under the 'news' category)"
      ],
      "metadata": {
        "id": "fKlo_yt7wVTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(brown.words('ca01')))"
      ],
      "metadata": {
        "id": "2GUudPvdwVg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New texts can also be prepared for classificiation. "
      ],
      "metadata": {
        "id": "RmmxGh7XL33Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The test text needs to be long enough in order to contain a significant amount of the 1500 most common words in our training corpus.\n",
        "text = \"1 God, infinitely perfect and blessed in himself, in a plan of sheer goodness freely created man to make him share in his own blessed life. For this reason, at every time and in every place, God draws close to man. He calls man to seek him, to know him, to love him with all his strength. He calls together all men, scattered and divided by sin, into the unity of his family, the Church. To accomplish this, when the fullness of time had come, God sent his Son as Redeemer and Saviour. In his Son and through him, he invites men to become, in the Holy Spirit, his adopted children and thus heirs of his blessed life. 2 So that this call should resound throughout the world, Christ sent forth the apostles he had chosen, commissioning them to proclaim the gospel: \\\"Go therefore and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, teaching them to observe all that I have commanded you; and lo, I am with you always, to the close of the age.\\\"4 Strengthened by this mission, the apostles \\\"went forth and preached everywhere, while the Lord worked with them and confirmed the message by the signs that attended it.\\\" 3 Those who with God's help have welcomed Christ's call and freely responded to it are urged on by love of Christ to proclaim the Good News everywhere in the world. This treasure, received from the apostles, has been faithfully guarded by their successors. All Christ's faithful are called to hand it on from generation to generation, by professing the faith, by living it in fraternal sharing, and by celebrating it in liturgy and prayer. 4 Quite early on, the name catechesis was given to the totality of the Church's efforts to make disciples, to help men believe that Jesus is the Son of God so that believing they might have life in his name, and to educate and instruct them in this life, thus building up the body of Christ. Catechesis is an education in the faith of children, young people and adults which includes especially the teaching of Christian doctrine imparted, generally speaking, in an organic and systematic way, with a view to initiating the hearers into the fullness of Christian life. While not being formally identified with them, catechesis is built on a certain number of elements of the Church's pastoral mission which have a catechetical aspect, that prepare for catechesis, or spring from it. They are: the initial proclamation of the Gospel or missionary preaching to arouse faith; examination of the reasons for belief; experience of Christian living; celebration of the sacraments; integration into the ecclesial community; and apostolic and missionary witness. Catechesis is intimately bound up with the whole of the Church's life. Not only her geographical extension and numerical increase, but even more her inner growth and correspondence with God's plan depend essentially on catechesis. Periods of renewal in the Church are also intense moments of catechesis. In the great era of the Fathers of the Church, saintly bishops devoted an important part of their ministry to catechesis. St. Cyril of Jerusalem and St. John Chrysostom, St. Ambrose and St. Augustine, and many other Fathers wrote catechetical works that remain models for us. The ministry of catechesis draws ever fresh energy from the councils. the Council of Trent is a noteworthy example of this. It gave catechesis priority in its constitutions and decrees. It lies at the origin of the Roman Catechism, which is also known by the name of that council and which is a work of the first rank as a summary of Christian teaching. The Council of Trent initiated a remarkable organization of the Church's catechesis. Thanks to the work of holy bishops and theologians such as St. Peter Canisius, St. Charles Borromeo, St. Turibius of Mongrovejo or St. Robert Bellarmine, it occasioned the publication of numerous catechisms. It is therefore no surprise that catechesis in the Church has again attracted attention in the wake of the Second Vatican Council, which Pope Paul Vl considered the great catechism of modern times. the General Catechetical Directory (1971) the sessions of the Synod of Bishops devoted to evangelization (1974) and catechesis (1977), the apostolic exhortations Evangelii nuntiandi (1975) and Catechesi tradendae (1979), attest to this. the Extraordinary Synod of Bishops in 1985 asked that a catechism or compendium of all Catholic doctrine regarding both faith and morals be composed. The Holy Father, Pope John Paul II, made the Synod's wish his own, acknowledging that this desire wholly corresponds to a real need of the universal Church and of the particular Churches. He set in motion everything needed to carry out the Synod Fathers' wish.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') # Picks out sequences of alphanumeric characters as tokens and drops everything else\n",
        "text_tokens = nonstop(tokenizer.tokenize(text.lower()))\n",
        "text_tokens = [w for w in text_tokens if w.isalpha()]\n",
        "#text_tokens\n",
        "\n",
        "#Determine whether list of tokens contain most frequent words set above\n",
        "text_features = document_features(text_tokens)\n",
        "#text_features"
      ],
      "metadata": {
        "id": "FlmQ8-5ZL4Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now classify the new document based on presence of frequent words in brown corpus categories"
      ],
      "metadata": {
        "id": "sokA0DK3MWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.classify(document_features(text_tokens))"
      ],
      "metadata": {
        "id": "b-rSNXKyMWfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SpaCy Pipeline\n",
        "\n",
        "Describe this pipeline"
      ],
      "metadata": {
        "id": "wSj-5-Own2KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Libraries, Packages and Corpora\n",
        "\n",
        "To perform analysis with spaCy, first load an NLP pipeline package as an object (nlp). This object will contain all the different components in the pipeline and can be used like a function to analyze text. Learn more about the different packages available and how to install them here: https://spacy.io/usage/models \n",
        "\n",
        "Discuss how there is much less to import, as everything is included with spaCy"
      ],
      "metadata": {
        "id": "z6ZaCBxQoChx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load NLP pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "6ergd5I_pgab"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Tokenization\n",
        "\n",
        "Use spaCy to create a document object and tokenize."
      ],
      "metadata": {
        "id": "UhnPogOcpyse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "218kLQyyqWEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy can also be used to tokenize corpora uploaded from a local machine. "
      ],
      "metadata": {
        "id": "zikGPJNYri6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import file importer\n",
        "from google.colab import files\n",
        "\n",
        "#Run file upload (Click \"Choose files\" to browse and select local file)\n",
        "text = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "FBCUk_3FuH_y",
        "outputId": "4abcba45-7eef-4f5e-c086-ce19ab27c4b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-93ea7da8-87de-4a9b-ba8e-4f4626c0246f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-93ea7da8-87de-4a9b-ba8e-4f4626c0246f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CLS.G0.06.1 Background.txt to CLS.G0.06.1 Background.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert document into string\n",
        "text= str(text)\n",
        "\n",
        "#Create document object and tokenize\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "CzQ39UBUroC9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "AwiS-6XUu7C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike NLTK, spaCy does NOT have a dedicated feature for n-gram analysis"
      ],
      "metadata": {
        "id": "FlM85J99vGpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Note About Text Cleaning\n",
        "\n",
        "Most tutorials recommend using NLTK to clean a text. Refer to the tutorial above. REFLECT MORE ON THIS? "
      ],
      "metadata": {
        "id": "6MQGWjMXvlnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Enrichment\n"
      ],
      "metadata": {
        "id": "iz-tTNgrvVp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Part of Speech and Dependency Parsing\n",
        " DISCUSS "
      ],
      "metadata": {
        "id": "vM60ISoM0Z8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "mh_OrMM-0kvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize POS and dependency parsing:"
      ],
      "metadata": {
        "id": "fgcnYbcU0uPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install visualizer\n",
        "from spacy import displacy\n",
        "\n",
        "#Visualize POS tags\n",
        "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "FmMirwDk0uis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Lemmatization\n",
        "\n",
        "DISCUSS"
      ],
      "metadata": {
        "id": "PnF7GznY01_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_sentence_example = \"Their Apples & Banana fruit salads are amazing. Would you like meeting me at the cafe?\"\n",
        "[(token, token.lemma_, token.lemma) for token in nlp(lemma_sentence_example)]"
      ],
      "metadata": {
        "id": "1lomLz3g02IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine them all"
      ],
      "metadata": {
        "id": "-K2B-iwa13TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "39viP73a17CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Named Entity Recognition\n",
        "\n",
        "DISCUSS"
      ],
      "metadata": {
        "id": "X2b0e1Kx0_59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "QtLNMa-p1RKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCyâ€™s built-in displaCy visualizer, hereâ€™s what our example sentence and its named entities look like:"
      ],
      "metadata": {
        "id": "jHGNJ6Yc1YHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "4kxMuEUF1YvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about entity recognition in spaCy, how to add your own entities to a document and how to train and update the entity predictions of a model, see the usage guides on [named entity recognition](https://spacy.io/usage/linguistic-features#named-entities) and [training pipelines.](https://spacy.io/usage/training)"
      ],
      "metadata": {
        "id": "fBGE8gAa1cQt"
      }
    }
  ]
}