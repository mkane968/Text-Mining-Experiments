{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part of Speech Tagging 101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrr+nqVHQ/xXgVJ8s4JYdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Text-Mining-Experiments/blob/main/Part_of_Speech_Tagging_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part of Speech Tagging 101\n",
        "\n",
        "[Add brief intro to POS tagging, what it is, various forms and applications]\n",
        "\n",
        "In this tutorial, you will learn: \n",
        "\n",
        "*   POS Tagging with NLTK\n",
        "*   POS Tagging with SpaCy\n",
        "*   POS Visualization with NLTK, SpaCy and PosVisualizer\n",
        "*   Other Applications of POS Tagging \n",
        "\n",
        "This workbook was created with the support of the following: \n",
        "CITATIONS"
      ],
      "metadata": {
        "id": "sU-oDptG9_--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting started, download all needed packages and corpora. "
      ],
      "metadata": {
        "id": "CfUU34CiGyZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-LGYQEm98FD"
      },
      "outputs": [],
      "source": [
        "##NLTK Packages and Corpora\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "!pip install svgling\n",
        "\n",
        "#spaCy Packages and Corpora\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "#PosTagVisualizer Packages\n",
        "from yellowbrick.text import PosTagVisualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS Tagging with NLTK \n",
        "\n",
        "The Natural Language Toolkit can be used to label all words in a sentence, text, or corpora according to their parts of speech. \n",
        "\n",
        "The part of speech tags used in NLTK tagging (`nltk.download('tagsets')`)\n",
        "are from the **Penn Treebank Project**. [Here's a list of the 36 tags recognized by the Penn Treeback. ](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ],
      "metadata": {
        "id": "bHGgqxDw-mXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging with NLTK is a relatively simple process. Here we'll `use nltk.pos_tag()` to tag text from the Brown corpus (including stop words) and print the first 10 tagged tokens."
      ],
      "metadata": {
        "id": "sz9iuFxm_zNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Get the first 100 words in the news category of the Brown corpus\n",
        "news_text = brown.words(categories='news')\n",
        "news_text_condensed = news_text[:100]\n",
        "\n",
        "##Label the text with POS tags\n",
        "brown_pos = nltk.pos_tag(news_text_condensed)\n",
        "print(brown_pos[:10])"
      ],
      "metadata": {
        "id": "G_CLih4YAa8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not sure what the POS tags mean? Retrieve the list of tags and their meanings."
      ],
      "metadata": {
        "id": "DeasPMqXAaX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define all tags (outputs long list)\n",
        "#nltk.help.upenn_tagset()\n",
        "\n",
        "#Define single tag\n",
        "nltk.help.upenn_tagset('NN')"
      ],
      "metadata": {
        "id": "PublLQ7q2fZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's likely you will want to perform POS tagging on your own text, file or corpus. Here is how to tag a sentence:"
      ],
      "metadata": {
        "id": "dWc6HnF723Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a variable for the sentence you want to tag\n",
        "sentence = \"This is an example sentence.\"\n",
        "\n",
        "#Tokenize sentence so POS tagger can identify each word \n",
        "tokenized_sent = nltk.sent_tokenize(sentence)\n",
        "\n",
        "##Label each word in the tokenized sentence with a POS tag\n",
        "sent_pos = [nltk.pos_tag(nltk.word_tokenize(word)) for word in tokenized_sent]\n",
        "print(sent_pos)"
      ],
      "metadata": {
        "id": "hECDglyg3Ud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how to perform part of speech tagging on a single file. You will first need to upload the file, convert it to a string and tokenize. "
      ],
      "metadata": {
        "id": "dOhLdvcn3UoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Google Colab: Choose file you want to upload (just choose one for now)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "##JupyterLab: Define file path on local machine (replace string with own path)\n",
        "#fn = '/Users/megankane/Desktop/DreamLab22/Corpus/Argument1.txt'"
      ],
      "metadata": {
        "id": "GYbIYSdO3jG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert document into string\n",
        "uploaded= str(uploaded)\n",
        "\n",
        "#Convert document into word tokens and print first ten tokens\n",
        "word_tokens = word_tokenize(uploaded)\n",
        "print(word_tokens[:10])"
      ],
      "metadata": {
        "id": "Wyz8vDR58-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice some strange characters in your file which will cause the POS tagger to misidentify tokens. Here is one way to clean any non-alphanumeric characters from your file. \n",
        "\n",
        "All cleaning decisions should be made carefully; if you are interested in punctuation, for example, the following would not be useful. Since we are interested in words (and numbers) only for NLTK part of speech tagging, all other characters can be removed"
      ],
      "metadata": {
        "id": "2_cOjQLn-x_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_words= [word for word in word_tokens if word.isalnum()]\n",
        "new_words[:10]"
      ],
      "metadata": {
        "id": "AebsSmZC_BFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the POS tagger on the tokenized file."
      ],
      "metadata": {
        "id": "GLdminX_-IPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Label each word in the tokenized file with a POS tag\n",
        "uploaded_pos = [nltk.pos_tag(nltk.word_tokenize(word)) for word in new_words]\n",
        "print(uploaded_pos)"
      ],
      "metadata": {
        "id": "CMVCuWSv-LDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD SOME WAY TO EXPORT RESULTING FILE??"
      ],
      "metadata": {
        "id": "d5CI7dBR3Ylr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how to perform POS tagging on a corpus. The basics are the same as above; instead of selecting one document from your machine, select multiple files. "
      ],
      "metadata": {
        "id": "UqUeKbPWDj8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Choose all the files you want to upload\n",
        "from google.colab import files\n",
        "uploaded_files = files.upload()"
      ],
      "metadata": {
        "id": "Eyk3edkbDjxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert documents into strings and clean--CURRENTLY CONVERTS TO ONE LONG STRING INSTEAD OF SPLITTING INTO MULTIPLE FILES"
      ],
      "metadata": {
        "id": "iWOZmteNELG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert documents into string\n",
        "uploaded_files= str(uploaded_files)\n",
        "\n",
        "#Convert document into word tokens and print first ten tokens\n",
        "word_tokens_c = word_tokenize(uploaded_files)\n",
        "print(word_tokens_c[:10])\n",
        "\n",
        "#Clean word tokens\n",
        "new_words_c= [word for word in word_tokens_c if word.isalnum()]\n",
        "new_words_c[:10]"
      ],
      "metadata": {
        "id": "CL_Hql1NELOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Label each word in the tokenized file with a POS tag\n",
        "uploaded_pos_c = [nltk.pos_tag(nltk.word_tokenize(word)) for word in new_words_c]\n",
        "print(uploaded_pos_c)"
      ],
      "metadata": {
        "id": "aEVRSyOEFmcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD WAY TO EXPORT FOR ANALYSIS? "
      ],
      "metadata": {
        "id": "COt5W3nxGHwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS Tagging with SpaCy\n",
        "POS tagging is one of several linguistic annotations spaCy generates to support analysis of texts' grammatical structure. SpaCy uses a trained pipeline and statistical models to assign POS classifications to tokens. [Here's a list of the 19 tags recognized by SpaCy's POS classifier.](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#POS_Tagging_in_Spacy_Library)\n",
        "\n",
        "It is a relatively simple process to print the part of speech corresponding to each word in a sample sentence. "
      ],
      "metadata": {
        "id": "Z2xPLLOkGUqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load nlp model and create doc object (contains original text tokens and results from nlp processing)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"This is an example sentence.\")\n",
        "\n",
        "#Print part of speech tags for each word in doc\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "  \n",
        "output_str = f'{token.pos_}\\n'"
      ],
      "metadata": {
        "id": "5dQYCGG3GkDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice some differences from the NLTK tagset dictionary (NLTK tagging of the same sentence reproduced below).\n",
        "\n",
        "`[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]`\n",
        "\n",
        "Both taggers recognize 'an' as a determiner, but all other words are assigned slightly different labels: \n",
        "*   \"This\" is either a determiner (NLTK) or a pronoun (spaCy). In the context of the sentence, the pronoun label is likely more accurate, since \"this\" takes the place of a whole noun phrase instead of qualifying a larger phrase.\n",
        "*   \"Is\" is either a present-tense third-person verb with a singluar basis (NLTK) or an auxiliary verb (spaCy). In this sentence, is stands alone, so NLTK is more accurate if one's definition of auxiliary indicates it is a helping verb. \n",
        "*   \"Example\" and \"sentence\" are both tagged as nouns, but NLTK provides additional information--that the nouns are singular or mass objects. \n",
        "*   Periods are not recognized by NLTK, but tagged as punctuation in spaCy; this is true of other punctuation (*., (, ), ?*) which may influence cleaning decisions. \n",
        "\n",
        "As noted above, SpaCy can returns additional information about the POS tags and can perform a variety of other linguistic annotations. For example, spaCy also generates \"fine-grained\" POS tags through the \".tag_\" attribute and explanation for each feature.\n"
      ],
      "metadata": {
        "id": "Dv8cNsR2GRUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print POS tags, fine-grained part of speech tags and explanation for each feature, separated by spaces for improved readability\n",
        "for token in doc:\n",
        "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')"
      ],
      "metadata": {
        "id": "AkOiSMTVHjHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discrepencies between NLTK and spaCy tags have been resolved here, though may persist in longer text as the fine-grained tagger has 56 tags; a list can be found [here](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#POS_Tagging_in_Spacy_Library).\n",
        "\n",
        "From this very brief example, one tagger is not always more accurate than another, but spaCy may be preferred if wanting multiple layers of POS information.  Deciding between NLTK and spaCy may also be guided by goals of analysis. NLTK modules are transparent and easy to run. SpaCy's processing is somewhat more abstract, but it is fast and can generate a variety of linguistic annotations in one go. [Here's even more information about the differences between spaCy and NLTK. ](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/#:~:text=There's%20a%20real%20philosophical%20difference,you%20get%20specific%20tasks%20done.)"
      ],
      "metadata": {
        "id": "7efJh_zkP_JL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how to run spaCy POS analysis on a single text file. "
      ],
      "metadata": {
        "id": "S5f0ms50RVT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Choose file you want to upload (just choose one for now)\n",
        "from google.colab import files\n",
        "uploaded_spaCy = files.upload()"
      ],
      "metadata": {
        "id": "Ag4ebygkRYiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert document into string\n",
        "uploaded_spaCy = str(uploaded_spaCy)\n",
        "\n",
        "#Clean document of all non-alnum characters--DON'T USE if wanting to keep punctuation, need solution that replaces all chars EXCEPT those recognized by spaCy\n",
        "#uploaded_spaCy = [word for word in word_tokens if word.isalnum()]\n",
        "#uploaded_spaCy [:10]\n",
        "\n",
        "#Convert string to doc object spaCy can analyze\n",
        "doc_file = nlp(uploaded_spaCy)\n",
        "\n",
        "#Print POS tags, fine-grained part of speech tags and explanation for each feature, separated by spaces for improved readability\n",
        "for token in doc_file:\n",
        "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')"
      ],
      "metadata": {
        "id": "NphyrJVRTCsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add way to export POS output here/below as well?? \n",
        "\n",
        "Here is how to run spacy POS analysis on multiple text files. "
      ],
      "metadata": {
        "id": "2mot6uHCRYqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WILL ADD ONCE UPLOAD PROCESS IS FIXED FOR NLTK ABOVE "
      ],
      "metadata": {
        "id": "4XmFjVNSRlRn"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POS Visualization \n",
        "\n",
        "Once POS tags have been gathered, visualization is a useful next step to better understand what information has been gathered and to conduct some preliminary analysis. SpaCy, NLTK, and the POS Visualization tool are three ways to visualize parts of speech in meaningful ways. "
      ],
      "metadata": {
        "id": "aJ92bBciGIBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NLTK POS Visualization \n",
        "Meaningful POS visualization in NLTK is done through a process called **chunking.** When a text is \"chunked,\" it is separated into grammatical units which contain a head and (optionally) additional words and modifiers. Examples of chunks include noun groups and verb groups. If you are interested in particular chunks, these can be output and visualized to get a better understanding of their distribution within a text. \n",
        "\n",
        "Below, a simple grammar for a noun phrase (NP) chunker is defined with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). \n",
        "\n",
        "Learn more about defining a chunk grammar [here](https://www.nltk.org/book_1ed/ch07.html). "
      ],
      "metadata": {
        "id": "7rtGsuBR2qhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Define a chunk grammar\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "#Create a parser to identify chunks in a text based on the grammar defined\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "#Run the chunk parser on our single sentence\n",
        "tree = chunk_parser.parse(brown_pos)"
      ],
      "metadata": {
        "id": "4vwdbW7K2qqH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the chunks using draw(). We first need to create a virtual environment and then display the visualized chunks. "
      ],
      "metadata": {
        "id": "Wzu_F1iB2vOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CREATE VIRTUAL DISPLAY ###\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0. \n",
        "\n",
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT (Required to display NLTK trees) ###\n",
        "!apt install ghostscript python3-tk"
      ],
      "metadata": {
        "id": "m1JmlmgX2vWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can visualize the text in terms of its part of speech tags, delineated as grammatical chunks. Click the resulting images to enlarge. "
      ],
      "metadata": {
        "id": "WwBkl8x-2yyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(tree)"
      ],
      "metadata": {
        "id": "BFm_gLGL20UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SpaCy POS Visualization \n",
        "\n",
        "Similar to NLTK, parts of speech and the grammatical relationships between words can be visualized in spaCy using the displaCy visualizer. In the visualization below, each word is denoted by its part of speech and the arrows denote the relationships between words (dependencies). "
      ],
      "metadata": {
        "id": "QjV5UGg6X7AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dependency visualizer\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "LA7ovqy0XUlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###POS Visualization with PosTagVisualizer\n",
        "\n",
        "The POS visualizer creates a bar chart to visualize the relative proportions of different parts-of-speech in a corpus. It works with corpora tagged by either via NLTK or spaCy. [Learn more about the visualizer here. ](https://www.scikit-yb.org/en/latest/api/text/postag.html)"
      ],
      "metadata": {
        "id": "92A6WCE4Yiq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's visualize the condensed news text from the Brown corpus defined above. The PosTagVisualizer can be used with untagged text by using the \"parse\" keyword and specifying NLTK as the dictionary of use."
      ],
      "metadata": {
        "id": "lt47kFT5ZCev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the visualizer, fit, score, and show it\n",
        "viz = PosTagVisualizer(parser='nltk')\n",
        "viz.fit(news_text_condensed)\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "bjM6tN83ZCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tag and visualize text using the universal dependency tags from spaCy by specifying spaCy as the parsing dictionary. Let's try it on the file we have prepared as a spaCy doc object. **NOT SURE WHY THIS ISN'T WORKING!!**"
      ],
      "metadata": {
        "id": "e1ydG-vbZI5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PosTagVisualizer(parser='spacy')\n",
        "viz.fit(doc_file)\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "ZZ__Fd7tZFs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PosTagVisualizer also works with texts already tagged by NLTK or spaCy. Here's an example of text tagged via NLTK. **NOT WORKING YET EITHER**"
      ],
      "metadata": {
        "id": "B9UAVxNoZ25q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the visualizer, fit, score, and show it \n",
        "viz = PosTagVisualizer()\n",
        "viz.fit(uploaded_pos)\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "vLedSDnXZVs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PosTagVisualizer can also be used with text tagged using spaCy's universal dependency framework by specifying the tagset keyword as “universal.\" **NEEDS WORK**"
      ],
      "metadata": {
        "id": "I3uf5FrdaSip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the visualizer, fit, score, and show it\n",
        "viz = PosTagVisualizer(tagset=\"universal\")\n",
        "viz.fit(output_str)\n",
        "viz.show()"
      ],
      "metadata": {
        "id": "IdUQesKPZ_iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Other Applications of POS Tagging\n",
        "\n",
        "Why perform POS tagging? For many it is a useful pre-processing step prior to more advanced NLP like named entity recognition. For example, POS taggers can removeany unrecognized words like \"etc\" and \"i.e.\" since those labeled with the POS tag \"X.\" Below, all words tagged as \"X\" are identified and then removed."
      ],
      "metadata": {
        "id": "etwFS6zTarcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw text document\n",
        "raw_text=\"\"\"Apple, an American multinational technology company that specializes in consumer electronics, software, online services, etc. is looking at buying U.K. startup for $1 billion i.e. a very good indicator of the company's value\"\"\"\n",
        "\n",
        "# Creating a spacy object\n",
        "raw_doc=nlp(raw_text)\n",
        "\n",
        "# Checking if POS tag is X and printing them\n",
        "print('The junk values are..')\n",
        "for token in raw_doc:\n",
        "  if token.pos_=='X':\n",
        "    print(token.text)\n",
        "\n",
        "print('After removing junk')\n",
        "# Removing the tokens whose POS tag is junk.\n",
        "clean_doc=[token for token in raw_doc if not token.pos_=='X']\n",
        "print(clean_doc)"
      ],
      "metadata": {
        "id": "zgp3CbTRaq_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If interested in distributions of specific POS tags or patterns of tags (along with other linguistic attributes), these can also be identified using the Matcher."
      ],
      "metadata": {
        "id": "lFNsNV9Jaupl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Initialize the Matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Write a pattern that matches a form of \"buy\" plus proper noun\n",
        "pattern = [{\"LEMMA\": 'buy'}, {\"POS\": 'PROPN'}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"NEW_PATTERN\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)\n"
      ],
      "metadata": {
        "id": "uaoIbNSMahQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INCLUDE A MODULE TALKING ABOUT HOW POS TAGGING CAN BE USED FOR DISAMBIGUATION OF KEYWORDS"
      ],
      "metadata": {
        "id": "b65QNcpGboO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BvhcwITrbno-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also be interested in determining whether POS tags can meaningfully separate documents based on specific classifications (i.e. author, genre). This can be determined by running Most Distinctive Word (MDW) analysis, wherein most distinctive parts of speech (rather than words) are determined across subsets of interest in a corpus. "
      ],
      "metadata": {
        "id": "ZawrFbCmbxjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Um2V6MMqbVVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}